Title: Weekly AI Ethics News Roundup: Mar 3 - Mar 10
SubTitle: Mar 3 - Mar 10
Date: 2020-03-10 09:20
Modified: 2020-03-10 09:20
Category: news
Tags: roundup
Slug: ai-ethics-news-roundup-mar3-mar10-2020
Authors: EI-Team
Template: roundup
Summary: @Floridi & @agstrait ethical analysis, @carolinejmolloy: NHS and tech, @misslivirose: consent and virtuality, @jacobsonjenna: cybervetting, @mckelveyf: the public good, @timleberecht: AI, nature, and ethics, @NathalieSmuha: human rights, @SandraWachter5: fairness, @Sarah_Brayne: crime prediction + more!

Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.

Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.


## Ethical Foresight Analysis: What it is and Why it is Needed?

A new piece from Luciano Floridi and Andrew Strait:

"An increasing number of technology firms are implementing processes to identify and evaluate the ethical risks of their systems and products. A key part of these review processes is to foresee potential impacts of these technologies on different groups of users. In this article, we use the expression Ethical Foresight Analysis (EFA) to refer to a variety of analytical strategies for anticipating or predicting the ethical issues that new technological artefacts, services, and applications may raise. This article examines several existing EFA methodologies currently in use. It identifies the purposes of ethical foresight, the kinds of methods that current methodologies employ, and the strengths and weaknesses of each of these current approaches. The conclusion is that a new kind of foresight analysis on the ethics of emerging technologies is both feasible and urgently needed."

<a class="readmore" href="https://link.springer.com/article/10.1007%2Fs11023-020-09521-y">Read More</a>


## The Robots Are Coming: Ethics, Politics, and Society in the Age of Artificial Intelligence

The late Kenneth Taylor's previously unpublished essay on the impact of AI and Robotics just came out in the Boston Review:

"If we cannot stop or reverse the robot invasion of the built human world, we must turn and face them. We must confront hard questions about what will and should become of both them and us as we welcome ever more of them into our midst. Should we seek to regulate their development and deployment? Should we accept the inevitability that we will lose much work to them? If so, perhaps we should rethink the very basis of our economy. Nor is it merely questions of money that we must face. There are also questions of meaning. What exactly will we do with ourselves if there is no longer any economic demand for human cognitive labor? How shall we find meaning and purpose in a world without work?"

<a class="readmore" href="https://bostonreview.net/science-nature-philosophy-religion/kenneth-taylor-robots-are-coming">Read More</a>


## Algorithmic Legal Metrics

This is a fascinating paper that in places runs very much against the grain of current thinking. 

"In this paper I link the sociological and legal analysis of AI, highlighting the reflexive social processes that are engaged by algorithmic metrics. This paper examines these overlooked social effects of predictive legal algorithms, and contributes to the literature a vital fundamental but missing critique of such analytics. Specifically, this paper shows how the problematic social effects of algorithmic legal metrics extend far beyond the concerns about accuracy that have thus far dominated critiques of such metrics. Second, it demonstrates that corrective governance mechanisms such as enhanced due process or transparency will be inadequate to remedy such corrosive effects, and that some such remedies, such as transparency, may actually exacerbate the worst effects of algorithmic governmentality. Third, the paper shows that the application of algorithmic metrics to legal decisions aggravates the latent tensions between equity and autonomy in liberal institutions, undermining democratic values in a manner and on a scale not previously experienced by human societies. Illuminating these effects casts new light on the inherent social costs of AI metrics, particularly the perverse effects of deploying algorithms in legal systems. "


<a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3537337">Read More</a>


## How ‘Smart Tech’ Masks an Emerging Era of Corporate Control

"Harvesting data requires the technical ability and social authority to probe things, people, and places. Control systems are fueled by data, which allows for more granular, more effective, and more instantaneous command over those same things, people, and places. Smart tech is the offspring of both imperatives."

<a class="readmore" href="https://onezero.medium.com/how-smart-tech-masks-an-emerging-era-of-corporate-control-779c96b05f85">Read More</a>


## Cybervetting job applicants on social media: the new normal?

"Our research, using an online survey with 482 participants, investigates young people’s concerns with their publicly available social media data being used in the context of job hiring. Grounded in stakeholder theory, we analyze the relationship between young people’s concerns with social media screening and their gender, job seeking status, privacy concerns, and social media use. We find that young people are generally not comfortable with social media screening. A key finding of this research is that concern for privacy for public information on social media cannot be fully explained by some “traditional” variables in privacy research. The research extends stakeholder theory to identify how social media data ethics should be inextricably linked to organizational practices. The findings have theoretical implications for a rich conceptualization of stakeholders in an age of social media and practical implications for organizations engaging in cybervetting."

<a class="readmore" href="https://link.springer.com/article/10.1007%2Fs10676-020-09526-2">Read More</a>


## Before Adopting New Technologies, We Must Define the Common Good

"Canada (and the rest of the world) faces an opportunity: to avoid (or halt) the harmful effects of digital control and techno-social engineering, regulators must garner more public interest in the ways that technologies shape public life. Wrestling with the effects of technology, its wide reach and impact provides opportunity to foster more inclusive and engaged consultations about the environmental, health, social, cultural and democratic implications of powerful technologies such as AI and digital platforms. Here, there is excitement as much as risk in searching for the right formats to find, define and debate the common good."

<a class="readmore" href="https://www.cigionline.org/articles/adopting-new-technologies-we-must-define-common-good">Read More</a>


## How Artificial Intelligence Can Turn Us Into Bigots

An intriguing thought: AI can devleop it's own pseudoscience. 

"While the computing involved in AI is often incomprehensibly vast and fast, it always rests upon foundations of simplifying, quantifying, and generalising. And that’s okay: it’s how engineers traditionally solve problems, by breaking them down into simple parts, simplifying where necessary, and building in safety margins to account for inaccuracies."

<a class="readmore" href="https://10daily.com.au/views/a200304obwdr/how-artificial-intelligence-can-turn-us-into-bigots-20200306">Read More</a>


## The case for an AI that puts nature and ethics first, not humans

"...concern is growing that we are surrendering to a paradigm of “forced reductionism” (to borrow a term from former MIT Media Lab director Joi Ito), shoehorning ourselves into a purely mechanistic, utilitarian model of technology. As AI becomes more and more powerful and invasive, it may inevitably change our world to align with these very design principles. The consequence might be a world full of “monochrome societies,” as Infineon CEO Dr. Reinhard Pless puts it."

<a class="readmore" href="https://thenextweb.com/neural/2020/03/07/the-case-for-an-ai-that-puts-nature-and-ethics-first-not-humans-syndication/">Read More</a>


## This Company Is Using Racially-Biased Algorithms to Select Jurors

"Momus Analytics' predictive scoring system is using race to grade potential jurors on vague qualities like "leadership" and "personal responsibility.""

<a class="readmore" href="https://www.vice.com/en_us/article/epgmbw/this-company-is-using-racially-biased-algorithms-to-select-jurors">Read More</a>


## Beyond a Human Rights-based approach to AI Governance: Promise, Pitfalls, Plea

This paper discusses the establishment of a governance framework to secure the development and deployment of “good AI”, and describes the quest for a morally objective compass to steer it. Asserting that human rights can provide such compass, this paper first examines what a human rights-based approach to AI governance entails, and sets out the promise it propagates. Subsequently, it examines the pitfalls associated with human rights, particularly focusing on the criticism that these rights may be too Western, too individualistic, too narrow in scope and too abstract to form the basis of sound AI governance. After rebutting these reproaches, a plea is made to move beyond the calls for a human rights-based approach, and start taking the necessary steps to attain its realisation. It is argued that, without elucidating the applicability and enforceability of human rights in the context of AI; adopting legal rules that concretise those rights where appropriate; enhancing existing enforcement mechanisms; and securing an underlying societal infrastructure that enables human rights in the first place, any human rights-based governance framework for AI risks falling short of its purpose. 

<a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3543112">Read More</a>


## Will Big Tech save the NHS – or eat it alive?


"Technophile politicians and tech companies are making big promises. But the broader impact of the digital transformation of our health and public services has been too little examined." 

"Digitalisation is a tool to ‘tailor’ services, we’re told. But in a context of austerity, the biggest concern is that ‘tailoring’ becomes a fancy word for ‘cutting’ – that it provides the cover for cuts, privatisation, co-payments, and the loss of communal space, public accountability and social connection."

<a class="readmore" href="https://www.opendemocracy.net/en/ournhs/will-big-tech-save-the-nhs-or-eat-it-alive/">Read More</a>


## Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI

"This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automat-ed fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as “contextual equality.”"

<a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922">Read More</a>


## It’s Time To Develop A Consent Framework For Virtual Beings

The rise of virtual beings leads to new questions regarding the rights of digital representations.

<a class="readmore" href="https://vrscout.com/news/consent-framework-for-virtual-beings/">Read More</a>


## AI In Policing: Better Than ‘A Knife Through The Chest?’

"The U.K. police are rolling out AI crime fighting with little regard to the societal risks. In an impassioned speech about police use of AI, the commissioner of the Metropolitan police, Cressida Dick, criticised privacy advocates. She intoned that concern over the use of Live Face Recognition (LFR) on law abiding citizens, “feels much, much smaller than my and the public’s vital expectation to be kept safe from a knife through the chest.” "

<a class="readmore" href="https://www.forbes.com/sites/noelsharkey/2020/03/06/ai-in-policing-better-than-a-knife-through-the-chest/#e1361f3548e4">Read More</a>

## Technologies of Crime Prediction: The Reception of Algorithms in Policing and Criminal Courts 

"The number of predictive technologies used in the U.S. criminal justice system is on the rise. Yet there is little research to date on the reception of algorithms in criminal justice institutions. We draw on ethnographic fieldwork conducted within a large urban police department and a midsized criminal court to assess the impact of predictive technologies at different stages of the criminal justice process. '

<a class="readmore" href="https://academic.oup.com/socpro/advance-article-abstract/doi/10.1093/socpro/spaa004/5782114">Read More</a>

## JAGI Special Issue “On Defining Artificial Intelligence”

<a class="readmore" href="https://content.sciendo.com/view/journals/jagi/11/2/jagi.11.issue-2.xml">Read More</a>
