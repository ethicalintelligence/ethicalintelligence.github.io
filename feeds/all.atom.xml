<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ethical Intelligence</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2020-01-08T07:20:00+01:00</updated><entry><title>Tweeting from the Afterlife: Exploring the Deaths of Social Network Members and the Birth of Online Remembrance</title><link href="/tweeting-from-the-afterlife.html" rel="alternate"></link><published>2020-01-08T07:20:00+01:00</published><updated>2020-01-08T07:20:00+01:00</updated><author><name>Robert Seddon</name></author><id>tag:None,2020-01-08:/tweeting-from-the-afterlife.html</id><summary type="html">&lt;p&gt;Tweeting from the Afterlife: Exploring the Deaths of Social Network Members and the Birth of Online Remembrance&lt;/p&gt;</summary><content type="html">&lt;p&gt;Social networks such as Twitter and Facebook are frequent targets of criticism for handling personal data in ways that undermine the privacy and dignity of their users. Twitter recently found itself in the unusual position of receiving criticism for an effort to protect the privacy of user data, by announcing plans to initiate a cull of abandoned accounts on December 11 2019 as part of an effort to comply with EU data privacy regulations.&lt;/p&gt;
&lt;p&gt;Who would miss accounts that had been disused for ages? Quite a few people, it turned out. It wasn’t because lapsed users thought they might start using their accounts again. The accounts most at stake will never tweet again, because they fell silent when their owners died.&lt;/p&gt;
&lt;p&gt;By leaving their thoughts behind on the social network, people had created a presence for themselves that persisted long after death. Twitter suddenly faced objections from people who would visit the preserved thoughts of a departed &lt;a href="https://www.inc.com/jason-aten/twitter-said-it-would-delete-unused-accounts-then-it-realized-some-of-them-belong-to-people-we-want-to-remember.html"&gt;father&lt;/a&gt;, &lt;a href="https://www.bbc.co.uk/news/newsbeat-50584688"&gt;boyfriend&lt;/a&gt; or other loved ones.&lt;/p&gt;
&lt;p&gt;For a site that promotes itself as the place to go to discover ‘what’s happening in the world and what people are talking about right now’ this came as a surprise. In fact, it highlights one of the significant changes in online culture since use of the Internet became widespread. A decade ago, Friendster was a major social network, but usage declined until the site eventually closed, an event foreshadowed by an article in the satirical publication The Onion, which &lt;a href="https://www.theonion.com/internet-archaeologists-find-ruins-of-friendster-civili-1819594871"&gt;spoofed Friendster&lt;/a&gt; as an archaeological site marking a lost civilisation. Abandoned accounts on a social network aren’t just a liability, they reflect the health of the platform, and can be an undesirable signal that platforms might wish to conceal. It’s very natural for a business to value current and potential customers, and lost customers that might yet be regained, but much less to value those that are deceased. How should a social network value users that can no longer use its product? &lt;/p&gt;
&lt;h2&gt;Read-Only Memorials&lt;/h2&gt;
&lt;p&gt;As more and more of us have started living significant parts of our lives online, however, an increasing amount of the content on social platforms has been created by people who are no longer alive. Because of this, we can all expect to have ‘digital afterlives’.&lt;/p&gt;
&lt;p&gt;The sheer scale is remarkable. Take Facebook as an example, which already has a memorialisation feature for deceased users. &lt;a href="https://journals.sagepub.com/doi/full/10.1177/2053951719842540"&gt;Carl Öhman and David Watson project&lt;/a&gt; that billions of Facebook users will have passed away before 2100, by which time ‘the dead may well outnumber the living’. Öhman’s research with Luciano Floridi examines a whole &lt;a href="https://link.springer.com/article/10.1007/s11023-017-9445-2"&gt;digital afterlife industry&lt;/a&gt; dealing with &lt;a href="https://ora.ox.ac.uk/objects/uuid:c059841d-a702-4218-8c1b-39ff49dc6c65/download_file?safe_filename=Ohman_Floridi_R1_edited.pdf&amp;amp;file_format=application%252Fpdf&amp;amp;type_of_work=Journal+article"&gt;online remains&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Firms such as Eterni.me and Replica now offer consumers online chat bots, based on one’s digital footprint, which continue to live on after users die, enabling the bereaved to “stay in touch” with the deceased. This new phenomenon has opened up opportunities for commercial enterprises to monetise the digital afterlife of Internet users. As a consequence the economic interests of these firms are increasingly shaping the presence of the online dead.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In large part this is about how those still living can be helped to feel better and cope with loss—and that’s no small thing. Technology can honour the past as well as building the future. When London’s underground railway upgraded its automatic announcement system, in the process replacing the old ‘Mind the gap’ recording, it emerged that the actor who made it had left behind a widow who still listened for her husband’s voice at her station. Railway staff worked to digitise and restore the recording, securing emotional succour for one woman and over &lt;a href="https://twitter.com/garius/status/1204795961731629058"&gt;forty thousand likes on Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another form of benefit for those still living can come from the rich historical resource which all these online posters have cumulatively created. Öhman and Watson emphasise this aspect, describing the aggregate contributions of social media users as a form of cultural heritage which is of value both to historians and ‘to future generations as part of their record and self-understanding’. They advocate ‘a multi-stakeholder approach’ to the maintenance of this record: a commercial platform like Facebook has an obvious economic interest in the running of its own services, but other interested parties might include ‘states, NGOs, universities, libraries, museums’ and the like. Historic data might someday be handled like historic buildings, as assets that come with special responsibilities for their owners.&lt;/p&gt;
&lt;h2&gt;Mortal Obligations?&lt;/h2&gt;
&lt;p&gt;To say that preserving data of this sort benefits the living, however, isn’t to say that we should understand the value of their digital artifacts entirely in relation to the interests of living people’s wants and needs. We may care not only about doing good for the living, but also about doing the rightly respectful thing for the departed themselves.&lt;/p&gt;
&lt;p&gt;If we believe they are departed, though, to either oblivion or an otherworldly afterlife, then we may be perplexed about how we might be able to treat them well or badly: how they could be, in the philosophical jargon, moral patients. Explanations we might give for moral responsibilities towards sentient beings—explanations involving the capacity to suffer, for example—seem doubtfully applicable towards those who have gone to rest in peace.&lt;/p&gt;
&lt;p&gt;The problem isn’t that we can’t conceive of how there could be any kind of moral patient besides a living, thinking, feeling being. Some philosophers do believe there are other kinds of moral patient, and quite possibly you do as well: if you care about ‘the environment’ then you care for something that, though it incorporates various kinds of sentient organism, isn’t reducible to any of them. The problem is that the conceptual toolkit I’d use in asking, say, what could be wrong with wantonly destroying a fossilised ammonite isn’t a toolkit one can simply go ahead and apply to things left behind by human beings. We don’t relate to that long-dead organism as we do to a dead person.&lt;/p&gt;
&lt;p&gt;So questions are explored about what could make dead people, as a class, qualify as moral patients. Is it possible to harm the dead? Do practices like writing and honouring wills imply that obligations towards the dead person who is are disguised duties to the living one who was, or does that merely restate the paradox in another form?&lt;/p&gt;
&lt;h2&gt;Social Media Absence&lt;/h2&gt;
&lt;p&gt;Twitter was forced to consider dead people as a class among its account-holders, but that’s because of some users’ very specific connections with particular people they’ve lost: with parents and spouses and lovers and friends, people with names and personal histories. Parts of those individual histories linger online. When we remember people through mementos—a portrait, say—our treatment of the objects expresses attitudes towards the people themselves. If you see a social media profile in this way, like a portrait you’d hang in a place of honour, then the point of view from which it’s obsolete clutter in the database is going to be far from what you can personally endorse.&lt;/p&gt;
&lt;p&gt;Öhman and Floridi suggest that we should literally regard the digital material people leave behind as a form of human remains—‘not merely regarded as a chattel or an estate, but as something constitutive of one’s personhood’—and should draw on archaeological ethics to identify the principles behind respectful display of them. It’s unclear how far existing archaeological ethics will take us towards working out whether anyone has an obligation to fund the ongoing display, however; the Internet has nothing analogous to reburial.&lt;/p&gt;
&lt;p&gt;Some writers on the ethics of heritage and human remains contrast a materialist, empiricist West, which thinks of being dead as being gone, with different societies in which the dead are understood to have an ongoing presence in the life of a community. &lt;a href="http://www.piotrbienkowski.co.uk/"&gt;Piotr Bienkowski&lt;/a&gt;, for example, has written that scepticism about connections with people from former times arises from a view of the world that regards the dead ‘as no longer existing or having personhood in any sense’. If that’s how the West truly thinks, though, then perhaps our technological society is developing so that we start to think differently.&lt;/p&gt;
&lt;p&gt;It is now common for some of our most significant relationships to take place entirely online. If we encounter people through their online presence while they live to update it, maybe it’s not so strange a notion that something of them remains present while it persists on the servers. We should bear in mind, though, that for many people, their  online personae are not authoritative self-portraitures but often impressionistic or playful takes on themselves, sometimes multiple masques that relate only part of their personalities to specific audiences. We should perhaps be careful about these subtleties when memorializing online self-expression, and not take it more seriously in death than the mind behind it did in life. Nevertheless, even an outright parody account can be fondly regarded as part of a community. It leaves a hole in that community when the posting suddenly ceases, and can retain its place of honour in the ‘social graph’ of friend or follower relationships.&lt;/p&gt;
&lt;h2&gt;Last Words&lt;/h2&gt;
&lt;p&gt;In ten years we’ve gone from seeing Friendster’s decline spoofed as an archaeological dig to Öhman and Watson’s serious proposal that Internet hosts are custodians of a form of human cultural heritage. And as stories about bereavement go, Twitter’s experience carries a heartwarming moral: what seemed to be a load of disused data was in fact a memorial to the dead and is actually giving living people reasons to keep coming back to the site.&lt;/p&gt;
&lt;p&gt;Twitter was taken aback by the discovery that deceased people can still be part of its community. This is not the only form online remembrance can take—you may even have had a secret memorial sent to you in &lt;a href="https://xclacksoverhead.org/home/about"&gt;HTTP headers&lt;/a&gt; — but it’s a form that underlines how those content posters who’ve passed on can still have significant social and therefore ethical relationships with living people: relationships that matter to those people in ways that foster a deep commitment to keeping them going.&lt;/p&gt;
&lt;p&gt;Digital afterlives are a recent and growing source of questions for industry and policy—and Twitter will still be working out how best to square them with EU privacy rules. These are the early stages of working out how to handle personal grief on the public Internet: who shall bear the costs, and when it’s acceptable for the ‘digital afterlife industry’ to explore the opportunities. Will this lead to the involvement of religious bodies, such as the Vatican, when it comes to the preservation of digital afterlives? For tech companies already under the spotlight, approaching these questions with ethical sensitivity will be part of securing their own survival. For regulators and ethicists, this is an important issue that problematises recent approaches to privacy and data rights. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 17-24</title><link href="/ai-ethics-news-roundup-dec17-dec24.html" rel="alternate"></link><published>2019-12-24T09:20:00+01:00</published><updated>2019-12-24T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-24:/ai-ethics-news-roundup-dec17-dec24.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 17 - Dec 24&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://twitter.com/random_walker/status/1208050796476215296"&gt;A great tweet from Arvind Narayanan&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/random_walker"&gt;Arvind Narayanan&lt;/a&gt; writes:&lt;/p&gt;
&lt;p&gt;"If you think there's too much yelling about algorithmic bias, here's an analogy. By the mid 90s the privacy community knew there was a huge problem. But it took two decades of yelling and a million privacy disasters before the public and policy makers started taking it seriously."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.youtube.com/watch?v=dEFI05Gtalc"&gt;Persons yet Unknown: Animals, Chimeras, Artificial Intelligence and Beyond&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/grok_"&gt;Kate Darling&lt;/a&gt; and &lt;a href="https://twitter.com/PKathrani"&gt;Paresh Kathrani&lt;/a&gt; in an fascinating discussing of robotics and AI and The Animal Law Conference. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://scholarship.law.duke.edu/dltr/vol18/iss1/17/"&gt;Implementing Ethics Into Artificial Intelligence: A Contribution, From A Legal Perspective, To The Development Of An Ai Governance Regime&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a new article added to a great issue of the Duke Law and Technology Review that came out in August, a &lt;a href="https://scholarship.law.duke.edu/dltr/vol18/iss1/"&gt;Symposium for John Perry Barlow&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;"This Article advocates for the need to conduct in-depth risk-benefit-assessments with regard to the use of AI and autonomous systems. This Article points out major concerns in relation to AI and autonomous systems such as likely job losses, causation of damages, lack of transparency, increasing loss of humanity in social relationships, loss of privacy and personal autonomy, potential information biases and the error proneness, and susceptibility to manipulation of AI and autonomous systems. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://thenextweb.com/artificial-intelligence/2019/12/17/researchers-were-about-to-solve-ais-black-box-problem-then-the-lawyers-got-involved/"&gt;Researchers were about to solve AI’s black box problem, then the lawyers got involved&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When things go wrong and AI runs amok, the lawyers will be there to tell us the most company-friendly version of what happened. Most importantly, they’ll protect companies from having to share how their AI systems work.&lt;/p&gt;
&lt;p&gt;We’re trading a technical black box for a legal one. Somehow, this seems even more unfair.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://deepai.org/publication/explainability-and-adversarial-robustness-for-rnns"&gt;Explainability and Adversarial Robustness for RNNs&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a common notion of adversarial robustness, and show that an adversarial training procedure can significantly and successfully reduce the attack surface."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.nature.com/articles/d41586-019-03895-5"&gt;This AI researcher is trying to ward off a reproducibility crisis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Joelle Pineau doesn’t want science’s reproducibility crisis to come to artificial intelligence (AI).&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.forbes.com/sites/sap/2019/12/18/ethics-in-ai/#2d7dd5285af4"&gt;Ethics In AI: Why Values For Data Matter &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;An argument for better corporate governance around AI and data. Corporations should "treat data as an asset .... the same way organizations treat inventory, fleet, and manufacturing assets. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.capgemini.com/research/the-virtuous-circle-of-trusted-ai-turning-ethical-and-transparent-ai-into-a-competitive-advantage-luciano-floridi/"&gt;The Virtuous Circle of Trusted AI: Turning Ethical and Transparent AI Into a Competitive Advantage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A nice short piece from &lt;a href="https://twitter.com/Floridi"&gt;Luciano Floridi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Most large organizations today across the United States and Europe are talking about “duty of care” and AI (i.e. the duty to take care to refrain from causing another person injury or loss). We also hear a lot about the need for clear normative frameworks in areas such as driverless cars, drones, facial recognition, and algorithmic decisionmaking guidelines in public-facing services such as banking or retail. I shall be surprised if we will have this conversation again in two years’ time and legislation hasn’t already been seriously discussed or put in place."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 10-17</title><link href="/ai-ethics-news-roundup-dec10-dec17.html" rel="alternate"></link><published>2019-12-17T09:20:00+01:00</published><updated>2019-12-17T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-17:/ai-ethics-news-roundup-dec10-dec17.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 10 - Dec 17&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://hai.stanford.edu/news/introducing-ai-index-2019-report"&gt;AI Index 2019 Report&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;An independent initiative within Stanford University’s Human-Centered Artificial Intelligence Institute, the report is in its third year and is the result of a collaborative effort led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry, in collaboration with more than 35 sponsoring partners and data contributors. The purpose of the project is to ground the discussion on AI in data, serving practitioners, industry leaders, policymakers and funders, the general public and the media that informs it. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;When should we decline to write code? A small case study.&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We picked this up on twitter when Emily Bender &lt;a href="https://twitter.com/ethicalai_co/status/1202638293269176321"&gt;tweeted&lt;/a&gt; that there was a task in an AI competition to create an AI that would solve problems involving the  "Prediction of Intellectual Ability and Personality Traits from Text". She's since &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;posted a thoughtful followup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an important problem. Technical and regulatory solutions should be augmented by professional codes of conduct and ethics if we want to ensure the safe and fair development of AI. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.newsweek.com/amazon-health-care-jeff-bezos-telemedicine-1475154"&gt;Do You Trust Jeff Bezos With Your Life? Tech Giants Like Amazon Are Getting into the Health Care Business&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Would you trust the Tech Giants with your health data in exchange for more personalized and on-demand healthcare? This article covers the current initiative of telehealth by Amazon and dives into a few key implications that this new commodity would carry for society at large.&lt;/p&gt;
&lt;p&gt;"What health insurance companies, as well as employers who foot the bulk of the U.S.'s health care bill, especially fear from telehealth is that it's so easy to use that people will reach out more often for care. "It creates the risk that every little ache and pain results in a claim that has to be paid out," says the University of Pennsylvania's Asch. "Making people come into the office is health care rationing by inconvenience."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.axios.com/ai-bias-c7bf3397-a870-4152-9395-83b6bf1e6a67.html"&gt;A tug-of-war over biased AI&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"A critical split divides AI reformers. On one side are the bias-fixers, who believe the systems can be purged of prejudice with a bit more math. (Big Tech is largely in this camp.) On the other side are the bias-blockers, who argue that AI has no place at all in some high-stakes decisions."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.bbc.co.uk/news/technology-50761116"&gt;Emotion-detecting tech should be restricted by law&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The AI Now Institute says the field is "built on markedly shaky foundations".
Despite this, systems are on sale to help vet job seekers, test criminal suspects for signs of deception, and set insurance prices. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.nytimes.com/2019/12/13/opinion/robot-caregiver-aging.html"&gt;Would you let a Robot Take Care of Your Mother? &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;AI usage for social care is not a new concept. However, as it becomes more and more of a reality, we are forced to shift our questions from theoretical to personal.&lt;/p&gt;
&lt;p&gt;"Some worry robot care would carry a stigma:the potential of being seen as “not worth human company,” said one participant in a study of potential users with mild cognitive impairments."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.cigionline.org/articles/owning-intelligence"&gt;Owning Intelligence&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The United States Patent and Trademark Office is trying to answer a very complicated question: who owns artificial intelligence?&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/48_aisg_neurips2019.pdf"&gt;AI Ethics for Systemic Issues: A Structural Approach&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"This paper calls for a "structural" approach to assessing AI’s effects inorder to understand and prevent such systemic risks where no individual can beheld accountable for the broader negative impacts. This is particularly relevantfor AI applied to systemic issues such as climate change and food security whichrequire political solutions and global cooperation. To properly address the widerange of AI risks and ensure ’AI for social good’, agency-focused policies must becomplemented by policies informed by a structural approach."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 2 - Dec 9</title><link href="/ai-ethics-news-roundup-dec-9.html" rel="alternate"></link><published>2019-12-09T09:20:00+01:00</published><updated>2019-12-09T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-09:/ai-ethics-news-roundup-dec-9.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 2 - Dec 9&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://arxiv.org/abs/1912.00761"&gt;On the Legal Compatibility of Fairness Definitions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Although the article was on arxiv last week, the author publicized it this week, and it's a good one. There's poor alignment between operationalized definitions of fairness in machine learning and the legal definitions that may in fact apply to the deployment of these systems. &lt;/p&gt;
&lt;p&gt;"Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex socio-technical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts, as well as what both the legal and ML fairness communities can learn from these tensions. We focus this paper on U.S. anti-discrimination law since the ML fairness research community regularly references terms from this body of law."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.benzevgreen.com/wp-content/uploads/2019/11/19-ai4sg.pdf"&gt;'Good' isn’t good enough&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A critical reflection on the problems that arise when the pursuit of good is taken on as a technical objective too hastily, and why sustained and rigorous ethical reflection is a necessary if we want to have any confidence that such efforts will actually succeed. &lt;/p&gt;
&lt;p&gt;"Despite widespread enthusiasm among computer scientists to contribute to “socialgood,” the field’s efforts to promote good lack a rigorous foundation in politicsor social change. There is limited discourse regarding what “good” actuallyentails, and instead a reliance on vague notions of what aspects of society aregood or bad. Moreover, the field rarely considers the types of social changethat result from algorithmic interventions, instead following a “greedy algorithm”approach of pursuing technology-centric incremental reform at all points."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/arthur-ai/uk-gpdr-watchdog-says-explain-your-ai-373ef76d3c"&gt;New guidelines on the GDPR Right to Explanation&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The UK’s Data Protection Authority just issued much-anticipated guidance that clarifies the complicated issue of the GDPR’s ‘right to explanation’. Here is some background on the issue and what the new information means.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.linkedin.com/pulse/ethics-objective-reid-blackman-ph-d-/?trackingId=B5NT8Dd1BMx16FH15vHvZQ%3D%3D"&gt;Ethics is Objective&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Here are three arguments for the idea that ethics is subjective, presented with thoughtful rebuttals. This is a theme we took up in our last bog post, where we argued that there is a very large chunk of territory in tech ethics where ethical imperatives can be uncovered and agreed upon by sincere inquiry, even by those who disagree on more fundamental ethical and moral questions. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.academia.edu/41168605/Online_information_of_vaccines_information_quality_is_an_ethical_responsibility_of_search_engines"&gt;Online information of vaccines: information quality is an ethical responsibility of search engines&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When health-related disinformation is available online, who is responsible? There's a growing backlash against the idea of platforms as "mere tools", but perhaps we should think the same of search engines. We don't usually think that a library is responsible for dangerous information in its books, but should we think differently about Google?&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://deepai.org/publication/recovering-from-biased-data-can-fairness-constraints-improve-accuracy"&gt;Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Multiple fairness constraints have been proposed in the literature, motivated by a range of concerns about how demographic groups might be treated unfairly by machine learning classifiers. In this work we consider a different motivation; learning from biased training data. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://policyreview.info/concepts/datafication"&gt;Datafication&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the course of researching and discussing AI ethics challenges, we might run across the claim while the rate and scope of our generation of data has increased, it can be understood on a continuum with the ways in which human activity have always left traces and records. This article on the concept of "datafication" argues against this, and shows several ways to understand what is distinctive about the new systems and actors that collect and use our data. &lt;/p&gt;
&lt;p&gt;"Datafication is not just the making of information, which, in one sense, human beings have been doing since the creation of symbols and writing. Rather, datafication is a contemporary phenomenon which refers to the quantification of human life through digital information, very often for economic value. This process has major social consequences. Disciplines such as political economy, critical data studies, software studies, legal theory, and—more recently— decolonial theory, have considered different aspects of those consequences to be important. Fundamental to all such approaches is the analysis of the intersection of power and knowledge. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.techuk.org/insights/news/item/16447-welcome-to-techuk-digital-ethics-week"&gt;This is techUK Digital Ethics Week&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="https://www.thetimes.co.uk/article/amazon-ready-to-cash-in-on-free-access-to-nhs-data-bbzp52n5m"&gt;Amazon ready to cash in on free access to NHS data&lt;/a&gt;&lt;/h2&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Nov 24 - Dec 1</title><link href="/ai-ethics-news-roundup-nov-24-dec-1.html" rel="alternate"></link><published>2019-12-02T09:20:00+01:00</published><updated>2019-12-02T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-02:/ai-ethics-news-roundup-nov-24-dec-1.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Nov 24 - Dec 1&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to our first EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;The Second Wave of Algorithmic Accountability&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"... the first wave of algorithmic accountability focuses on improving existing systems, a second wave of research has asked whether they should be used at all—and, if so, who gets to govern them".&lt;/p&gt;
&lt;p&gt;Frank Pasquale argues that we can distinguish the "... first wave of algorithmic accountability research and activism", which has targeted existing systems and helped illuminate urgent ethical concerns in the AI systems already online, from "...an emerging “second wave” of algorithmic accountability has begun to address more structural concerns.". &lt;/p&gt;
&lt;p&gt;"Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology". &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://juliacomputing.com/blog/2019/11/22/encrypted-machine-learning.html"&gt;Machine Learning on Encrypted Data Without Decrypting It&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Recent breakthroughs in cryptography have made it practical to perform computation on data without ever decrypting it. In our example, the user would send encrypted data (e.g. images) to the cloud API, which would run the machine learning model and then return the encrypted answer. Nowhere was the user data decrypted and in particular the cloud provider does not have access to either the orignal image nor is it able to decrypt the prediction it computed."&lt;/p&gt;
&lt;p&gt;This application of homomorphic encryption systems might mitigate a number of data protection problems, even though it would still be imperative to ensure the data was collected ethically, and that the data itself is free from biases that are not understood and accounted for. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.wired.com/story/tainted-data-teach-algorithms-wrong-lessons/"&gt;Tainted Data Can Teach Algorithms the Wrong Lessons&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Security problems become ethics problems when vulnerabilities in software systems produce risks in their application that the stakeholders (both users, and the people in the social environment) are unable to understand, assess, and freely and knowingly accept. &lt;/p&gt;
&lt;p&gt;Adversarial input is a particularly powerful way to undermine machine learning systems and to cause them to behave in unexpected and unintended ways. "“Current deep-learning systems are very vulnerable to a variety of attacks, and the rush to deploy the technology in the real world is deeply concerning,” says Cristiano Giuffrida, an assistant professor at VU Amsterdam who studies computer security, and who previously discovered a major flaw with Intel chips affecting millions of computers."&lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://link.springer.com/article/10.1007%2Fs13347-019-00354-x"&gt;recent paper&lt;/a&gt; Luciano Floridi draws our attention to the extension of the practice of ethics dumping, "the export of unethical research practices to countries where there are weaker legal and ethical frameworks" into the digital realm. This is an ethical risk, but it is also a more basic risk to the quality of research. This article argues that there are also security problems with this sort of practice. "... some companies outsource the training of their AI systems, a practice known as machine learning as a service. This makes it far harder to guarantee that an algorithm has been developed securely."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.tandfonline.com/doi/full/10.1080/21670811.2019.1685395"&gt;Algorithms, Automation, and News&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A special issue of the Journal of Digital Journalism has been published on "Algorithms, Automation, and News".&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.halfwaytothefuture.org/programme/mcmillan-against-ethical-ai-guidelines-and-self-interest"&gt;Against Ethical AI: Guidelines and Self Interest&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"In this paper we use the EU guidelines on ethical AI, and the responses to it, as a starting point to discuss the problems with our community's focus on such manifestos, principles, and sets of guidelines. We cover how industry and academia are at times complicit in ‘Ethics Washing’, how developing guidelines carries the risk of diluting our rights in practice, and downplaying the role of our own self interest. We conclude by discussing briefly the role of technical practice in ethics."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/"&gt;5 Q’s for Anne Kao, Senior Technical Fellow at Boeing Research and Technology&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The Center for Data Innovation spoke with Anne Kao, Senior Technical Fellow at Boeing Research and Technology. Kao discussed how she uses machine learning to analyze maintenance reports and how philosophy influences how she approaches data science."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O"&gt;Ukraine denounces Apple for calling Crimea part of Russia in apps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Reuters reporters in Moscow who typed the name of the Crimean provincial capital Simferopol into Apple’s Maps and Weather apps on Wednesday saw it displayed as “Simferopol, Crimea, Russia”. Users elsewhere — including in Ukraine’s capital Kiev and in Crimea itself — see locations in Crimea displayed without specifying which country they belong to. "&lt;/p&gt;
&lt;p&gt;One might wonder if the technical specifications in the ticket for the engineering work that was involved discussed the political and ethical implications. It's perhaps difficult to imagine that something of this magnitude wasn't remarked upon, but on the other, so much engineering work in software proceeds as though it happens in at least partial isolation from the downstream social and political environment.&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Falsehoods Programmers Believe About Ethics</title><link href="/falsehoods-programmers-believe-about-ethics.html" rel="alternate"></link><published>2019-12-02T07:20:00+01:00</published><updated>2019-12-02T07:20:00+01:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2019-12-02:/falsehoods-programmers-believe-about-ethics.html</id><summary type="html">&lt;p&gt;Some thoughts about the interface between programmers and tech ethics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a Tech Ethicist, I come into contact with programmers almost daily, as my work requires me to navigate the technical objectives and implementation details in addition to the ethical dimensions of the projects I analyze. Through these experiences, I’ve found myself having some of the same conversations over and over, all surrounding the applicability of ethics in the tech industry. There are some persistent misconceptions about both the study of ethics and its application to technical work. In order to help bring clarity to the confusion surrounding ethics in tech, I’m going to briefly discuss three significant misconceptions I've observed. &lt;/p&gt;
&lt;h2&gt;There is one right answer &lt;br /&gt; &lt;em&gt;(or no right answers, if you take a subjective approach)&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Ethics is the study of right and wrong, so shouldn’t there be a right and wrong answer to every ethical question? If there was, then the ethical dilemmas programers and technologists face would have long been answered by general principles with universal applicability across projects, cultures, and domains. Of course, if this were true, the daily news would not include so many stories involving ethical lapses or miscalculations where technological platforms, tools, and solutions cause serious harms. We are all familiar with the headline stories, self-driving cars making errors that human drivers would not, autonomous weapons systems being developed and deployed by militaries, banks and governments using AI driven gatekeepers to control access to credit and social services, and fintech companies making trading decisions at the speed of light, far faster than humans can intervene. These are all headline worthy examples of tech gone wrong due to ethical lapses. But, when we take a step back from these bigger stories, we also discover the points in which software engineers and technologists are at risk for the same ethical lapses, as they make decisions every day in the course of their work that can have real ethical impacts. ‘Do I use a social sign-on SDK, because it’s faster, even though there are privacy tradeoffs? Do we let a test suite languish to save time? Do we optimize for the biggest curve, even if that means missing important edge cases? Is the dataset we are pre-training an algorithm with fair, and ethically sourced?’&lt;/p&gt;
&lt;p&gt;It might seem like there are only two possible solutions to these ethical dilemmas. Either we finally discover this elusive single, straightforward, universal guideline that can be applied across the board, or, if that doesn’t exist, then we must throw up our hands in defeat because ethics is too subjective and we will never arrive at an acceptable single answer.  Why is it that these seem to be our only two possible solutions to solving tech ethics problems? Well, we all come from different backgrounds, which in turn shape our understanding of what constitutes right and wrong. Everything from our culture, our education, and our life experiences can potentially influence how we approach ethical problems. Because of this, being aware of, and finding solutions to the ethical dilemmas raised by technology requires careful navigation of the multiple understandings of right and wrong at play. To complicate matters further, ethics is an emotionally charged topic. We are very attached to our personal ethical frameworks, and are hurt when it is threatened or violated. And yet, there are situations in which two people can have opposing ethical frameworks, and still both arrive at what could be considered a right answer in accordance to those individual frameworks.  The combination of this emotional attachment along with the varying ethical frameworks results in the feeling that the right answer to our ethical dilemmas is impossible to find. &lt;/p&gt;
&lt;p&gt;However, I would argue otherwise, as there are significant overlaps in the ethical frameworks between individuals. This overlap in turn enables us to uncover an understanding of the problems we face and from there make informed decisions. What I'm suggesting is that we don't need to solve the fundamental metaethical dilemmas that philosophers have been arguing about for millennia in order to appreciate and mitigate the ethical risks that technical decisions expose us to. Often an informed ethical awareness is the most significant step towards improving our ability to foresee and to mitigate the ethical risks in our technical practices. &lt;/p&gt;
&lt;h2&gt;It is possible to eliminate ethical bias&lt;/h2&gt;
&lt;p&gt;Algorithmic bias has become a buzzword with a strong negative connotation which results in  often hearing that we need to eliminate bias in our algorithms. However, when it comes to bias in terms of ethical decision making, it is not actually possible to completely eliminate that bias. This is because data is not neutral, it reproduces the biases in the world it comes from, as well as new biases introduced knowingly or accidentally through collection, processing and use. When we try to mitigate bias in data, we must often confront real ethical dilemmas from the world it came from, and make our best efforts to address them. Confronting these dilemmas, or, in other words, attempting to mitigate the biases inherent in the data, involves taking a stance of our own, which is a sort of bias itself. So, in this sense, ethical bias is impossible to fully eliminate, which makes it something we should take on as a serious responsibility.&lt;/p&gt;
&lt;p&gt;Let’s explore this. You’re working for a university, collecting simple data on students such as class attendance, grades, library usage, when you’re asked to develop specific insight on student mental health out of this data. You of course know to clean the data and set controls to mitigate any previous biases as a first step. However, in order to move on to creating the algorithm that will allow you to draw insights into the mental health status of students, you must first take a biased stance on ethics. You either have to decide to create an algorithm that will track student mental health with the hopes of providing care and wellbeing for the greatest amount of people, or decide that this would be a violation of respecting the dignity of an individual, and so not create the algorithm.  In other words, your decision between the two equally as important ethical values depends on your own bias towards those values. Since there is no way to honor both values equally in this situation, a decision must be taken by prioritizing one value over the other, and the ethical bias cannot be avoided. It is also important to note that in this example, the ethical dilemma surfaces only after the info from the data is extracted. By extracting this knowledge on the mental status of students, an ethical imperative to help the identified students is created. This all goes to show, collecting knowledge from datasets is not an ethically neutral task. 
Programmers need to be acutely aware of what ethical values are at play and how they are prone to prioritize these values. It's easy to get lost in the weeds solving architectural and implementation problems, but even these ground level decisions can have ethical impacts we must be aware of. Choices of technologies, datasets, integration partners, and problem definitions all expose the ethical edges of technical development. At the macro level, it's important to take a step back, and make sure we have articulated and examined the values that have guided and shape the formulation of business and technical goals, so that the ways these values interface with other values in the world in which our systems are deployed are understood and made clear. It is therefore essential to to build this kind of ethical awareness into organizational culture and technical decision-making. &lt;/p&gt;
&lt;h2&gt;Ethics is a blocker to innovation&lt;/h2&gt;
&lt;p&gt;Is ethics someone else's problem? Surely the software’s only responsibility is to is to work, not understand ethics. This is a view I often hear when speaking with programmers, as they express an underlying fear of having constraints placed on their previously unencumbered freedom to develop their technology. Ethics can appear to be a blocker if it is viewed as just another piece of paperwork that needs to be filed or the opportunity for someone from a non-technical background who just doesn’t get ‘it’ to kill an interesting project before it’s had the chance to get off the ground. &lt;/p&gt;
&lt;p&gt;However, there’s another perspective. You can think about tech ethics in much the same way programmers are accustomed to thinking about technical debt. Making short term architectural decisions to meet immediate needs involves taking on a notion of "well, it works" that is dangerously myopic. Most programmers are intimately familiar with the exponential accumulation of technical debt, and the astounding difficulty in mitigating it, when working in an environment that doesn’t value longer term thinking. Ethics is the same way. Tiny shortcuts in seemingly insignificant systems can have real effects on people’s lives. In a recent project we looked at, a contractor working on the logged-out view of a particular web application state left in personally identifying information that could be used to unmask users in ways at risk to be that could be abused into pinpointing places and times they had attended particular events. The contractor satisfied the requirements they were given, the test suite didn’t check this, QA didn’t, and a higher level-commitment to recognizing the failure modes that mishandling the unique data this company held had never been made.&lt;/p&gt;
&lt;p&gt;Building in ethical awareness from the ground up is an asset, not a liability. If we go about developing our technology ethically, we are innovating for long-term sustainability, not short-term profit. Whereas, if we “move fast and break things”, without ethical considerations, then it is only a matter of time before consequences arise that are difficult or impossible to unwind. When we break things, people get hurt. There are financial, social, legal, and moral costs involved. Technical innovation needs ethics (and technical advancement likewise allows use to make ethical advances), because at the end of the day, we as humans try to do the right thing, and expect our technology to do the same. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry></feed>