<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ethical Intelligence</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2020-06-16T17:00:00+02:00</updated><entry><title>Weekly AI Ethics News Roundup: June 9 - June 16</title><link href="/ai-ethics-news-roundup-june03-june09.html" rel="alternate"></link><published>2020-06-16T17:00:00+02:00</published><updated>2020-06-16T17:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-06-16:/ai-ethics-news-roundup-june03-june09.html</id><summary type="html">&lt;p&gt;@vdignum @MullerCatelijne @RecklessCoding analyze EU AI whitepaper at @ALLAI_EU @inafried COVID-19 generating AI bias @karenkho big tech's facial recognition policy @Graphika_NYC info-operations on 300 tech platforms @ProfFerguson police surveillance tech @brookingsinst robustness key to AI @RosariaTaddeo #covid19 digital governance + much more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Fresh concerns about AI bias in the age of COVID-19&lt;/h2&gt;
&lt;p&gt;"Businesses facing unprecedented demands during the coronavirus pandemic have boosted their use of artificial intelligence in some of society's most sensitive areas."&lt;/p&gt;
&lt;p&gt;"Why it matters: Algorithms and the data they rely on are prone to automating preexisting biases — and are more likely to do so when they're rushed into the field without careful testing and review."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.axios.com/fresh-concerns-about-ai-bias-in-the-age-of-covid-19-70288389-8398-4941-ae5e-42f939f65f4a.html?utm_campaign=organic&amp;utm_medium=socialshare&amp;utm_source=twitter"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;IBM will no longer offer, develop, or research facial recognition technology&lt;/h2&gt;
&lt;p&gt;"IBM will no longer offer general purpose facial recognition or analysis software, IBM CEO Arvind Krishna said in a letter to Congress today. The company will also no longer develop or research the technology, IBM tells The Verge. Krishna addressed the letter to Sens. Cory Booker (D-NJ) and Kamala Harris (D-CA) and Reps. Karen Bass (D-CA), Hakeem Jeffries (D-NY), and Jerrold Nadler (D-NY)."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The two-year fight to stop Amazon from selling face recognition to the police&lt;/h2&gt;
&lt;p&gt;"This week’s moves from Amazon, Microsoft, and IBM mark a major milestone for researchers and civil rights advocates in a long and ongoing fight over face recognition in law enforcement."&lt;/p&gt;
&lt;p&gt;"The cynical part of me says Amazon is going to wait until the protests die down...to revert to its prior position."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/06/12/1003482/amazon-stopped-selling-police-face-recognition-fight/?truid=9492a43e68469586573ba1202bf74bc8&amp;utm_source=the_algorithm&amp;utm_medium=email&amp;utm_campaign=the_algorithm.unpaid.engagement&amp;utm_content=06-12-2020"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Final Analysis of the EU Whitepaper on AI&lt;/h2&gt;
&lt;p&gt;"The White Paper is the European Commission’s first concrete attempt at discussing AI  policy beyond the  high-level  statements  of previous Communications.In  this sense, the Commission takes up a rule setting role(rather than a referee role). In our opinion, this is a good first step. "&lt;/p&gt;
&lt;p&gt;"In its Whitepaper on Artificial Intelligence, Europe took a clear stance on AI;  foster  uptake  of  AI  technologies,  underpinned  by  what  it  calls  ‘an ecosystem  of  excellence’,  while  also  ensuring  their  compliance  with  to European   ethical   norms,   legal   requirements   and   social   values,   ‘an ecosystem   of   trust’.   While   the   Whitepaper   on   AI   of   the   European Commission  does  not  propose  legislation  yet,  it  announces  some  bold legislative measures, that will likely materialize in the beginning of 2021."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://allai.nl/wp-content/uploads/2020/06/ALLAI-Final-Analysis-of-the-EU-Whitepaper-on-AI-consultation.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Joanna Bryson: Regulating AI as a pervasive technology: My response to the EU AI Whitepaper Consultation&lt;/h2&gt;
&lt;p&gt;"Basically it's a good document heading the right direction."&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation is actually easy / AI is never necessarily opaque&lt;/li&gt;
&lt;li&gt;Humans are always responsible, AI can only be transparent. &lt;/li&gt;
&lt;li&gt;AI isn't really some weird byproduct of data (the Rumpelstilzchen fallacy). Computation and cybersecurity are more essential than data storage for any length of time.&lt;/li&gt;
&lt;li&gt;AI is produced by programmers, so the EU needs to add program code, architecture documents and specifications to the list of documents they require companies to be able to produce for inspection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Trust and excellence — the EU is missing the mark again on AI and human rights&lt;/h2&gt;
&lt;p&gt;"The European Commission’s consultation on the “White Paper on Artificial Intelligence — a European approach to excellence and trust” is closing on Sunday, June 14. In its current form, the policy approach of the EU will bring about neither trust nor excellence in automated decision-making (ADM) / artificial intelligence (AI) systems, and will do nothing to ensure that both the private and public sector respect and promote human rights in the context of artificial intelligence."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.accessnow.org/trust-and-excellence-the-eu-is-missing-the-mark-again-on-ai-and-human-rights/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Forgeries, interference, and attacks on Kremlin critics across six years and 300 sites and platforms.&lt;/h2&gt;
&lt;p&gt;"Secondary Infektion is a series of operations run by a large-scale, persistent threat actor from Russia that worked in parallel to the Internet Research Agency and the GRU but was systematically different in its approach."&lt;/p&gt;
&lt;p&gt;"The campaign used fake accounts and forged documents to sow conflict between Western countries and most often targeted Ukraine. It produced at least 2,500 pieces of content in seven languages across over 300 platforms from 2014 into 2020."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://secondaryinfektion.org/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;High-tech surveillance amplifies police bias and overreach&lt;/h2&gt;
&lt;p&gt;"Police use of these national security-style surveillance techniques – justified as cost-effective techniques that avoid human bias and error – has grown hand-in-hand with the increased militarization of law enforcement. Extensive research, including my own, has shown that these expansive and powerful surveillance capabilities have exacerbated rather than reduced bias, overreach and abuse in policing, and they pose a growing threat to civil liberties. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/high-tech-surveillance-amplifies-police-bias-and-overreach-140225"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why robustness is key to deploying AI&lt;/h2&gt;
&lt;p&gt;"The takeaway for policymakers—at least for now—is that when it comes to high-stakes settings, machine learning (ML) is a risky choice. “Robustness,” i.e. building reliable, secure ML systems, is an active area of research. But until we’ve made much more progress in robustness research, or developed other ways to be confident that a model will fail gracefully, we should be cautious in relying on these methods when accuracy really matters."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/techstream/why-robustness-is-key-to-deploying-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Fair Is Zoom Justice?&lt;/h2&gt;
&lt;p&gt;"Court hearings are going virtual in response to COVID-19. Studies show they can lead to harsher outcomes for defendants."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://onezero.medium.com/how-fair-is-zoom-justice-4c1169195fb4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Digital inclusion and data literacy&lt;/h2&gt;
&lt;p&gt;ICYMI: Special Issue of Internet Policy Review&lt;/p&gt;
&lt;p&gt;"As more of our everyday lives become digital, it has become crucial to include everyone in the digital society. This special issue is examining the different layers of digital inclusion and data literacy by drawing on research, policy, and practice developments around literacies in various regions and contexts. It highlights the politics around them so as to propose policies that are needed to include more people in datafied societies, and what types of literacies they should learn. This issue includes three commentaries by experts in the field and five peer-reviewed academic papers that go towards tackling digital inclusion. This means to find solutions to the fact that many people are left behind technological advancements, and that these create what is commonly called - the digital divide. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://policyreview.info/digital-inclusion"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Point and Counterpoint on Robot Rights&lt;/h2&gt;
&lt;p&gt;NOEMA published &lt;a href="https://www.noemamag.com/a-misdirected-application-of-ai-ethics/"&gt;A Misdirected Application Of AI Ethics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"The debate about robot rights diverts moral philosophy away from the pressing matter of the oppressive use of AI technology against vulnerable groups in society."&lt;/p&gt;
&lt;p&gt;@eripsa collected &lt;a href="https://twitter.com/eripsa/status/1272181435307360257"&gt;critical responses on twitter&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data &amp;amp; Dystopia&lt;/h2&gt;
&lt;p&gt;New Podcast: &lt;/p&gt;
&lt;p&gt;"Europe is increasingly caught between upholding privacy of citizens and promoting intrusive artificial intelligence. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://euscream.com/data-dystopia/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Ethical Governance of the Digital During and After the COVID‑19 Pandemic&lt;/h2&gt;
&lt;p&gt;Mariarosaria Taddeo's editor's letter for Minds &amp;amp; Machines on the ethical governance of digital technologies during and after the COVID pandemic.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/content/pdf/10.1007/s11023-020-09528-5.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://joanna-bryson.blogspot.com/2020/06/regulating-ai-as-pervasive-technology.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Transparency in Language Generation: Levels of Automation&lt;/h2&gt;
&lt;p&gt;"Language models and conversational systems are growing increasingly advanced, creating outputs that may be mistaken for humans. Consumers may thus be misled by advertising, media reports, or vagueness regarding the role of automation in the production of language. We propose a taxonomy of language automation, based on the SAE levels of driving automation, to establish a shared set of terms for describing automated language. It is our hope that the proposed taxonomy can increase transparency in this rapidly advancing field. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/2006.06295"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Czech civil society fights back against fake news&lt;/h2&gt;
&lt;p&gt;"In the Czech Republic, the media ecosystem is plagued by disinformation. A group of PR professionals have teamed up to cut off dodgy outlets from their main, and often only, source of income — online ads."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.dw.com/en/czech-civil-society-fights-back-against-fake-news/a-53758412"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Ethical Balance of Using Smart Information Systems for Promoting the United Nations’ Sustainable Development Goals&lt;/h2&gt;
&lt;p&gt;"SIS have the potential to exacerbate inequality and further entrench the market dominance of big tech companies, if left uncontrolled. "&lt;/p&gt;
&lt;p&gt;"The paper explores how technology can be used to address the SDGs and in particular Smart Information Systems (SIS). SIS, the technologies that build on big data analytics, typically facilitated by AI techniques such as machine learning, are expected to grow in importance and impact. Some of these impacts are likely to be beneficial, notably the growth in efficiency and profits, which will contribute to societal wellbeing. At the same time, there are significant ethical concerns about the consequences of algorithmic biases, job loss, power asymmetries and surveillance, as a result of SIS use." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.mdpi.com/2071-1050/12/12/4826"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Privacy-preserving A.I. is the future of A.I.&lt;/h2&gt;
&lt;p&gt;"...But researchers have shown that this kind of anonymization doesn’t guarantee privacy: There are often other fields in data, such as location, age, or occupation, that might allow you to re-identify an individual, especially if you are able to cross-reference it with another dataset that does include personal information."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://fortune.com/2020/06/16/privacy-preserving-a-i-is-the-future-of-a-i/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;From robodebt to racism: what can go wrong when governments let algorithms make the decisions&lt;/h2&gt;
&lt;p&gt;"Algorithmic decision-making has enormous potential to do good. From identifying priority areas for first response after an earthquake hits, to identifying those at risk of COVID-19 within minutes, their application has proven hugely beneficial."&lt;/p&gt;
&lt;p&gt;"But things can go drastically wrong when decisions are trusted to algorithms without ensuring they adhere to established ethical norms. Two recent examples illustrate how government agencies are failing to automate fairness."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/from-robodebt-to-racism-what-can-go-wrong-when-governments-let-algorithms-make-the-decisions-132594"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI firm that worked with Vote Leave given new coronavirus contract&lt;/h2&gt;
&lt;p&gt;"Deal may allow Faculty, linked to senior Tory figures, to analyse social media data, utility bills and credit ratings"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/jun/02/ai-firm-that-worked-with-vote-leave-wins-new-coronavirus-contract"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data versus lore: an introduction to the ethical concerns surrounding Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Tracking the debate on COVID-19 surveillance tools&lt;/h2&gt;
&lt;p&gt;"Contact-tracing apps could help keep countries open before a vaccine is available. But do we have a sufficient understanding of their efficacy, and can we balance protecting public health with safeguarding civil rights? We interviewed five experts, with backgrounds in digital health ethics, internet law and social sciences.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/s42256-020-0194-1"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: June 9 - June 16</title><link href="/ai-ethics-news-roundup-june03-june09.html" rel="alternate"></link><published>2020-06-16T17:00:00+02:00</published><updated>2020-06-16T17:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-06-16:/ai-ethics-news-roundup-june03-june09.html</id><summary type="html">&lt;p&gt;@vdignum @MullerCatelijne @RecklessCoding analyze EU AI whitepaper at @ALLAI_EU @inafried COVID-19 generating AI bias @karenkho big tech's facial recognition policy @Graphika_NYC info-operations on 300 tech platforms @ProfFerguson police surveillance tech @brookingsinst robustness key to AI @RosariaTaddeo #covid19 digital governance + much more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Fresh concerns about AI bias in the age of COVID-19&lt;/h2&gt;
&lt;p&gt;"Businesses facing unprecedented demands during the coronavirus pandemic have boosted their use of artificial intelligence in some of society's most sensitive areas."&lt;/p&gt;
&lt;p&gt;"Why it matters: Algorithms and the data they rely on are prone to automating preexisting biases — and are more likely to do so when they're rushed into the field without careful testing and review."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.axios.com/fresh-concerns-about-ai-bias-in-the-age-of-covid-19-70288389-8398-4941-ae5e-42f939f65f4a.html?utm_campaign=organic&amp;utm_medium=socialshare&amp;utm_source=twitter"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;IBM will no longer offer, develop, or research facial recognition technology&lt;/h2&gt;
&lt;p&gt;"IBM will no longer offer general purpose facial recognition or analysis software, IBM CEO Arvind Krishna said in a letter to Congress today. The company will also no longer develop or research the technology, IBM tells The Verge. Krishna addressed the letter to Sens. Cory Booker (D-NJ) and Kamala Harris (D-CA) and Reps. Karen Bass (D-CA), Hakeem Jeffries (D-NY), and Jerrold Nadler (D-NY)."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The two-year fight to stop Amazon from selling face recognition to the police&lt;/h2&gt;
&lt;p&gt;"This week’s moves from Amazon, Microsoft, and IBM mark a major milestone for researchers and civil rights advocates in a long and ongoing fight over face recognition in law enforcement."&lt;/p&gt;
&lt;p&gt;"The cynical part of me says Amazon is going to wait until the protests die down...to revert to its prior position."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/06/12/1003482/amazon-stopped-selling-police-face-recognition-fight/?truid=9492a43e68469586573ba1202bf74bc8&amp;utm_source=the_algorithm&amp;utm_medium=email&amp;utm_campaign=the_algorithm.unpaid.engagement&amp;utm_content=06-12-2020"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Final Analysis of the EU Whitepaper on AI&lt;/h2&gt;
&lt;p&gt;"The White Paper is the European Commission’s first concrete attempt at discussing AI  policy beyond the  high-level  statements  of previous Communications.In  this sense, the Commission takes up a rule setting role(rather than a referee role). In our opinion, this is a good first step. "&lt;/p&gt;
&lt;p&gt;"In its Whitepaper on Artificial Intelligence, Europe took a clear stance on AI;  foster  uptake  of  AI  technologies,  underpinned  by  what  it  calls  ‘an ecosystem  of  excellence’,  while  also  ensuring  their  compliance  with  to European   ethical   norms,   legal   requirements   and   social   values,   ‘an ecosystem   of   trust’.   While   the   Whitepaper   on   AI   of   the   European Commission  does  not  propose  legislation  yet,  it  announces  some  bold legislative measures, that will likely materialize in the beginning of 2021."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://allai.nl/wp-content/uploads/2020/06/ALLAI-Final-Analysis-of-the-EU-Whitepaper-on-AI-consultation.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Joanna Bryson: Regulating AI as a pervasive technology: My response to the EU AI Whitepaper Consultation&lt;/h2&gt;
&lt;p&gt;"Basically it's a good document heading the right direction."&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation is actually easy / AI is never necessarily opaque&lt;/li&gt;
&lt;li&gt;Humans are always responsible, AI can only be transparent. &lt;/li&gt;
&lt;li&gt;AI isn't really some weird byproduct of data (the Rumpelstilzchen fallacy). Computation and cybersecurity are more essential than data storage for any length of time.&lt;/li&gt;
&lt;li&gt;AI is produced by programmers, so the EU needs to add program code, architecture documents and specifications to the list of documents they require companies to be able to produce for inspection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Trust and excellence — the EU is missing the mark again on AI and human rights&lt;/h2&gt;
&lt;p&gt;"The European Commission’s consultation on the “White Paper on Artificial Intelligence — a European approach to excellence and trust” is closing on Sunday, June 14. In its current form, the policy approach of the EU will bring about neither trust nor excellence in automated decision-making (ADM) / artificial intelligence (AI) systems, and will do nothing to ensure that both the private and public sector respect and promote human rights in the context of artificial intelligence."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.accessnow.org/trust-and-excellence-the-eu-is-missing-the-mark-again-on-ai-and-human-rights/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Forgeries, interference, and attacks on Kremlin critics across six years and 300 sites and platforms.&lt;/h2&gt;
&lt;p&gt;"Secondary Infektion is a series of operations run by a large-scale, persistent threat actor from Russia that worked in parallel to the Internet Research Agency and the GRU but was systematically different in its approach."&lt;/p&gt;
&lt;p&gt;"The campaign used fake accounts and forged documents to sow conflict between Western countries and most often targeted Ukraine. It produced at least 2,500 pieces of content in seven languages across over 300 platforms from 2014 into 2020."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://secondaryinfektion.org/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;High-tech surveillance amplifies police bias and overreach&lt;/h2&gt;
&lt;p&gt;"Police use of these national security-style surveillance techniques – justified as cost-effective techniques that avoid human bias and error – has grown hand-in-hand with the increased militarization of law enforcement. Extensive research, including my own, has shown that these expansive and powerful surveillance capabilities have exacerbated rather than reduced bias, overreach and abuse in policing, and they pose a growing threat to civil liberties. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/high-tech-surveillance-amplifies-police-bias-and-overreach-140225"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why robustness is key to deploying AI&lt;/h2&gt;
&lt;p&gt;"The takeaway for policymakers—at least for now—is that when it comes to high-stakes settings, machine learning (ML) is a risky choice. “Robustness,” i.e. building reliable, secure ML systems, is an active area of research. But until we’ve made much more progress in robustness research, or developed other ways to be confident that a model will fail gracefully, we should be cautious in relying on these methods when accuracy really matters."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/techstream/why-robustness-is-key-to-deploying-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Fair Is Zoom Justice?&lt;/h2&gt;
&lt;p&gt;"Court hearings are going virtual in response to COVID-19. Studies show they can lead to harsher outcomes for defendants."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://onezero.medium.com/how-fair-is-zoom-justice-4c1169195fb4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Digital inclusion and data literacy&lt;/h2&gt;
&lt;p&gt;ICYMI: Special Issue of Internet Policy Review&lt;/p&gt;
&lt;p&gt;"As more of our everyday lives become digital, it has become crucial to include everyone in the digital society. This special issue is examining the different layers of digital inclusion and data literacy by drawing on research, policy, and practice developments around literacies in various regions and contexts. It highlights the politics around them so as to propose policies that are needed to include more people in datafied societies, and what types of literacies they should learn. This issue includes three commentaries by experts in the field and five peer-reviewed academic papers that go towards tackling digital inclusion. This means to find solutions to the fact that many people are left behind technological advancements, and that these create what is commonly called - the digital divide. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://policyreview.info/digital-inclusion"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Point and Counterpoint on Robot Rights&lt;/h2&gt;
&lt;p&gt;NOEMA published &lt;a href="https://www.noemamag.com/a-misdirected-application-of-ai-ethics/"&gt;A Misdirected Application Of AI Ethics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"The debate about robot rights diverts moral philosophy away from the pressing matter of the oppressive use of AI technology against vulnerable groups in society."&lt;/p&gt;
&lt;p&gt;@eripsa collected &lt;a href="https://twitter.com/eripsa/status/1272181435307360257"&gt;critical responses on twitter&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data &amp;amp; Dystopia&lt;/h2&gt;
&lt;p&gt;New Podcast: &lt;/p&gt;
&lt;p&gt;"Europe is increasingly caught between upholding privacy of citizens and promoting intrusive artificial intelligence. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://euscream.com/data-dystopia/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Ethical Governance of the Digital During and After the COVID‑19 Pandemic&lt;/h2&gt;
&lt;p&gt;Mariarosaria Taddeo's editor's letter for Minds &amp;amp; Machines on the ethical governance of digital technologies during and after the COVID pandemic.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/content/pdf/10.1007/s11023-020-09528-5.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://joanna-bryson.blogspot.com/2020/06/regulating-ai-as-pervasive-technology.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Transparency in Language Generation: Levels of Automation&lt;/h2&gt;
&lt;p&gt;"Language models and conversational systems are growing increasingly advanced, creating outputs that may be mistaken for humans. Consumers may thus be misled by advertising, media reports, or vagueness regarding the role of automation in the production of language. We propose a taxonomy of language automation, based on the SAE levels of driving automation, to establish a shared set of terms for describing automated language. It is our hope that the proposed taxonomy can increase transparency in this rapidly advancing field. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/2006.06295"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Czech civil society fights back against fake news&lt;/h2&gt;
&lt;p&gt;"In the Czech Republic, the media ecosystem is plagued by disinformation. A group of PR professionals have teamed up to cut off dodgy outlets from their main, and often only, source of income — online ads."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.dw.com/en/czech-civil-society-fights-back-against-fake-news/a-53758412"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Ethical Balance of Using Smart Information Systems for Promoting the United Nations’ Sustainable Development Goals&lt;/h2&gt;
&lt;p&gt;"SIS have the potential to exacerbate inequality and further entrench the market dominance of big tech companies, if left uncontrolled. "&lt;/p&gt;
&lt;p&gt;"The paper explores how technology can be used to address the SDGs and in particular Smart Information Systems (SIS). SIS, the technologies that build on big data analytics, typically facilitated by AI techniques such as machine learning, are expected to grow in importance and impact. Some of these impacts are likely to be beneficial, notably the growth in efficiency and profits, which will contribute to societal wellbeing. At the same time, there are significant ethical concerns about the consequences of algorithmic biases, job loss, power asymmetries and surveillance, as a result of SIS use." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.mdpi.com/2071-1050/12/12/4826"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Privacy-preserving A.I. is the future of A.I.&lt;/h2&gt;
&lt;p&gt;"...But researchers have shown that this kind of anonymization doesn’t guarantee privacy: There are often other fields in data, such as location, age, or occupation, that might allow you to re-identify an individual, especially if you are able to cross-reference it with another dataset that does include personal information."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://fortune.com/2020/06/16/privacy-preserving-a-i-is-the-future-of-a-i/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;From robodebt to racism: what can go wrong when governments let algorithms make the decisions&lt;/h2&gt;
&lt;p&gt;"Algorithmic decision-making has enormous potential to do good. From identifying priority areas for first response after an earthquake hits, to identifying those at risk of COVID-19 within minutes, their application has proven hugely beneficial."&lt;/p&gt;
&lt;p&gt;"But things can go drastically wrong when decisions are trusted to algorithms without ensuring they adhere to established ethical norms. Two recent examples illustrate how government agencies are failing to automate fairness."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/from-robodebt-to-racism-what-can-go-wrong-when-governments-let-algorithms-make-the-decisions-132594"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI firm that worked with Vote Leave given new coronavirus contract&lt;/h2&gt;
&lt;p&gt;"Deal may allow Faculty, linked to senior Tory figures, to analyse social media data, utility bills and credit ratings"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/jun/02/ai-firm-that-worked-with-vote-leave-wins-new-coronavirus-contract"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data versus lore: an introduction to the ethical concerns surrounding Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Tracking the debate on COVID-19 surveillance tools&lt;/h2&gt;
&lt;p&gt;"Contact-tracing apps could help keep countries open before a vaccine is available. But do we have a sufficient understanding of their efficacy, and can we balance protecting public health with safeguarding civil rights? We interviewed five experts, with backgrounds in digital health ethics, internet law and social sciences.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/s42256-020-0194-1"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Immunity Passports: The Dangers of Mixing Health Data and Economic Liberties</title><link href="/immunity-passports-covid19.html" rel="alternate"></link><published>2020-06-11T15:00:00+02:00</published><updated>2020-06-11T15:00:00+02:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2020-06-11:/immunity-passports-covid19.html</id><summary type="html">&lt;p&gt;Why Health Data and Economic Liberties Don’t Mix - An Ethical Analysis of Immunity Passports&lt;/p&gt;</summary><content type="html">&lt;p&gt;As we transition into the beginning phases of deconfinement, we are seeing countries incorporate different variations of tech-based solutions to help assist in these delicate endeavors. Digital contract tracing (DCT) applications have taken the main stage in conversations for proposed solutions, however this is not the only tech being offered as an answer to our current crisis. Although on a much smaller scale in comparison to DCT, discussions on immunity passports have begun to appear on our timelines as another potential tech solution to expedite the exit from quarantine for certain individuals. However, just as we asked with DCT, so we must ask again with immunity passports: what are the ethics behind this technology, and, with human values in mind, is this worth pursuing? &lt;/p&gt;
&lt;h2&gt;Defining Immunity Passports&lt;/h2&gt;
&lt;p&gt;An immunity passport is essentially a digital certificate attached to a person’s identity indicating whether or not an individual has previously contracted the coronavirus and is subsequently immune to a second infection. The purpose of such technology would be to begin allowing individuals who are immune, and therefore risk-free in respect to contracting COVID-19, to begin returning to ‘normal life’ activities and liberties. &lt;/p&gt;
&lt;p&gt;Currently, the application of immunity passports has been put on hold due to the fact of insufficient evidence of the necessary antibodies that would declare an individual immune. In fact, the &lt;a href="https://www.who.int/news-room/commentaries/detail/immunity-passports-in-the-context-of-covid-19"&gt;World Health Organization&lt;/a&gt; advised against governments adopting any form of immunity passport, at this point in time, due to lack of evidence and therefore risk of potential for re-infection. Although this is a significant barrier to the immediate use of immunity passports, it does not mean that it is an absolute end to the discussion. &lt;/p&gt;
&lt;p&gt;If sufficient evidence in effective antibodies does arise, then the idea of an immunity passport will very likely come back into the conversation. Furthermore, as the world now understands the importance of preparation for a pandemic, immunity passports may be viable in future cases. So, even though we are not utilizing them at this very moment in time, it does not imply that we never will, and instead affords us the time to properly consider the ethical angles to this technology prior to technical implementation.&lt;/p&gt;
&lt;h2&gt;Breaking down the tech - what are we really working with here?&lt;/h2&gt;
&lt;p&gt;In order to properly consider the ethics of this technology, let’s take a step back from any technical, medical, or le gal barrier to immunity passports that may currently exist. These are of course vital components to the success of such passports, but can often cause distractions, confusing the debate and pulling us away from our original goal of unpacking the core ethical principles at work. So, for just a moment, let’s step into a thought experiment in which the required antibodies exist and immunity passports are technically feasible. &lt;/p&gt;
&lt;p&gt;At its very core, immunity passports are simply digital markers indicating whether or not a person has previously contracted coronavirus and so now has the antibodies required to prevent a second infection. Fundamentally speaking, the act of indicating whether or not an individual is immune is ethically neutral. As it is neither ethically good nor bad to have previously contracted coronavirus, the marker is not ethically laden, it’s merely a descriptor indicative of reality.  &lt;/p&gt;
&lt;p&gt;It is important to consider, however, that such a digital marker should be classified as health data as it is essentially an indicator of an individual’s medical history. Just as a medical record of whether or not a person has had the chickenpox is kept securely as health data, so should a record of whether or not they have had coronavirus. Under normal circumstances, broadcasting information about individuals’ medical histories to the public would be immediately rejected as we have come to respect health information as something private between an individual and their doctor. However, there are times in which individuals are willing to share health data if it means they gain some sort of benefit in return. In the case of the coronavirus, information regarding prior infection results in the gain of an immunity passport.&lt;/p&gt;
&lt;p&gt;Again, the act of indicating prior infection is ethically neutral. Sharing this information, on the other hand, starts to bring this ethical neutrality into question. However, the real complications start to arise when we begin assigning economic and social value to this immunity marker. &lt;/p&gt;
&lt;h2&gt;What happens when our medical histories affect our economic abilities?&lt;/h2&gt;
&lt;p&gt;Although an immunity passport may be a simple digital marker at its core, this is not its only layer. The differentiating factor between an immunity passport and, say, a digital marker indicating whether a person has a strawberry allergy or not, is the value that we assign to the immunity marker. Normally, a digital marker indicating some detail or another about an individual’s medical history remains securely in the medical field. However, this is where an immunity passport would differ, as the digital marker for immunity is created with the purpose of allowing an individual with the marker access to certain economic and social liberties. It is this crossover that we now need to take a closer critical eye to. &lt;/p&gt;
&lt;p&gt;Our medical histories are often out of our control. So, we try to keep them as separate as possible from our economic and social standing, as conversely these are aspects of our lives we try to maintain high amounts of control over. It is not unheard of for medical conditions to crossover into influencing economic and social factors of our lives, however these crossovers are what we term as disabilities or handicaps. For example, a person born without legs is physically handicapped and may face economic and social setbacks because of this. The important thing though, is we work as a society to combat any challenge or disadvantage caused by medical handicaps to the best of our abilities. &lt;/p&gt;
&lt;p&gt;Returning back to immunity passports, we see that such a marker would be a crossover from individuals’ medical histories into the economic and social aspects of their lives. However, in this case we would not term this crossover as a handicap, but rather an advantage, while those without the passport would be the ones put at a disadvantage. This means that an immunity passport would essentially be creating socially-imposed economic handicaps that were not previously there. Except in this case the handicap would not grant the individual access to additional aid, rather it would instead restrict the individual further.&lt;/p&gt;
&lt;p&gt;Currently, the tech industry does not possess any overarching code of ethics, so I’m going to fall back on the principles in medical ethics here for a moment to help illustrate what is going on ethically in this situation. Medical ethics has four main principles, one of which is the principle of justice that essentially states there must be an element of fairness in all medical decisions. Although immunity passports are issued to make economic and social decisions, these decisions are based on medical data, so in this case it is fair to apply the principle of justice to this technology. &lt;/p&gt;
&lt;p&gt;As currently proposed, immunity passports would be violating the ethical principle of justice due to the fact that the possession of an immunity passport would disproportionately either benefit or burden an individual based on circumstances beyond their own control. The action of assigning access to economic and social liberties to the digital marker of immunity is what makes the originally neutral technology into an ethically laden one, and not in a good way. &lt;/p&gt;
&lt;h2&gt;Respecting Ethical Limitations Leads to Stronger Solutions&lt;/h2&gt;
&lt;p&gt;As we now see, the ethical principle of justice calls immunity passports into question. When justice is put at risk in this context, we are looking at substantial consequences such as people voluntarily infecting themselves out of desperation, discrimination in access to proper testing, and a disproportionate effect on people whose professions require them to work onsite, to name a few. This is why it is so essential to break down technology proposals to the point of ethical analysis as it helps uncover, what we hope were, unintended consequences and allows us to tackle the issues before they arise. &lt;/p&gt;
&lt;p&gt;In the case of immunity passports, we see that although the technology is ethically neutral, the proposed application of it is not. It is important to note now that this does not mean we should give up on the technology itself. Instead, this implies that we have not found the best possible application for the technology according to our ethical limitations. &lt;/p&gt;
&lt;p&gt;In fact, it is quite possible to use the concept of a digital marker to help us fight COVID-19. For example, it could be beneficial for high-risk individuals to voluntarily apply for a digital marker that grants them certain aids, such as access to grocery stores the hour after they have been disinfected, or priority for access to masks. These are of course only examples that need to be further examined to see if they are viable, but the point here is that there are other applications of this technology that are better suited both technically and ethically to our end goal of the fight against coronavirus. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="immunity passports"></category><category term="covid-19"></category></entry><entry><title>Garbage In, Cyber Truck Out</title><link href="/bogani-truck.html" rel="alternate"></link><published>2020-06-10T15:00:00+02:00</published><updated>2020-06-10T15:00:00+02:00</updated><author><name>Ronny Bogani</name></author><id>tag:None,2020-06-10:/bogani-truck.html</id><summary type="html">&lt;p&gt;A proposal for Ethical Artificial Intelligence Sustainability Impact Statements (E.A.I.S.I.S.) in Autonomous Vehicle Manufacturing&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/EASIS%20-%20Bogani%202020.pdf"&gt;Read the Full Article&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Aiming to address the increasing demand for data presented by Artificial
Intelligence (AI). This essay proposes adopting an Ethical Artificial Intelligence
Sustainability Impact Statement (E.A.I.S.I.S) through the various stages of the
Artificial Intelligence Development Cycle (AIDC). The following analysis treats data as a limited resource requiring sustainable exploitation and management. Following a brief discussion about sustainability and AI, the different stages of the AIDC and accompanying factors for an ethical impact evaluation are addressed through proposed questions. These inquiries are intended to identify, disclose and address the data’s toxicity level, or potential ethical harm for use.&lt;/p&gt;
&lt;p&gt;The proposed approach to data as a public good benefits both the digital
environment (DE) and conserves natural resource demand in the real world (IRL).
Data is currently a commodity commanding prices higher than oil within a booming
data brokerage industry.1 As with other commodities, derivative markets will form
with the price for data lowering proportionally to its level of impurity, or debasement. Through using an E.A.I.S.I.S. approach, certain ethical issues attached to data can be flagged, evaluated, and provided to all users and redeployed in a transparent manner. The following discussion addresses factors to be considered in creating an E.A.I.S.I.S. in the development of an autonomous vehicle (AV). As a novel approach to AIDC, the issues raised are intended to serve as a framework for later discussion, research, development, and application.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/EASIS%20-%20Bogani%202020.pdf"&gt;Read the Full Article&lt;/a&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="autonomous vehicles"></category><category term="sustainability"></category></entry><entry><title>Weekly AI Ethics News Roundup: June 3 - June 9</title><link href="/ai-ethics-news-roundup-june03-june09.html" rel="alternate"></link><published>2020-06-09T17:00:00+02:00</published><updated>2020-06-09T17:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-06-09:/ai-ethics-news-roundup-june03-june09.html</id><summary type="html">&lt;p&gt;Racist algorithms and the people fighting against them, the AI Gonvernance Forum'20 is on now, IBM will no longer research facial recognition tech, is Europe building a Chinese-style firewall?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;AI Governance Forum 2020: Schedule&lt;/h2&gt;
&lt;p&gt;"The AI Governance Forum is a multi-stakeholders platform, open to all interested parties and dedicated to build Human-Trust in AI for the benefit of all. Stakeholders can be from public or private sector, scientific community and civil society. The AI Governance Forum considers each stakeholder as equal partner to the discussion.&lt;/p&gt;
&lt;p&gt;The AI Governance Forum is based on the conviction that a collective intelligence process is an essential component to manage the AI impact on our society. It contribute to build an open artificial intelligence for the benefit of all."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ai-gf.com/index.php/schedule/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;IBM will no longer offer, develop, or research facial recognition technology&lt;/h2&gt;
&lt;p&gt;“IBM firmly opposes and will not condone uses of any [facial recognition] technology, including facial recognition technology offered by other vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms, or any purpose which is not consistent with our values and Principles of Trust and Transparency,” Krishna said in the letter. “We believe now is the time to begin a national dialogue on whether and how facial recognition technology should be employed by domestic law enforcement agencies.”&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;This startup is using AI to give workers a “productivity score”&lt;/h2&gt;
&lt;p&gt;"Companies have asked remote workers to install a whole range of such tools. [...] Now, one firm wants to take things even further. It is developing machine-learning software to measure how quickly employees complete different tasks and suggest ways to speed them up. The tool also gives each person a productivity score, which managers can use to identify those employees who are most worth retaining—and those who are not." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/06/04/1002671/startup-ai-workers-productivity-score-bias-machine-learning-business-covid/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Germany, France launch Gaia-X platform in bid for ‘tech sovereignty’&lt;/h2&gt;
&lt;p&gt;"The idea behind the project, named “Gaia-X” after an ancient goddess, is to convince firms to store their data with home-grown alternatives to U.S. and Chinese tech giants like Amazon Web Services and Alibaba — known in the industry as "hyperscalers."&lt;/p&gt;
&lt;p&gt;But Gaia-X will not be a cloud service in itself. Set up as a nonprofit based in Belgium, it's conceived as a platform joining up cloud-hosting services from dozens of companies, allowing business to move their data freely with all information protected under Europe's tough data processing rules, France and Germany announced Thursday."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.politico.eu/article/germany-france-gaia-x-cloud-platform-eu-tech-sovereignty/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Legal Remedies For a Forgiving Society: Children's rights, data protection rights and the value of forgiveness in AI-mediated risk profiling of children by Dutch authorities&lt;/h2&gt;
&lt;p&gt;"30 years after the United Nations Convention on the Right of the Child (CRC) and two years after the new EU data protection regime, the social value of forgiveness is not part of these legal instruments. The lack of this value within these legal instruments and the lack of research on the subject of forgiveness in relation to improving the legal position of children require urgent addressing especially when children are exposed to artificial intelligence (AI)-mediated risk profiling practices by Dutch government authorities. Developmental psychologists underline that the erosion of this value could hamper children's ability to develop flourishing human relationships.&lt;/p&gt;
&lt;p&gt;This article contributes to fill this niche."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.sciencedirect.com/science/article/pii/S0267364920300352"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;EU signs contract for large-scale biometric database to protect borders&lt;/h2&gt;
&lt;p&gt;"As part of the four-year deal, technology providers IDEMIA and Sopra Steria will be involved in helping to build a new shared biometric matching system (sBMS), with the objective of fighting illegal immigration and trans-border crime across the 26 European countries in the passport-free Schengen area, eventually becoming one of the largest biometric systems in the world.&lt;/p&gt;
&lt;p&gt;As part of the new set up, third-country nationals crossing the external borders of the Schengen states will be required to use the technology to submit their biometric data for identification purposes."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.euractiv.com/section/digital/news/eu-signs-contract-for-large-scale-biometric-database-to-protect-borders/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Hamid Khan: The activist dismantling racist police algorithms&lt;/h2&gt;
&lt;p&gt;"Algorithms have no place in policing. I think it’s crucial that we understand that there are lives at stake. This language of location-based policing is by itself a proxy for racism. They’re not there to police potholes and trees. They are there to police people in the location. So location gets criminalized, people get criminalized, and it’s only a few seconds away before the gun comes out and somebody gets shot and killed."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/06/05/1002709/the-activist-dismantling-racist-police-algorithms/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;From Robodebt to Racism: What Can Go wrong When Governments Let Algorithms Make Decisions&lt;/h2&gt;
&lt;p&gt;"Algorithmic decision-making has enormous potential to do good. From identifying priority areas for first response after an earthquake hits, to identifying those at risk of COVID-19 within minutes, their application has proven hugely beneficial.&lt;/p&gt;
&lt;p&gt;But things can go drastically wrong when decisions are trusted to algorithms without ensuring they adhere to established ethical norms. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/from-robodebt-to-racism-what-can-go-wrong-when-governments-let-algorithms-make-the-decisions-132594"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Racial Illiteracy in Tech&lt;/h2&gt;
&lt;p&gt;"Computer science, user experience, machine learning, data analysis — the practitioners of these fields and involved in the design, development, and deployment of technology, should consider racial literacy an essential, necessary skill. And there are so many places this skill can be taught, from computer science classrooms to the teams of social media platform companies."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://racialliteracy.tech/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Case for Better Cybersecurity to Support the Bioeconomy&lt;/h2&gt;
&lt;p&gt;"The amount of data in the biotech sector is rapidly growing; both the number of genomes sequenced, and our worldwide sequencing capacity, double roughly every 7–18 months. The bioeconomy is built on this data, and on the software and hardware tools used to collect, process and store it.
However, as we increase our dependence on data, securing that data is becoming increasingly more important. Biological data is extremely personal, and immutable; you cannot apply for a new fingerprint."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/bioeconomy-xyz/the-case-for-better-cybersecurity-to-support-the-bioeconomy-37f2951836a8"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI News Anchor deployed in China&lt;/h2&gt;
&lt;p&gt;"An editor must still type in text for Xin Xiaowei to say, but the AI anchor never needs a break and, perhaps more importantly for its users, does not need to be paid, putting it in direct competition with real news anchors, possibly heralding the future of televised news, at least in China."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cbronline.com/news/ai-news-anchor-china"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Probability &amp;amp; Moral Responsibility with (our very own) Olivia Gambelin&lt;/h2&gt;
&lt;p&gt;Olivia Gambelin in conversation with Ben Byford.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.machine-ethics.net/podcast/probability-moral-responsibility-with-olivia-gambelin/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Could Europe introduce a China-style internet firewall?&lt;/h2&gt;
&lt;p&gt;"The bloc’s Digital Services Act (DSA) – for which the consultation period is underway – is due to update the rules governing the internet, in particular targeting the dominance and impunity of monolithic big tech.&lt;/p&gt;
&lt;p&gt;It’s not clear what form this legislation will take yet, but a new policy paper moots a radical idea: the creation of  a “European internet”, which “like the Chinese firewall” could block services that condoned “unlawful conduct” from third parties."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://tech.newstatesman.com/policy/europe-china-style-internet-firewall"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Understanding Transparency in Algorithmic Accountability&lt;/h2&gt;
&lt;p&gt;"Transparency has been in the crosshairs of recent writing about accountable algorithms. Its critics argue that releasing data can be harmful, and releasing source code won’t be useful. They claim individualized explanations of artificial intelligence (AI) decisions don’t empower people, and instead distract from more effective ways of governing. While criticizing transparency’s efficacy with one breath, with the next they defang it, claiming corporate secrecy exceptions will prevent useful information from getting out.
This chapter bucks the tide. Transparency is necessary, if not sufficient, for building and governing accountable algorithms. But for transparency to be effective, it has to be designed. It can’t be sprinkled on like seasoning; it has to be built into a regulatory system from the onset. And determining the who, what, when, and how of transparency requires first addressing the question of why."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3622657"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;"Confronting Our Reality: Racial Representation and Systemic Transformation with Dr. Timnit Gebru" on The Radical AI Podcast&lt;/h2&gt;
&lt;p&gt;"How do we respond to the racism in the world we have been given? What does it mean to transform technology systems in the spirit of justice and equity? How do we engage with diversity and representation without reducing our efforts to simple branding and lip service? To answer these questions and more the Radical AI Podcast welcomes one of our heroes Dr. Timnit Gebru to the show.  Dr. Timnit Gebru is a research scientist at Google on the ethical AI team and a co-founder of Black in AI. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://podcasts.apple.com/us/podcast/confronting-our-reality-racial-representation-systemic/id1505229145?i=1000476619938"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Defining AI Ethics&lt;/h2&gt;
&lt;p&gt;"When it comes to Ethics in Artificial Intelligence there are many different views, perspectives and lexicons."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/RRITools/status/1267731324435148806"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Algorithmic Equity Toolkit&lt;/h2&gt;
&lt;p&gt;"A set of resources designed to identify &amp;amp; interrogate surveillance &amp;amp; automated decision systems used by governments."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/DorotheaBaur/status/1268155102952652800"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Case Against Pandemic Research Exceptionalism&lt;/h2&gt;
&lt;p&gt;Zachary Lipton and Alex John London on lowering the bar on research in a crisis.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/zacharylipton/status/1269416057242030088"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: June 3 - June 9</title><link href="/ai-ethics-news-roundup-june03-june09.html" rel="alternate"></link><published>2020-06-09T17:00:00+02:00</published><updated>2020-06-09T17:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-06-09:/ai-ethics-news-roundup-june03-june09.html</id><summary type="html">&lt;p&gt;Racist algorithms and the people fighting against them, the AI Gonvernance Forum'20 is on now, IBM will no longer research facial recognition tech, is Europe building a Chinese-style firewall?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;AI Governance Forum 2020: Schedule&lt;/h2&gt;
&lt;p&gt;"The AI Governance Forum is a multi-stakeholders platform, open to all interested parties and dedicated to build Human-Trust in AI for the benefit of all. Stakeholders can be from public or private sector, scientific community and civil society. The AI Governance Forum considers each stakeholder as equal partner to the discussion.&lt;/p&gt;
&lt;p&gt;The AI Governance Forum is based on the conviction that a collective intelligence process is an essential component to manage the AI impact on our society. It contribute to build an open artificial intelligence for the benefit of all."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ai-gf.com/index.php/schedule/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;IBM will no longer offer, develop, or research facial recognition technology&lt;/h2&gt;
&lt;p&gt;“IBM firmly opposes and will not condone uses of any [facial recognition] technology, including facial recognition technology offered by other vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms, or any purpose which is not consistent with our values and Principles of Trust and Transparency,” Krishna said in the letter. “We believe now is the time to begin a national dialogue on whether and how facial recognition technology should be employed by domestic law enforcement agencies.”&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;This startup is using AI to give workers a “productivity score”&lt;/h2&gt;
&lt;p&gt;"Companies have asked remote workers to install a whole range of such tools. [...] Now, one firm wants to take things even further. It is developing machine-learning software to measure how quickly employees complete different tasks and suggest ways to speed them up. The tool also gives each person a productivity score, which managers can use to identify those employees who are most worth retaining—and those who are not." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/06/04/1002671/startup-ai-workers-productivity-score-bias-machine-learning-business-covid/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Germany, France launch Gaia-X platform in bid for ‘tech sovereignty’&lt;/h2&gt;
&lt;p&gt;"The idea behind the project, named “Gaia-X” after an ancient goddess, is to convince firms to store their data with home-grown alternatives to U.S. and Chinese tech giants like Amazon Web Services and Alibaba — known in the industry as "hyperscalers."&lt;/p&gt;
&lt;p&gt;But Gaia-X will not be a cloud service in itself. Set up as a nonprofit based in Belgium, it's conceived as a platform joining up cloud-hosting services from dozens of companies, allowing business to move their data freely with all information protected under Europe's tough data processing rules, France and Germany announced Thursday."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.politico.eu/article/germany-france-gaia-x-cloud-platform-eu-tech-sovereignty/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Legal Remedies For a Forgiving Society: Children's rights, data protection rights and the value of forgiveness in AI-mediated risk profiling of children by Dutch authorities&lt;/h2&gt;
&lt;p&gt;"30 years after the United Nations Convention on the Right of the Child (CRC) and two years after the new EU data protection regime, the social value of forgiveness is not part of these legal instruments. The lack of this value within these legal instruments and the lack of research on the subject of forgiveness in relation to improving the legal position of children require urgent addressing especially when children are exposed to artificial intelligence (AI)-mediated risk profiling practices by Dutch government authorities. Developmental psychologists underline that the erosion of this value could hamper children's ability to develop flourishing human relationships.&lt;/p&gt;
&lt;p&gt;This article contributes to fill this niche."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.sciencedirect.com/science/article/pii/S0267364920300352"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;EU signs contract for large-scale biometric database to protect borders&lt;/h2&gt;
&lt;p&gt;"As part of the four-year deal, technology providers IDEMIA and Sopra Steria will be involved in helping to build a new shared biometric matching system (sBMS), with the objective of fighting illegal immigration and trans-border crime across the 26 European countries in the passport-free Schengen area, eventually becoming one of the largest biometric systems in the world.&lt;/p&gt;
&lt;p&gt;As part of the new set up, third-country nationals crossing the external borders of the Schengen states will be required to use the technology to submit their biometric data for identification purposes."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.euractiv.com/section/digital/news/eu-signs-contract-for-large-scale-biometric-database-to-protect-borders/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Hamid Khan: The activist dismantling racist police algorithms&lt;/h2&gt;
&lt;p&gt;"Algorithms have no place in policing. I think it’s crucial that we understand that there are lives at stake. This language of location-based policing is by itself a proxy for racism. They’re not there to police potholes and trees. They are there to police people in the location. So location gets criminalized, people get criminalized, and it’s only a few seconds away before the gun comes out and somebody gets shot and killed."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/06/05/1002709/the-activist-dismantling-racist-police-algorithms/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;From Robodebt to Racism: What Can Go wrong When Governments Let Algorithms Make Decisions&lt;/h2&gt;
&lt;p&gt;"Algorithmic decision-making has enormous potential to do good. From identifying priority areas for first response after an earthquake hits, to identifying those at risk of COVID-19 within minutes, their application has proven hugely beneficial.&lt;/p&gt;
&lt;p&gt;But things can go drastically wrong when decisions are trusted to algorithms without ensuring they adhere to established ethical norms. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/from-robodebt-to-racism-what-can-go-wrong-when-governments-let-algorithms-make-the-decisions-132594"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Racial Illiteracy in Tech&lt;/h2&gt;
&lt;p&gt;"Computer science, user experience, machine learning, data analysis — the practitioners of these fields and involved in the design, development, and deployment of technology, should consider racial literacy an essential, necessary skill. And there are so many places this skill can be taught, from computer science classrooms to the teams of social media platform companies."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://racialliteracy.tech/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Case for Better Cybersecurity to Support the Bioeconomy&lt;/h2&gt;
&lt;p&gt;"The amount of data in the biotech sector is rapidly growing; both the number of genomes sequenced, and our worldwide sequencing capacity, double roughly every 7–18 months. The bioeconomy is built on this data, and on the software and hardware tools used to collect, process and store it.
However, as we increase our dependence on data, securing that data is becoming increasingly more important. Biological data is extremely personal, and immutable; you cannot apply for a new fingerprint."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/bioeconomy-xyz/the-case-for-better-cybersecurity-to-support-the-bioeconomy-37f2951836a8"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI News Anchor deployed in China&lt;/h2&gt;
&lt;p&gt;"An editor must still type in text for Xin Xiaowei to say, but the AI anchor never needs a break and, perhaps more importantly for its users, does not need to be paid, putting it in direct competition with real news anchors, possibly heralding the future of televised news, at least in China."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cbronline.com/news/ai-news-anchor-china"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Probability &amp;amp; Moral Responsibility with (our very own) Olivia Gambelin&lt;/h2&gt;
&lt;p&gt;Olivia Gambelin in conversation with Ben Byford.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.machine-ethics.net/podcast/probability-moral-responsibility-with-olivia-gambelin/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Could Europe introduce a China-style internet firewall?&lt;/h2&gt;
&lt;p&gt;"The bloc’s Digital Services Act (DSA) – for which the consultation period is underway – is due to update the rules governing the internet, in particular targeting the dominance and impunity of monolithic big tech.&lt;/p&gt;
&lt;p&gt;It’s not clear what form this legislation will take yet, but a new policy paper moots a radical idea: the creation of  a “European internet”, which “like the Chinese firewall” could block services that condoned “unlawful conduct” from third parties."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://tech.newstatesman.com/policy/europe-china-style-internet-firewall"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Understanding Transparency in Algorithmic Accountability&lt;/h2&gt;
&lt;p&gt;"Transparency has been in the crosshairs of recent writing about accountable algorithms. Its critics argue that releasing data can be harmful, and releasing source code won’t be useful. They claim individualized explanations of artificial intelligence (AI) decisions don’t empower people, and instead distract from more effective ways of governing. While criticizing transparency’s efficacy with one breath, with the next they defang it, claiming corporate secrecy exceptions will prevent useful information from getting out.
This chapter bucks the tide. Transparency is necessary, if not sufficient, for building and governing accountable algorithms. But for transparency to be effective, it has to be designed. It can’t be sprinkled on like seasoning; it has to be built into a regulatory system from the onset. And determining the who, what, when, and how of transparency requires first addressing the question of why."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3622657"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;"Confronting Our Reality: Racial Representation and Systemic Transformation with Dr. Timnit Gebru" on The Radical AI Podcast&lt;/h2&gt;
&lt;p&gt;"How do we respond to the racism in the world we have been given? What does it mean to transform technology systems in the spirit of justice and equity? How do we engage with diversity and representation without reducing our efforts to simple branding and lip service? To answer these questions and more the Radical AI Podcast welcomes one of our heroes Dr. Timnit Gebru to the show.  Dr. Timnit Gebru is a research scientist at Google on the ethical AI team and a co-founder of Black in AI. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://podcasts.apple.com/us/podcast/confronting-our-reality-racial-representation-systemic/id1505229145?i=1000476619938"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Defining AI Ethics&lt;/h2&gt;
&lt;p&gt;"When it comes to Ethics in Artificial Intelligence there are many different views, perspectives and lexicons."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/RRITools/status/1267731324435148806"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Algorithmic Equity Toolkit&lt;/h2&gt;
&lt;p&gt;"A set of resources designed to identify &amp;amp; interrogate surveillance &amp;amp; automated decision systems used by governments."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/DorotheaBaur/status/1268155102952652800"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Case Against Pandemic Research Exceptionalism&lt;/h2&gt;
&lt;p&gt;Zachary Lipton and Alex John London on lowering the bar on research in a crisis.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/zacharylipton/status/1269416057242030088"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Building Ethical Intelligence, the Podcast</title><link href="/webinar-june20.html" rel="alternate"></link><published>2020-06-05T15:00:00+02:00</published><updated>2020-06-05T15:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-06-05:/webinar-june20.html</id><summary type="html">&lt;p&gt;The first installment in our new series of webinars, &lt;em&gt;Building Ethical Intelligence&lt;/em&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Join us as we unpack what's happened this month in AI Ethics, take a deep dive into Michael Klenk's work on online manipulation and its implications for legislation, as well as hear his thoughts on fairness in digital contact tracing.&lt;/p&gt;
&lt;p&gt;For further information on Michael’s research, you can visit his website &lt;a href="http://www.michael-klenk.com/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you would like to contact Michael, you can reach him by &lt;a href="mailto:M.B.O.T.Klenk@tudelft.nl"&gt;email&lt;/a&gt;, or connect via &lt;a href="https://www.linkedin.com/in/mbklenk/"&gt;LinkedIn&lt;/a&gt; and &lt;a href="https://twitter.com/michaelklenk"&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This podcast is part of our wider educational series, Building Ethical Intelligence, and is the audio recording of the live webinar event. The series is designed to keep you up-to-date on &lt;strong&gt;developments in AI Ethics&lt;/strong&gt;, deepen your &lt;strong&gt;understanding of integral concepts&lt;/strong&gt;, and build your ability to &lt;strong&gt;translate ethical intelligence into practical action&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;If you would like to participate in the next webinar, please sign up &lt;a href="https://www.eventbrite.co.uk/e/building-ethical-intelligence-live-tickets-106091727212"&gt;here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Subscribe via Spotify, Apple Podcasts or Google Play or follow us on LinkedIn and Twitter for future episodes. &lt;/p&gt;
&lt;iframe title="Building Ethical Intelligence" id="multi_iframe" src="https://www.podbean.com/media/player/multi?playlist=http%3A%2F%2Fplaylist.podbean.com%2F8661426%2Fplaylist_multi.xml&amp;vjs=1&amp;kdsowie31j4k1jlf913=63a18e2c15a698cf2318ae77ec22e652f69d1786&amp;size=430&amp;skin=11&amp;episode_list_bg=%23ffffff&amp;bg_left=%23fafafa&amp;bg_mid=%23fafafa&amp;bg_right=%23fbfafc&amp;podcast_title_color=%23474547&amp;episode_title_color=%23080708&amp;auto=0&amp;download=1&amp;show_playlist_recent_number=10&amp;pbad=1" frameborder="0" scrolling="no" width="100%" height="432" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;Hosted by Olivia Gambelin, Oriana Medlicott and Amanda Curry from Ethical Intelligence&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="podcast"></category></entry><entry><title>Running an Ethics Audit - Case Study: Digital Contact Tracing</title><link href="/dct-report.html" rel="alternate"></link><published>2020-05-29T15:00:00+02:00</published><updated>2020-05-29T15:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-05-29:/dct-report.html</id><summary type="html">&lt;p&gt;An ethical analysis of Digital Contact Tracing based on the EU's guidelines for ethical AI.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/Digital%20Contact%20Tracing%20-%20EI.pdf"&gt;Read the Full Report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since the onset of the COVID-19 pandemic, the tech industry has been hard at work developing potential solutions to help fight the virus. One of the more prominent proposals has been for the use of Digital Contact Tracing (DCT), the technical application of manual contact tracing methods. DCT quickly became a topic of hot debate, as privacy and trust concerns were thrust into the spotlight as never before. Suddenly, ethical principles that had been discussed theoretically at length had a clear actionable case study plastering headlines as society began to understand the importance of ethics in emerging technology and the risks that unchecked innovation holds.&lt;/p&gt;
&lt;p&gt;Although there is now a clear call for the incorporation of ethical principles in DCT applications, the path to such is not certain. There is no rule book for how to implement DCT while still respecting human rights and ethics, no clear cut guide laying out each step to take. However, this does not mean that we are without a solution. As we look to roll out DCT applications to help fight the coronavirus, it is vital that these applications undergo due diligence ethics audits if we are to respect human dignity.&lt;/p&gt;
&lt;p&gt;The purpose of this paper is to illustrate, from a high level perspective, what it would look like to run a preliminary ethics audit on a DCT application, with the hope that it may provide some guiding structure to those currently working on similar applications.&lt;/p&gt;
&lt;p&gt;To demonstrate such, we will be evaluating a hypothetical Bluetooth Digital Contact Tracing application against the seven principles laid out by the European Union’s High-Level Expert Ethics Guidelines. For each of the seven principles we will highlight the essential considerations that must be made in order to embed the principle into practice in terms of the chosen technology. These considerations are not only applicable to DCT, but also exemplify the high-level process any COVID-19 tech should be undertaking.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/Digital%20Contact%20Tracing%20-%20EI.pdf"&gt;Read the Full Report&lt;/a&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="dct"></category></entry><entry><title>Is Public Surveillance the Answer to COVID-19? Why Community Engagement is Needed to Shape Better Informed Decisions</title><link href="/community-engagement-covid19.html" rel="alternate"></link><published>2020-05-28T18:00:00+02:00</published><updated>2020-05-28T18:00:00+02:00</updated><author><name>Lily Grogan</name></author><id>tag:None,2020-05-28:/community-engagement-covid19.html</id><summary type="html">&lt;p&gt;More invasive tech solutions for a post-COVID-19 world do present public health benefits, but they are not infallible, and the trade-off is personal privacy, autonomy and dignity.&lt;/p&gt;</summary><content type="html">&lt;p&gt;COVID-19 has seen governments and tech working together to combat its spread, leading to the implementation of increasingly invasive surveillance measures. This has given rise to the illusion that one cannot have both privacy and health at once. This is a false choice.&lt;/p&gt;
&lt;p&gt;This &lt;a href="https://www.linkedin.com/pulse/why-we-need-ethics-tech-now-more-than-ever-covid-19-olivia-gambelin/"&gt;trade-off illusion&lt;/a&gt; must be challenged. But the implementation of emergency measures across the globe such as digital contact tracing, mobile phone tracking and even &lt;a href="https://www.bbc.com/news/av/technology-52619568/coronavirus-robot-dog-enforces-social-distancing-in-singapore-park"&gt;robot police dogs&lt;/a&gt; continues to raise ethical concerns. While these surveillance measures have great potential to help curb the spread of COVID-19, they are far from perfect, and continue to raise concerns over privacy, autonomy, and dignity. Companies and policy makers must turn to the stakeholders within their communities to develop tech that is locally acceptable- or risk losing public trust.&lt;/p&gt;
&lt;p&gt;While the social distance enforcing dog ‘droids roaming Bishan-Ang Mo Kio Park may seem like a dystopian fantasy to many in the West, they are not the first COVID-19 tech solution to be pioneered by Singapore. They were one of the first countries to implement digital contact tracing with their &lt;a href="https://www.tracetogether.gov.sg/common/privacystatement"&gt;TraceTogether&lt;/a&gt; app launched in March- the likes of which are now being developed across the globe.&lt;/p&gt;
&lt;p&gt;Digital contact tracing is a good focal point for the discussion surrounding public surveillance, for if the intended ends are achieved, it is easy to see how they might justify the means.&lt;/p&gt;
&lt;h2&gt;DCT makes sense, but how effective really is it?&lt;/h2&gt;
&lt;p&gt;The idea is simple. A user reports positive on the app if they become sick, and other users who have recently been in contact with them will receive a notification that they have been exposed to COVID-19. &lt;/p&gt;
&lt;p&gt;The UK’s NHS is developing its own app, while Australia recently released &lt;a href="https://www.health.gov.au/resources/apps-and-tools/covidsafe-app#about-the-app"&gt;COVIDSafe&lt;/a&gt;, it’s version of TraceTogether. Unlike China’s &lt;a href="https://www.theguardian.com/world/2020/apr/01/chinas-coronavirus-health-code-apps-raise-concerns-over-privacy"&gt;Health Code&lt;/a&gt; service, which freely shares the GPS location of its users with authorities, apps like TraceTogether and COVIDSafe do not require location sharing, instead relying on Bluetooth proximity to deduce when an infected person has been within range. Users are kept anonymous through an encrypted ID and safeguarded by the systematic destruction of data (usually after a period of 14-21 days).&lt;/p&gt;
&lt;p&gt;Perhaps more so than other public surveillance initiatives, these apps have the potential to drastically curb the spread of infection as they alleviate the laborious task of manual tracing. But how reliable are they? &lt;/p&gt;
&lt;p&gt;There is no guarantee that someone who self-reports positive on an app actually has the virus. Those who receive a notification that they have been in contact with a sick person have no idea who this person is and whether their encounter has credibly left them at risk of infection. Some will self-isolate, while other well people may feel compelled to contact emergency services, putting more pressure on systems already under massive strain. The potential for false positives may lead others to ignore such notifications altogether, making the app redundant. &lt;/p&gt;
&lt;p&gt;False negatives are just as problematic; Bluetooth limitations mean accuracy is not guaranteed. This could create a false sense of security leading to complacency. DCT users also require a smart phone with internet access, creating a distribution challenge with socioeconomic implications.&lt;/p&gt;
&lt;p&gt;Furthermore, the trustworthiness of those charged with handling these intimate datasets is up for debate, as tech giants and government bodies alike have been known to break their own privacy policies in the past (Google’s &lt;a href="https://www.theguardian.com/commentisfree/2017/jul/05/sensitive-health-information-deepmind-google"&gt;Deepmind scandal&lt;/a&gt; being just one example- and not the worst). The &lt;a href="https://www.lightbluetouchpaper.org/2020/04/12/contact-tracing-in-the-real-world/"&gt;difficulties&lt;/a&gt; associated with running decentralised tech systems alone mean that the temptation to centralise would be great, and Apple and Google’s efforts to predispose all of their devices to DCT scales the associated risks enormously. Also worth noting is the efficacy of such a system to cause widespread panic if masses of users suddenly report positive. This makes them excellent targets for malicious hackers.&lt;/p&gt;
&lt;p&gt;Governments are aware of these concerns, which is why these apps are voluntary. Yet as it stands, simply not enough people are opting in for them to be effective. DCT requires 60% of a population to participate for it to work, but as of now, only 20% of the Singaporean population have opted in. This means that unless these apps are made mandatory, manual contact tracing is likely to remain the predominant method. &lt;/p&gt;
&lt;p&gt;Dr Chia Shi-Lu, chairman of the Singapore Government Parliamentary Committee for Health, recently &lt;a href="https://www.tnp.sg/news/singapore/tracetogether-app-should-be-mandatory-all-experts"&gt;called for TraceTogether to become mandatory&lt;/a&gt; for the entire population. This negates previous assurances that the app was on a strictly opt-in basis, which may lead to further reluctance for public participation in similar initiatives across the globe.&lt;/p&gt;
&lt;h2&gt;Engaging with Communities Helps Shape Better Informed Decisions&lt;/h2&gt;
&lt;p&gt;How can developers and policy makers implement tech that people can trust? &lt;/p&gt;
&lt;p&gt;The discussion surrounding the widespread implications of COVID-19 must aim to address public concerns and engage with a rich variety of researchers and ethicists as well as stakeholders within the community. In doing so, responsible technologies and policies may be developed which encompass the values of those affected and respect personal and public boundaries.&lt;/p&gt;
&lt;p&gt;The sense of collective responsibility within communities has been demonstrated as people have been required to act in unison to combat the challenges at hand. Almost everyone has willingly sacrificing personal liberties to remain at home or placed their own selves at risk by travelling to work every day and providing humanities essential services. &lt;/p&gt;
&lt;p&gt;The public have demonstrated their ability to use their judgment and common sense in face of COVID-19 and should be trusted to do so moving forwards. Increasingly invasive policing and surveillance measures risk building resentment towards authority. This threatens the inter-reliance which binds communities and encourages this kind of positive collaboration. &lt;/p&gt;
&lt;p&gt;As expounded by global health emergency expert &lt;a href="https://www.nuffieldbioethics.org/publications/research-in-global-health-emergencies/"&gt;Katherine Wright&lt;/a&gt;, an environment of trust, which in turn encourages public compliance, can only be achieved through a regulatory response that is locally acceptable. This requires the appropriate assessment and incorporation of the values of those affected by given measures. When stakeholder’s concerns are sidelined in the development of solutions, effectiveness becomes compromised as people’s willingness to comply diminishes.&lt;/p&gt;
&lt;h2&gt;The Time to Set an Ethical Precedent is Now&lt;/h2&gt;
&lt;p&gt;As we move through uncertain terrain it is difficult to imagine what circumstances will develop in the coming months and the implications they may bring, making it essential that we set an ethical precedent now. More invasive tech solutions do present public health benefits, but they are not infallible, and the trade-off is personal privacy, autonomy and dignity. &lt;/p&gt;
&lt;p&gt;While the current crisis may seem to justify measures which infringe upon our personal and public boundaries, it also presents an opportunity to identify these core values not merely as a luxury afforded in times of normality, but as key components of the framework within which these policies are formed.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: May 12 - May 19</title><link href="/ai-ethics-news-roundup-may12-may19-2020.html" rel="alternate"></link><published>2020-05-19T09:20:00+02:00</published><updated>2020-05-19T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-05-19:/ai-ethics-news-roundup-may12-may19-2020.html</id><summary type="html">&lt;p&gt;@NeurIPSConf introduces impact statements, @_karenhao on facebooks ai moderators, @j2bryson anonymity is cancelled, @mitsmr on AI and COVID-19 @AnoukRuhaak open letter to NHS @mathbabedotorg FEMA's COVID AI model @3QD Systemic relevance &amp;amp; philosophy&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;NeurIPS to require ethical and social impact statements&lt;/h2&gt;
&lt;p&gt;"NeurIPS has introduced a requirement that all paper submissions include a statement of the “potential broader impact of their work, including its ethical aspects and future societal consequences.” This is an exciting innovation in scientifically informed governance of technology (Hecht et al 2018 &amp;amp; Hecht 2020). It is also an opportunity for authors to think about and better explain the motivation and context for their research to other scientists."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@operations_18894/a-guide-to-writing-the-neurips-impact-statement-4293b723f832"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Scientists are drowning in COVID-19 papers. Can new tools keep them afloat?&lt;/h2&gt;
&lt;p&gt;"Timothy Sheahan, a virologist studying COVID-19, wishes he could keep pace with the growing torrent of new scientific papers about the disease and the novel coronavirus that causes it. But there are just too many—more than 4000 alone last week. “I’m not keeping up,” says Sheahan, who works at the University of North Carolina, Chapel Hill. “It’s impossible.”"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.sciencemag.org/news/2020/05/scientists-are-drowning-covid-19-papers-can-new-tools-keep-them-afloat"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI in the Courtroom: A Comparative Analysis of Machine Evidence in Criminal Trials&lt;/h2&gt;
&lt;p&gt;"As artificial intelligence (AI) has become more commonplace, the monitoring of human behavior by machines and software bots has created so-called machine evidence. This new type of evidence poses procedural challenges in criminal justice systems across the world due to the fact that they have traditionally been tailored for human testimony. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3602038"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Growing Role Of IoT In COVID-19 Response&lt;/h2&gt;
&lt;p&gt;"COVID-19 broke out and it’s highly infectious nature was discovered and healthcare professionals all around the world face the challenge of treating the diseased with minimal contact. This pandemic advanced the modification and deployment of IoT devices to support the healthcare sector."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.iotforall.com/the-growing-role-of-iot-in-covid-19-response/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Facebook’s AI is still largely baffled by COVID-19 misinformation&lt;/h2&gt;
&lt;p&gt;"In its latest Community Standards Enforcement Report, released today, Facebook detailed the updates it has made to its AI systems for detecting hate speech and disinformation. The tech giant says 88.8% of all the hate speech it removed this quarter was detected by AI, up from 80.2% in the previous quarter. The AI can remove content automatically if the system has high confidence that it is hate speech, but most is still checked by a human being first."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/05/12/1001633/ai-is-still-largely-baffled-by-covid-misinformation/amp/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;2 short pieces from Dr. Joanna Bryson&lt;/h2&gt;
&lt;p&gt;"I don’t think we are ever going to have anonymity again." from &lt;a href="https://ieai.mcts.tum.de/wp-content/uploads/2020/05/Reflections-on-AI-Ethics-FINAL.pdf"&gt;Reflections on AI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another short piece we missed last week: &lt;a href="https://joanna-bryson.blogspot.com/2020/05/big-data-is-not-win-rumpelstiltskin-ai.html"&gt;"Big data" is not a win: the Rumpelstiltskin AI (Rumpelstilzchen KI) fallacy and manifesto &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"The Rumpelstiltskin (originally, Rumpelstilzchen) theory of AI is just wrong. You do not automatically get more or better intelligence in proportion to the amount of data you use. "&lt;/p&gt;
&lt;h2&gt;For all its sophistication, AI isn't fit to make life-or-death decisions&lt;/h2&gt;
&lt;p&gt;"Following the science’ is a disingenuous policy because mathematical reckoning and human judgments are very different things"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/commentisfree/2020/may/16/for-all-its-sophistication-ai-isnt-fit-to-make-life-or-death-decisions-for-us"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;If We're Not Careful, Tech Could Hurt the Fight against COVID-19&lt;/h2&gt;
&lt;p&gt;"Even well-meant technologies can shift power away from those they purport to help. We have come to recognize that while the desire to help during COVID-19 is right, the rush to push just any COVID-19 technology is wrong and even has the potential to kill."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://blogs.scientificamerican.com/observations/if-were-not-careful-tech-could-hurt-the-fight-against-covid-19/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI, Robots, and Ethics in the Age of COVID-19&lt;/h2&gt;
&lt;p&gt;"Before COVID-19, most people had some degree of apprehension about robots and artificial intelligence. Though their beliefs may have been initially shaped by dystopian depictions of the technology in science fiction, their discomfort was reinforced by legitimate concerns. Some of AI’s business applications were indeed leading to the loss of jobs, the reinforcement of biases, and infringements on data privacy.&lt;/p&gt;
&lt;p&gt;Those worries appear to have been set aside since the onset of the pandemic as AI-infused technologies have been employed to mitigate the spread of the virus."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://sloanreview.mit.edu/article/ai-robots-and-ethics-in-the-age-of-covid-19/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Digital epidemiology: the ethics of using health data in a pandemic&lt;/h2&gt;
&lt;p&gt;"COVID-19 has forced the rapid adoption of digital healthcare – from telehealth to remote monitoring. Digital epidemiology is an area of technological adoption in medicine that can help inform the future of disease surveillance, but there are ethical questions about how this data is used."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.healtheuropa.eu/digital-epidemiology-the-ethics-of-using-health-data-in-a-pandemic/99895/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Overcoming Barriers to Cross-cultural Cooperation in AI Ethics and Governance&lt;/h2&gt;
&lt;p&gt;"Achieving the global benefits of artificial intelligence (AI) will require international cooperation on many areas of governance and ethical standards, while allowing for diverse cultural perspectives and priorities. There are many barriers to achieving this at present, including mistrust between cultures, and more practical challenges of coordinating across different locations. This paper focuses particularly on barriers to cooperation between Europe and North America on the one hand and East Asia on the other, as regions which currently have an outsized impact on the development of AI ethics and governance. We suggest that there is reason to be optimistic about achieving greater cross-cultural cooperation on AI ethics and governance.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007/s13347-020-00402-x"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Special Issue on Algorithmic Bias and Fairness in Search and Recommendation&lt;/h2&gt;
&lt;p&gt;"We solicit different types of contributions (research papers, surveys, replicability and reproducibility studies, resource papers, systematic review articles) on algorithmic bias in search and recommendation, focused but not limited to the following areas. If in doubt about the suitability, please contact the Guest Editors."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.journals.elsevier.com/information-processing-and-management/call-for-papers/special-issue-on-algorithmic-bias-and-fairness-in-search"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Open Letter: NHS Covid-19 datastore&lt;/h2&gt;
&lt;p&gt;We urge the NHS to provide answers to all of the questions below and to not proceed with the development of the datastore until the public has had a chance to have their say.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@anoukruhaak/open-letter-b7cb79832064"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Congress is shaping data privacy laws during the pandemic&lt;/h2&gt;
&lt;p&gt;"After saving lives, the most urgent — and hotly debated — problem facing government policymakers in the age of COVID-19 may be how to strike a balance between privacy and public health. The fast-moving and unprecedented story around surveillance tech highlights a long-delayed push for comprehensive consumer data privacy laws, even as privacy advocates grudgingly agree that governments may need to suspend some civil liberties during the pandemic. It’s about a global scramble to stop the spread of COVID-19 and get everyone back to work — without killing privacy or a lot of people in the process."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://venturebeat.com/2020/05/18/how-congress-is-shaping-data-privacy-laws-during-the-pandemic/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Let’s Weaponize Social Media Against COVID-19&lt;/h2&gt;
&lt;p&gt;The WHO &lt;a href="http://www.euro.who.int/en/media-centre/sections/statements/2020/statement-behavioural-insights-are-valuable-to-inform-the-planning-of-appropriate-pandemic-response-measures"&gt;recently announced&lt;/a&gt; it will look at so-called "nudges" to improve pandemic response. &lt;/p&gt;
&lt;p&gt;In this article, Vivek Krishnamurthy argues that: &lt;/p&gt;
&lt;p&gt;"It’s time for a different approach to dealing with the scourge of coronavirus-related misinformation—one that leverages the most powerful and dangerous features of online platforms to advance the cause of public health. At this time of crisis, we should harness the very behavioral advertising techniques that have been so widely misused in recent years to help us combat the coronavirus."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://slate.com/technology/2020/05/weaponize-social-media-coronavirus.html?via=recirc_recent"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Google and Apple outflanked governments in the race to build coronavirus apps&lt;/h2&gt;
&lt;p&gt;"Tech giants played hardball in forcing policymakers to fall in line with their approach to building digital tracking tools."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.politico.eu/article/google-apple-coronavirus-app-privacy-uk-france-germany/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;‘Systemic relevance’ and the value of philosophy&lt;/h2&gt;
&lt;p&gt;"Academic philosophers do not predict the virus’s spread like epidemiologists. They don’t take care of patients in hospitals like nurses, and they do not stack the much-desired toilet paper into supermarket shelves. What, then, is their societal relevance?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.3quarksdaily.com/3quarksdaily/2020/05/systemic-relevance-and-the-value-of-philosophy.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: May 5 - May 12</title><link href="/ai-ethics-news-roundup-may5-may12-2020.html" rel="alternate"></link><published>2020-05-12T09:20:00+02:00</published><updated>2020-05-12T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-05-12:/ai-ethics-news-roundup-may5-may12-2020.html</id><summary type="html">&lt;p&gt;@johnchavens @nextvisions happiness &amp;amp; AI @ruha9 &amp;amp; @RadicalAIPod race &amp;amp; AU @rebheilweil AI background checks @mathbabedotorg FEMA's COVID-19 model @IEEESpectrum AI &amp;amp; COVID-19 @hackylawyER blockchain &amp;amp; immunity passports @strwbilly covid-19 changes behaviour, confuses ML models&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;A Metric Learning Reality Check&lt;/h2&gt;
&lt;p&gt;"Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than dou-bling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental setup of these papers, and propose a new way to evaluate metric learning algorithms. Finally, we present experimental results that show that the improvements over time have been marginal at best."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/pdf/2003.08505.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;About positive psychology and ethics in AI&lt;/h2&gt;
&lt;p&gt;"In the third episode of the Next Visions Podcast, the Creative Director Florian Schmitt and John C. Havens, Director of the Global Initiative on Ethics of autonomous and intelligent systems, meet in the Academia das Ciências in Lisbon’s Old Town to talk about happiness and its interface with technology."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/next-level-german-engineering/next-visions-podcast-florian-schmitt-john-havens-b45b45a12a5b"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Love, Challenge, and Hope: Building a Movement to Dismantle the New Jim Code with Ruha Benjamin&lt;/h2&gt;
&lt;p&gt;"How is racism embedded in technological systems? How do we address the root causes of discrimination? How do we as designers and consumers of AI technology reclaim our agency and create a world of equity for all? To answer these questions and more The Radical AI Podcast welcomes Dr. Ruha Benjamin to the show.  Dr. Benjamin is Associate Professor of African American Studies at Princeton University and founder of the Just Data Lab. She is author of People’s Science: Bodies and Rights on the Stem Cell Frontier (2013) and Race After Technology: Abolitionist Tools for the New Jim Code (2019) among other publications. Her work investigates the social dimensions of science, medicine, and technology with a focus on the relationship between innovation and inequity, health and justice, knowledge, and power.Full show notes for this episode can be found at Radicalai.orgIf you enjoy this episode please make sure to subscribe, submit a rating and review, and connect with us on twitter at twitter.com/radicalaipod"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://podcasts.apple.com/us/podcast/love-challenge-hope-building-movement-to-dismantle/id1505229145?i=1000473742475"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Beware of these futuristic background checks&lt;/h2&gt;
&lt;p&gt;"Tons of people are looking for work. AI-powered background checks could stand in the way."&lt;/p&gt;
&lt;p&gt;"But experts have expressed skepticism about the role that AI can actually play in hiring. The technology doesn’t always work and can exacerbate bias and privacy problems. Inevitably, it also raises bigger questions of how powerful AI should become."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.vox.com/recode/2020/5/11/21166291/artificial-intelligence-ai-background-check-checkr-fama"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A Secret Algorithm Is Deciding Who Will Die in America&lt;/h2&gt;
&lt;p&gt;"In my book, “Weapons of Math Destruction,” I identified three properties that make a predictive algorithm particularly dangerous: It must be important, secret and destructive. FEMA’s new model has them all. If it can persuade a governor to lift stay-at-home orders in the middle of a pandemic, it’s important. As of this writing, its details remain a secret. And while what will happen in Arizona remains to be seen, it has the potential to destroy many lives by justifying bad decisions."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.bloomberg.com/opinion/articles/2020-05-07/coronavirus-don-t-let-a-secret-algorithm-decide-who-will-die"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems&lt;/h2&gt;
&lt;p&gt;The ACM CHI Conference on Human Factors in Computing Systems 2020 edition (#CHI2020) didn't take place this year, but the proceedings contain a wealth of new papers, all open access. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://chi2021.acm.org/"&gt;CHI 2021&lt;/a&gt; will be May 8-13, 2021 in Yokohama, Japan. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/proceedings/10.1145/3313831#issue-downloads"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Statement Regarding the Ethical Implementation of Artificial Intelligence Systems (AIS) for Addressing the COVID-19 Pandemic&lt;/h2&gt;
&lt;p&gt;"Digital technologies including Artificial Intelligence Systems (AIS)1 can play an important and beneficial role in addressing the COVID-19 crisis. They can help model infection dynamics and socio-economic impact, monitor physical distancing, identify vaccines and help fight disease spread. However, these same technologies can also increase surveillance of individuals and populations and undermine fundamental human values such as privacy and human agency. As we lean on AIS technology, it is therefore important to carefully navigate the possible tension between basic ethical principles and fundamental rights and values developed in non-crisis times with the need to address major public health and individual safety issues in this crisis.  "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://spectrum.ieee.org/the-institute/ieee-news/statement-regarding-the-ethical-implementation-of-artificial-intelligence-systems-ais-for-addressing-the-covid19-pandemic#.XrXYiLrc1Ws.twitter"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Our weird behavior during the pandemic is messing with AI models&lt;/h2&gt;
&lt;p&gt;"Machine-learning models trained on normal behavior are showing cracks —forcing humans to step in to set them straight."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/05/11/1001563/covid-pandemic-broken-ai-machine-learning-amazon-retail-fraud-humans-in-the-loop/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;TLDR on why blockchain-enabled immunity passports, certificates, or credentials for #COVID19&lt;/h2&gt;
&lt;p&gt;A great twitter thread from @hackylawyER (Elizabeth M. Renieris) on blockchain and immunity passports. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/hackylawyER/status/1258103375004131329"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Oppressive Things&lt;/h2&gt;
&lt;p&gt;Not directly about AI, but a provocative way to think about biases in ML and AI. &lt;/p&gt;
&lt;p&gt;"In analyzing oppressive systems like racism, social theorists have articulated accounts of the dynamic interaction and mutual dependence between psychological components, such as individuals’ patterns of thought and action, and social components, such as formal institutions and informal interactions. We argue for the further inclusion of physical components, such as material artifacts and spatial environments. Drawing on socially situated and ecologically embedded approaches in the cognitive sciences, we argue that physical components of racism are not only shaped by, but also shape psychological and social components of racism. Indeed, while our initial focus is on racism and racist things, we contend that our framework is also applicable to other oppressive systems, including sexism, classism, and ableism. This is because racist things are part of a broader class of oppressive things, which are material artifacts and spatial environments that are in congruence with an oppressive system."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://philpapers.org/rec/LIAOT"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Apr 28 - May 5</title><link href="/ai-ethics-news-roundup-apr28-may5-2020.html" rel="alternate"></link><published>2020-05-05T09:20:00+02:00</published><updated>2020-05-05T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-05-05:/ai-ethics-news-roundup-apr28-may5-2020.html</id><summary type="html">&lt;p&gt;@wef &amp;amp; @LofredM trust,AI,COVID-19 @KayFButterfield limits of AI and COVID-19. @VincentCMueller SEP AI ethics entry @algorithmwatch #aiethics Global Inventory @AdaLovelaceInst assessing algorithmic systems @mannymoss ethics &amp;amp; industry @johndkelleher,@marguerade,@AphraK performativity of ai ethics&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Examining the Black Box: Tools for Assessing Algorithmic Systems&lt;/h2&gt;
&lt;p&gt;A new report by the Ada Lovelace Institute and DataKind UK clarifies the terms around algorithmic audits and impact assessments, and the current state of research and practice.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.adalovelaceinstitute.org/examining-the-black-box-tools-for-assessing-algorithmic-systems/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Expectations of artificial intelligence and the performativity of ethics: Implications for communication governance&lt;/h2&gt;
&lt;p&gt;"...despite societal expectations that we can design ethical AI, and public expectations that developers and governments should share responsibility for the outcomes of AI use, there is a significant divergence between these expectations and the ways in which AI technologies are currently used and governed in large scale communication systems. We conclude that discourses of ‘ethical AI’ are generically performative, but to become more effective we need to acknowledge the limitations of contemporary AI and the requirement for extensive human labour to meet the challenges of communication governance. An effective ethics of AI requires domain appropriate AI tools, updated professional practices, dignified places of work and robust regulatory and accountability frameworks."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://journals.sagepub.com/doi/10.1177/2053951720915939"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Don't Regulate Artificial Intelligence: Starve It&lt;/h2&gt;
&lt;p&gt;We think a better approach is to make AI less powerful. That is, not to control artificial intelligence, but to put it on an extreme diet. And what does AI consume? Our personal information.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://blogs.scientificamerican.com/observations/dont-regulate-artificial-intelligence-starve-it/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics of Artificial Intelligence and Robotics @ The Stanford Encyclopedia of Philosophy&lt;/h2&gt;
&lt;p&gt;The Stanford Encyclopedia of Philosophy is a high quality resource produced and edited by domain experts and widley relied on. Vincent C. Müller's new entry on AI Ethics is now live!&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://plato.stanford.edu/entries/ethics-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics Guidelines Global Inventory UPDATED&lt;/h2&gt;
&lt;p&gt;The AI Ethics Guidelines Global Inventory is a project by AlgorithmWatch that maps frameworks that seek to set out principles of how systems for automated decision-making (ADM) can be developed and implemented ethically. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://inventory.algorithmwatch.org/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics: A Self Reflection&lt;/h2&gt;
&lt;p&gt;"From my readings, I realised that there are some fundamental ethical aspects of AI, which I have listed below.Transparency &amp;amp; explainability, Privacy protection and security, Human-centred values, Accountability. Based on the above aspects, I believe that the following steps will help the organisations to fully utilise the potential of AI without compromising on the ethical side."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://towardsdatascience.com/ai-ethics-a-self-reflection-2abe8e8bfd86?gi=c4af10de3af1"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Too Big a Word: What does it mean to do “ethics” in the technology industry? We found four overlapping meanings&lt;/h2&gt;
&lt;p&gt;"A broad range of crises have convulsed the tech industry in recent years, from the Snowden revelations to racially biased algorithms, from Cambridge Analytica to the Google Walkout, or from ICE contracts for data brokers to censored search engines. The keyword inextricably bound up with discussions of these problems has been ethics. It is a concept around which power is contested: who gets to decide what ethics is will determine much about what kinds of interventions technology can make in all of our lives, including who benefits, who is protected, and who is made vulnerable."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://points.datasociety.net/too-big-a-word-13e66e62a5bf?gi=c4b21a95ed69"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Is AI trustworthy enough to help us fight COVID-19?&lt;/h2&gt;
&lt;p&gt;"Considering the rapid adoption of AI in high-stakes domains, the question of “how do we ensure trustworthy use of AI through audit frameworks?” is too important to be left to the industry. On the other hand, its responses vary a lot depending on the use-case being considered and requires a sound understanding of the system introduced and therefore cannot be addressed by policymakers alone. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.weforum.org/agenda/2020/05/covid19-coronavirus-artificial-intelligence-ai-response"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Power of AI has limits in fight against Covid-19, experts caution&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence forecasts useful for allocating healthcare resources, but less so for pinpointing end of pandemic."&lt;/p&gt;
&lt;p&gt;"Recently, a team at the Singapore University of Technology and Design sought to come up with an answer using artificial intelligence. Their algorithm predicts the end of the Covid-19 pandemic in different countries as well as for the world - and the charts have understandably been making the rounds on Twitter and picked up by media.&lt;/p&gt;
&lt;p&gt;But experts warn this type of certainty is - certainly - “too good to be true”, and an example of what to watch out for amid a cacophony of research. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.thenational.ae/business/technology/power-of-ai-has-limits-in-fight-against-covid-19-experts-caution-1.1014214"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bruce Schnier on COVID-19 Contact Tracing Apps&lt;/h2&gt;
&lt;p&gt;"Assume you take the app out grocery shopping with you and it subsequently alerts you of a contact. What should you do? It's not accurate enough for you to quarantine yourself for two weeks. And without ubiquitous, cheap, fast, and accurate testing, you can't confirm the app's diagnosis. So the alert is useless."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.schneier.com/blog/archives/2020/05/me_on_covad-19_.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COVID-19, Content Moderation and the EU Digital Services Act: Key Takeaways from CDT Roundtable&lt;/h2&gt;
&lt;p&gt;"As government leaders, policymakers, and technology companies continue to navigate the global coronavirus pandemic, CDT is actively monitoring the latest responses and working to ensure they are grounded in civil rights and liberties. Our policy teams aim to help leaders craft solutions that balance the unique needs of the moment, while still respecting and upholding individual human rights. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://cdt.org/insights/covid-19-content-moderation-and-the-eu-digital-services-act-key-takeaways-from-cdt-roundtable/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Computers Do Not Make Art, People Do&lt;/h2&gt;
&lt;p&gt;"I do not believe any software system in our current understanding could be called an "artist." Art is a social activity, and our "AI" software is still just software, mechanically following the instructions we give it.&lt;/p&gt;
&lt;p&gt;Moreover, calling a software system an artist is irresponsible, because it is misleading: it could make people think that the software has human-like intelligence, autonomy, and emotions."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://cacm.acm.org/magazines/2020/5/244330-computers-do-not-make-art-people-do/fulltext"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Apr 15 - Apr 28</title><link href="/ai-ethics-news-roundup-apr15-apr28-2020.html" rel="alternate"></link><published>2020-04-28T09:20:00+02:00</published><updated>2020-04-28T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-04-28:/ai-ethics-news-roundup-apr15-apr28-2020.html</id><summary type="html">&lt;p&gt;@MCoeckelbergh #covid-19 and the postdigital @strwbilly Google's Medical AI @mjasay human monitoring of AI bias @SandraWachter5 Fairness and AI @frossi_t on IBM's approach to AI ethics, @mediamocracy on AI procurement, @SpencerOverton holding social media companies responsible for discrimination + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;Last week we were busy hosting &lt;a href="https://twitter.com/ethicalai_co/status/1250788447385780235"&gt;an ethics workshop on COVID-19&lt;/a&gt;, so this week is a double post!&lt;/p&gt;
&lt;h2&gt;The Postdigital in Pandemic Times: a Comment on the Covid-19 Crisis and its Political Epistemologies&lt;/h2&gt;
&lt;p&gt;What does the ‘digital’ and ‘digitalization’ mean today, for education and in general? Instead of jumping to an abstract theoretical discussion in the literature, let me start with something concrete, very concrete unfortunately: the current Covid-19 pandemic that currently disrupts our lives and societies.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007/s42438-020-00119-2"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Identifying Bias in Hospital Length of Stay Algorithm&lt;/h2&gt;
&lt;p&gt;"Recognizing the need to support shorter lengths of stay, Dr. John Fahrenbach, a data scientist at the University of Chicago Medicine (UCM), developed a machine learning model that used clinical characteristics to identify patients most suitable for discharge after 48 hours."&lt;/p&gt;
&lt;p&gt;"After introducing zip codes into the model, however, a team member who reviewed the output raised concerns."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.boozallen.com/c/insight/blog/identifying-bias-in-hospital-length-of-stay-algorithm.html?dysig_tid=d275d4f5c6744ba980c01d3cad5b241a&amp;dsuserid=101300&amp;dsuserchannelid=30185&amp;dsuserchanneltype=CutAndPaste&amp;dspostid=a5e377df-426e-443a-bc75-18db6a67523c"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Checking AI bias is a job for the humans&lt;/h2&gt;
&lt;p&gt;"By pre-processing or post-processing data, or even setting datasets to expire, humans can step in to correct machine learning models"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.infoworld.com/article/3537968/checking-ai-bias-is-a-job-for-the-humans.html#tk.rss_all"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Fairness and AI&lt;/h2&gt;
&lt;p&gt;Sandra Wachter on why fairness cannot be automated.&lt;/p&gt;
&lt;p&gt;"In our paper, we analyzed the case law of the European Court of Justice and abstracted what the court considers to be “fair” based on its case law of nondiscrimination cases over recent decades.&lt;/p&gt;
&lt;p&gt;In a second step, we compared the Court’s notion of fairness to the current technical fairness metrics that have been developed by the computer science community."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/berkman-klein-center/fairness-and-ai-c5596faddd20"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Leveraging AI to Battle This Pandemic — And The Next One&lt;/h2&gt;
&lt;p&gt;"Throughout the pandemic, great emphasis has been placed on the sharing (or lack of it) of critical information across countries — in particular from China — about the spread of the disease.  By contrast, relatively little has been said about how Covid-19 could have been better managed by leveraging the advanced data technologies that have transformed businesses over the past 20 years. In this article we discuss one way that governments could  leverage those technologies in managing a future pandemic — and perhaps even the closing phases of the current one."&lt;/p&gt;
&lt;p&gt;"Implementing the technological innovations, however, will require policy changes. Existing policies covering data privacy and cybersecurity, and their respective and differing interpretations across countries, will largely prohibit the kind of personalized pandemic management approach we are advocating."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hbr.org/2020/04/leveraging-ai-to-battle-this-pandemic-and-the-next-one"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Google’s medical AI was super accurate in a lab. Real life was a different story.&lt;/h2&gt;
&lt;p&gt;"If AI is really going to make a difference to patients we need to know how it works when real humans get their hands on it, in real situations."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/?itm_source=parsely-api"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Israeli Doctors Warn Shin Bet Surveillance Actually Hindering Efforts to Combat Coronavirus&lt;/h2&gt;
&lt;p&gt;"Physicians' association claims the security service's use of technology to identify people who have come in contact with carriers of the coronavirus is not providing the right information "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.haaretz.com/israel-news/.premium-israeli-doctors-warn-shin-bet-surveillance-hindering-efforts-to-combat-coronavirus-1.8714359"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Contact-tracing apps are not a solution to the COVID-19 crisis&lt;/h2&gt;
&lt;p&gt;"We are concerned by this rising enthusiasm for automated technology as a centerpiece of infection control. Between us, we hold extensive expertise in technology, law and policy, and epidemiology. We have serious doubts that voluntary, anonymous contact tracing through smartphone apps—as Apple, Google, and faculty at a number of academic institutions all propose—can free Americans of the terrible choice between staying home or risking exposure. We worry that contact-tracing apps will serve as vehicles for abuse and disinformation, while providing a false sense of security to justify reopening local and national economies well before it is safe to do so. Our recommendations are aimed at reducing the harm of a technological intervention that seems increasingly inevitable."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/techstream/inaccurate-and-insecure-why-contact-tracing-apps-could-be-a-disaster/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Advancing AI ethics beyond compliance&lt;/h2&gt;
&lt;p&gt;"Ethical considerations must be elevated in the dialogue about AI systems across the business landscape. The level of cognitive understanding between humans and machines is inherently lower than it is between humans and other humans, yet the latter arena has been structured for centuries around ethics. Since AI relies on huge computing power, it can derive insight from massive amounts of data that would challenge human cognition. Relying only on traditional ethical approaches to decision making may be insufficient in addressing AI-powered decisions."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.ibm.com/thought-leadership/institute-business-value/report/ai-ethics"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How ‘Bias Bounties’ May Put Ethics Principles Into Practice&lt;/h2&gt;
&lt;p&gt;"The recently published paper suggests ten different approaches to turn AI ethics principles into practice. Taking a look at the recent efforts, more than 80 organisations have come up with different AI ethics principles. However, the authors of the paper firmly believe that the present set of norms and regulations is insufficient to develop a responsible AI. The team has also advised on ‘red-teaming’ to detect susceptibility, along with aligning with third-party auditing and government policies to create new regulations specific to market needs."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://analyticsindiamag.com/how-bias-bounties-may-put-ethics-principles-into-practice/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;White Paper on Data Ethics in Public Procurement&lt;/h2&gt;
&lt;p&gt;"The present white paper covers one pivotal general area of AI adoption in Europe in which a standardized general European approach is of key importance to the adoption of AI that truly respects fundamental rights and values in its governance structures, management systems, and technical, legal and social components."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dataethics.eu/publicprocurement/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;State Power to Regulate Social Media Companies to Prevent Voter Suppression&lt;/h2&gt;
&lt;p&gt;"State lawmakers should not be deterred by arguments that Section 230 of the federal Communications Act of 1934 “immunizes” social media companies from State liability. This Essay explains that Section 230 does not limit the power of States to hold social media companies legally responsible for using data collection and algorithms to target protected classes of voters with suppressive ads. By using such techniques, social media companies contribute materially to discrimination and are thus ineligible for Section 230 immunity."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://lawreview.law.ucdavis.edu/issues/53/4/feeney_symposium/53-4_overton.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Research summary: Different Intelligibility for Different Folks&lt;/h2&gt;
&lt;p&gt;"...there is a large problem in the current approach in the sense that there isn’t enough being done to meet the needs of a diverse set of stakeholders who require different kinds of intelligibility that is understandable to them and helps them meet their needs and goals. One might argue that a deeply technical explanation ought to suffice and others kinds of explanations might be derived from that but it makes them inaccessible to those who can’t parse well the technical details, often those who are the most impacted by such systems. This paper by Yishan Zhou and David Danks offers a framework to situate the different kinds of explanations such that they are able to meet the stakeholders where they are at and provide explanations that not only help them meet their needs but ultimately engender a higher level of trust from them by highlighting better both the capabilities and limitations of the systems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://montrealethics.ai/research-summary-different-intelligibility-for-different-folks/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Aligning AI to Human Values means Picking the Right Metrics&lt;/h2&gt;
&lt;p&gt;"How can we evaluate whether any particular product is ultimately producing positive outcomes for people and society? In principle we could create metrics that capture important aspects of the effect of an AI system on human lives, just as cities and countries today record a large variety of statistical indicators. These metrics would be useful to the teams building and operating the system, to researchers who want to understand what the system is doing, and as a transparency and accountability tool."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@PartnershipAI/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics at a Distance&lt;/h2&gt;
&lt;p&gt;We may feel individually powerless to contribute to social transformation. But each of us bears responsibility for helping to create a more just world.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://bostonreview.net/philosophy-religion/vafa-ghazavi-ethics-distance"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>EI Ethics Workshop on Digital Contact Tracing: Event Summary</title><link href="/dgt-event.html" rel="alternate"></link><published>2020-04-24T17:30:00+02:00</published><updated>2020-04-28T17:30:00+02:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2020-04-24:/dgt-event.html</id><summary type="html">&lt;p&gt;On 22 April 2020, Ethical Intelligence hosted a virtual ethics workshop specifically targeted at better understanding the ethical issues presented by digital contact tracing, in light of the COVID-19 crisis.&lt;/p&gt;</summary><content type="html">&lt;p&gt;On 22 April 2020, Ethical Intelligence hosted a virtual ethics workshop specifically targeted at better understanding the ethical issues presented by digital contact tracing, in light of the COVID-19 crisis. The event began with presentations on AI in healthcare and the ethics of digital contact tracing to lay the metaphorical building blocks for the group discussions that followed. This is a summary of the presentations and high level concepts that took place during the workshop. &lt;/p&gt;
&lt;h2&gt;AI, Healthcare &amp;amp; Ethics - Michael McAuley.&lt;/h2&gt;
&lt;p&gt;Disease has changed human history, with numerous disease hosts and transmission routes. The societal impact can range from moderate to serious, with the current Covid-19 crisis having parallels with the first recorded cholera pandemic. It is important to determine the disease involved initially, before mutation or DNA/RNA degradation, and easier to chart its spread at the beginning of an outbreak, potentially helping to change its course and save lives, alongside public engagement and education. AI technology can help by utilizing populations to track and trace disease, as well as alert people about any necessary behavioural changes, in addition to rapid vaccine development, virtual consultations, advanced screening and diagnostic software etc.&lt;/p&gt;
&lt;p&gt;The ethical debate must consider the four main principles of medical ethics starting with Beneficence, which is to ensure what is being proposed is for the benefit of individuals, making the well-being of people the primary concern, with due diligence in the design of AI, consideration for the impact on the patient-prescriber bond and, vitally, the technological prowess of healthcare professionals. Non-Maleficence, which is to evaluate the merit of the proposal to ensure no harm is caused, concerns advising creators of AI systems to identify the potential human rights implications of their creations, with a timely redress of any discriminatory outputs, while bearing in mind the impact on research, as AI may struggle to take the same approach to hazard perception as human beings. Justice, which is concerned with the proposal being fair and cost-effective, requires looking for bias, which can be introduced in development through a lack of diversity in input information, and determining whether the parameters for adoption require more than one type of data e.g. simulation data, as well as performing a cost-benefit analysis. Autonomy considers if individuals are willing participants, so investigating if the technology is moral and its role in decision-making, which must be transparent, understandable and reviewable by a competent human authority, such as a doctor or pharmacist. Protecting patients and professional autonomy will require a large number of organisations to work collaboratively and consider both the legal implications of utilising AI and the negligence implications on registrants of professional bodies. &lt;/p&gt;
&lt;p&gt;We have a robust understanding of medical ethics, but there remain many risks. In business these include future liability issues, financial penalties, loss of stakeholder trust and irreparable brand damage, while individual risks comprise matters such as privacy implications, data use and consent. We need to turn these risks into opportunities with positive, human-centric solutions.&lt;/p&gt;
&lt;h2&gt;Digital Contract Tracing - Andrew Buzzell&lt;/h2&gt;
&lt;p&gt;AI Ethics and tech ethics generally often leans heavily on functional and risk analysis. The justification for the use of technology, and for actions based on outputs from AI systems, depends on part on their strength, reliability, and the kinds of failures that can occur. Digital contact tracing depends on the viability of using bluetooth signal strength on mobile devices as a non-causal predictor of COVID-19 transmission. Properties of COVID-19, such as mode of transmission, infectivity, virulence, and pathogenicity, the nature of bluetooth radio signals, and the prevalence of compatible devices and the usage of the devices will all interact and affect the real life efficacy of DCT. &lt;/p&gt;
&lt;p&gt;Further  analysis reveals that this efficacy is not smoothly distributed - there will be uneven representation among identifiable groups of individuals along socioeconomic and other axes that will directly influence the degree to which DCT apps can detect real transmission events. &lt;/p&gt;
&lt;p&gt;This in turn should inform our ethical reflection on the kinds of actions that we should take as a result of a transmission event predicted by DCT, and the kinds of coercion that could be acceptable to drive adoption. It also should colour our analysis of the tradeoffs we might make legally, economically, and socially in order to use DCT. There has been significant discussion of the privacy tradeoffs, and the interaction between the technical power of big tech powers to make DCT easier to use (for example, by relaxing limitations on running apps access to the bluetooth antenna, and facilitating the transfer of data), and the demands of governments for support implementing DCT. &lt;/p&gt;
&lt;p&gt;There will be significant generation of false positives and false negatives, and highly uneven adoptions and coverage of DCT. As a result, the kinds of followup we take, such as automatic self-quarantine or mandated testing, will not only imperfectly track actual transmission risk, it will also affect some groups of people more than others. The technical question "can DCT work to control the spread of COVID-19" is thus tightly linked to the ethical question "should we use DCT as part of our effort to control the spread of COVID-19". There are some ethical questions we might wish to consider as being immune to context-sensitive factors - we might think some kinds of privacy are so important that no distal benefit could justify compromising. But a great deal of the ethical considerations we care about are in fact sensitive to the trade-offs we want to make, so there is value in being as clear as we can be about the real viability of DCT. Then we can lean on existing frameworks in public health ethics to begin to think about what ethical and effective deployment could look like, if it is possible at all. &lt;/p&gt;
&lt;h2&gt;Group Discussions&lt;/h2&gt;
&lt;p&gt;Digital contact tracing, if deployed on national and international levels, has the potential to touch, figuratively speaking, millions of lives. If we are to fully understand the implications and impacts this technology can have on society, then we must consider it from multiple angles through a diverse set of stakeholders. &lt;/p&gt;
&lt;p&gt;Although our various discussion groups were tasked with different topics of concentration, a common theme appeared across the board: the need for trust. If DCT is to be applied on a national and even international scale, users must first be able to not only trust that it works, but also trust that there is a transparent timeframe and scope for this technology. Currently, there is a significant amount of uncertainty surrounding the effectiveness of DCT in fighting COVID-19. Without a certain level of evidence that this technology does in fact help stop the spread of the virus, potential users are reluctant to use DCT applications as they feel that in doing so they have unevenly traded a level of privacy for an unknown, or even nonexistent benefit. In addition to this, there is fear of scope creep, with potential users lacking trust in the governments and companies deploying contact tracing to not use it for alternative means post COVID-19. Overall, the majority of considerations, implications, and fears surrounding DCT can all point back to the central concern of lacking trust in the technology and those deploying it. &lt;/p&gt;
&lt;p&gt;Extending beyond this concern for lack of trust, we dug into the considerations that need to be made in terms of bias, consent and privacy, and the short term versus long term implications of DCT within the various discussion groups. Looking into the potential sources of bias within DCT, we saw concerns surrounding access appear in forms such as a lack of proper cell phone coverage in rural areas and lack of properly updated smartphones in lower income households. When reflecting on issues of privacy and consent, there was a general consensus that these two issues are often communicated as binary entities, when in reality they exist on a scale. When classified in binary terms, more complications are created, but when consent and privacy are allowed to exist on a scale, many concerns are already addressed by the increase of control of personal data. Beyond the current application of DCT, there were a plethora of concerns on what DCT could mean in the future for our digital identities. If companies or even governments, require the use of DCT, what does that mean for individual consent and privacy? Furthermore, if DCT applications are tied to our access to resources, there is an even bigger increase of risk for bias, discrimination, and “shaming”. &lt;/p&gt;
&lt;p&gt;A global framework for track and trace data capture is possible, but many factors need to be addressed prior to implementation. Issues to address include international education and human rights, as well as ensuring that individuals are informed about data and how they consent to its use, as well as the implications of its capture and storage to them. Political freedoms need to be protected, and if this technology is misused or a framework cannot be agreed, then treaties such as CTBT may need to be used to control widespread human rights abuses. We must be positive about innovation and the future and do our best to build trust between nations and educate creators and innovators, while empowering  everyone to confidently engage with this global issue.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>A Summary of The European Commission White Paper on Artificial Intelligence – a European approach to excellence and trust</title><link href="/ec-wp-2020.html" rel="alternate"></link><published>2020-04-16T07:20:00+02:00</published><updated>2020-04-16T07:20:00+02:00</updated><author><name>Volha Litvinets</name></author><id>tag:None,2020-04-16:/ec-wp-2020.html</id><summary type="html">&lt;p&gt;On 19 February 2020, the European Commission launched a Consultation on Artificial Intelligence. Citizens and stakeholders and invited to provide their feedback by 14 June 2020. EI Expert Volha Litvinets provides an overview and discussion of the paper.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In February 2020 the European Commission released &lt;a href="https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf"&gt;"A White paper on Artificial Intelligence - A European approach to excellence and trust"&lt;/a&gt; aiming to give a definition of AI, underlining it’s benefits and technological advances in different areas, including medicine, security, farming, as well as identifying it’s potential risks: opaque decision making, gender inequality, discrimination, lack of privacy. Based on a &lt;a href="https://ec.europa.eu/digital-single-market/en/artificial-intelligence"&gt;European strategy for AI&lt;/a&gt; presented in April 2018, the current white paper is a complex document analyzing strengths, weaknesses, opportunities of Europe in the global market of Artificial Intelligence. &lt;/p&gt;
&lt;p&gt;In 2018 33 zettabytes of data was produced and it’s expected to exceed 175 zettabytes in 2025. The rapid development of new technologies and the increasing role of AI provokes a global competition and needs a global approach, in which Europe has to identify its own role. The European commission emphasizes creating multidisciplinary international cooperation practices between a private and public sector and academia. Furthermore, AI governance provokes debates and should guarantee maximum multi-stakeholders and multidisciplinarity in the European, national and international levels, and partnership between academia and the private and public sectors.&lt;/p&gt;
&lt;p&gt;The Guidelines of the High-Level Expert Group identified seven key requirements: technical robustness and safety, human agency and oversight, privacy and data governance, transparency, diversity, non-discrimination and fairness, societal and environmental wellbeing, accountability. &lt;/p&gt;
&lt;p&gt;With this, EU embraces the responsibility to addressing risk in the use and development of technologies, which must be developed according to the European values: to promote peace, to offer freedom, security, and justice, sustainable development, to combat discrimination, to ensure scientific and technological progress, to respect the culture and linguistic diversity. &lt;/p&gt;
&lt;h2&gt;Leadership in AI&lt;/h2&gt;
&lt;p&gt;Europe has an advantage for users and for technology development, a strong academic sector, innovative startups, and multiple manufacturing services in the fields of healthcare, finances, agriculture.  Europe is also a leader of AI algorithmic foundations. In addition to this, a quarter of all industrial service robots are produced in Europe. Nevertheless, Europe has a weak position in developing applications for customers, as well as a lack of investment, skills, and trust in AI, which is a significant disadvantage in the use of data assets.  The EU is a global leader in low-power electronics and neuromorphic solution, but the market of AI processor is dominated by non-EU players, European Processor Initiative can change this.&lt;/p&gt;
&lt;p&gt;The objective of the EU now is to become an attractive, safe and efficient data-agile economy, the global leader in AI. With this, the EU wants to make sure the developing technologies will be beneficial for the European citizens, “&lt;em&gt;improving their lives while respecting their rights&lt;/em&gt;”.&lt;/p&gt;
&lt;h2&gt;Increasing investments in AI&lt;/h2&gt;
&lt;p&gt;Over the past three years, EU funding for AI research has increased by 70% compared to the previous period and achieved an amount of €1.5 billion. To compare and show the need of the EU to increase the funds for AI research and development. In 2016 the EU invested in AI €3.2 billion, North America - around €12.1 billion in and Asia - €6.5 billion. &lt;/p&gt;
&lt;p&gt;Europe holds a large amount of under-used public and industrial data and has a secure digital system with low-power consumption. In order to ensure global leadership, the EU supports the investment-oriented approach. Europe needs to significantly increase its investments in this sector and to do so, there is a need to invest in next-generation technologies by mobilizing private and public funding.&lt;/p&gt;
&lt;p&gt;In December 2018 the Commission presented a Coordinated Plan aimed to force the AI progress development in Europe, proposing 70 joint actions in research, funding, market uptake, talent acquisition, international and multidisciplinary cooperation. The plan is to be adopted by the end of 2020. The objective of the European Union is to attract over €20 billions of investment per year from the Digital Europe Program, Horizon Europe as well as from the European Structural and Investment Fund.&lt;/p&gt;
&lt;h2&gt;Human-centric technologies, privacy as a fundamental human right.&lt;/h2&gt;
&lt;p&gt;The technologies have to be developed in compliance with EU rules, protecting fundamental rights and consumers aimed to give citizens confidence in AI systems, “&lt;em&gt;European AI is grounded in our values and fundamental rights such as human dignity and privacy protection&lt;/em&gt;”. With this, Europe wants to ensure the trust to tech by citizens, saying that the &lt;em&gt;trustworthiness necessary components&lt;/em&gt; in the tech development, which is impossible without expandability of opaque technologies, and from another perspective, considering poor awareness of digital users.  &lt;/p&gt;
&lt;h2&gt;Protection of Human Autonomy &amp;amp; Agency&lt;/h2&gt;
&lt;p&gt;While the European citizens are afraid of algorithmic decision-making capacities, countries are struggling with the legal uncertainty. This document states, that &lt;em&gt;AI is a collection of technologies that combine data, algorithms and computing power&lt;/em&gt;. All these three components can be biased, and by following, can lead to the material and immaterial harm and another unpredictable consequence. According to the EU, a significant role to achieve sustainable development goals (SDG), and ensure the democratic process and human rights. There should be concrete actions to protect human agency and autonomy and educate conscious digital citizens.&lt;/p&gt;
&lt;h2&gt;AI Ethics &amp;amp; research fragmentation&lt;/h2&gt;
&lt;p&gt;The complex nature of many new technologies results in cases where AI can be used to protect fundamental human rights, but can also be used for malicious purposes. As was mentioned above, international cooperation on AI matters must be based on the respect of fundamental rights, including human dignity, pluralism, inclusion, nondiscrimination and protection of privacy and personal data. &lt;/p&gt;
&lt;p&gt;The big issue with AI Ethics is a research fragmentation. The current situation with a fragmented knowledge landscape is not acceptable anymore, so it is critical to creating synergies between the multiple European research centers for cooperation in research and will create testing centers. The aim of an updated Digital Education Action Plan is to reinforce tech skills. &lt;/p&gt;
&lt;p&gt;EU position aims to promote the ethical use of AI.  The ethical guidelines were developed by the High-Level Expert Group. EU was also closely involved in developing the OECD’s ethical principles for AI. The G20 subsequently endorsed these principles in June 2019. EU recognizes that important work on AI by UNESCO, the Council of Europe, OECD, WTO, and ITU.  At the UN, the EU is involved in the follow-up of the report of the High-Level Panel on Digital Cooperation supporting regulatory convergence. &lt;/p&gt;
&lt;h2&gt;Legal Framework&lt;/h2&gt;
&lt;p&gt;The purpose of this white paper is to set out policy options and legal frameworks, based on European fundamental values to become a global leader in innovation in the data economy and its applications, and to develop a benefic AI ecosystem for citizens, business and public interest on both national and international levels. The Report, which accompanies this white paper, analyses the relevant legal framework and underlines its uncertainty.  In 2019 over 350 companies have tested this assessment list and sent feedback. A key result of the feedback process is that requirements are already reflected in existing legal or regulatory regimes, those regarding transparency, traceability, and human oversight are not specifically covered under current legislation. &lt;/p&gt;
&lt;p&gt;The regulatory framework requires compliance with EU legislation, principles, and values: freedom of expression, freedom of assembly, human dignity, gender, race, ethnic origin, religion or belief, disability, age or sexual orientation nondiscrimination, protection of personal data and private life. The document says that AI needs to be considered during the whole lifecycle, but machine learning, especially deep learning, presents challenges involving explainability that problematizes compliance with some policy goals. Europe has an academic strength in quantum computing and quantum simulators, and the document encouragis increase in the availability of testing and experimentation facilities in this field.&lt;/p&gt;
&lt;p&gt;In the white paper, it is clear that the EU legislative framework will include legislation. Some specific features of AI (e.g. opacity, complexity, unpredictability, and partially autonomous behavior) can be hard to verify and make the enforcement of legislation more difficult.  As a result, in addition to the current legislation, &lt;em&gt;new legislation specific to AI is needed&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The EU commission underlines the importance of improving digital literacy for all citizens and raising awareness of the issues related to data privacy, transparency, the definition of AI, data governance, responsibility, and trust and dual-use of technologies. The European Commission invited citizens to send comments and possible suggestions regarding this white paper.&lt;/p&gt;
&lt;p&gt;The EC &lt;a href="https://ec.europa.eu/commission/future-europe/feedback-future-europe_en"&gt;invites public comments&lt;/a&gt; by June 14. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: April 7-April 15</title><link href="/ai-ethics-news-roundup-april7-april15-2020.html" rel="alternate"></link><published>2020-04-15T09:20:00+02:00</published><updated>2020-04-15T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-04-15:/ai-ethics-news-roundup-april7-april15-2020.html</id><summary type="html">&lt;p&gt;A mini-roundup on digital-contact-tracing and #COVID19, @j2bryson AI and human agency @craigss #machinelearning and sports @Floridi/@JoshCowls/@RosariaTaddeo on AI4SG @AlexCEngler on #AI and #COVID19 hype @hackylawyER on selling your data + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;How to Design AI for Social Good: Seven Essential Factors&lt;/h2&gt;
&lt;p&gt;"The idea of artificial intelligence for social good (henceforth AI4SG) is gaining traction within information societies in general and the AI community in particular. It has the potential to tackle social problems through the development of AI-based solutions. Yet, to date, there is only limited understanding of what makes AI socially good in theory, what counts as AI4SG in practice, and how to reproduce its initial successes in terms of policies. This article addresses this gap by identifying seven ethical factors that are essential for future AI4SG initiatives. The analysis is supported by 27 case examples of AI4SG projects. Some of these factors are almost entirely novel to AI, while the significance of other factors is heightened by the use of AI. From each of these factors, corresponding best practices are formulated which, subject to context and balance, may serve as preliminary guidelines to ensure that well-designed AI is more likely to serve the social good."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007%2Fs11948-020-00213-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A guide to healthy skepticism of artificial intelligence and coronavirus&lt;/h2&gt;
&lt;p&gt;"The COVID-19 outbreak has spurred considerable news coverage about the ways artificial intelligence (AI) can combat the pandemic’s spread. Unfortunately, much of it has failed to be appropriately skeptical about the claims of AI’s value. Like many tools, AI has a role to play, but its effect on the outbreak is probably small. While this may change in the future, technologies like data reporting, telemedicine, and conventional diagnostic tools are currently far more impactful than AI."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/research/a-guide-to-healthy-skepticism-of-artificial-intelligence-and-coronavirus/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethical Implications of the Use of AI to Manage the COVID-19 Outbreak&lt;/h2&gt;
&lt;p&gt;"In the process of managing this pandemic, many artificial intel-ligence (AI) -based tools have been used (or their potential has been discussed) in order to gather and analyze relevant data, develop treatments, make medical decisions, track infected populations and manage quarantines and information dissemi-nation. In this Research Brief, we outline some of the current and potential uses for AI-based tools in managing pandemics and discuss the ethical implications of these efforts."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ieai.mcts.tum.de/wp-content/uploads/2020/04/April-2020-IEAI-Research-Brief_Covid-19-FINAL.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;You Really Don’t Want to Sell Your Data&lt;/h2&gt;
&lt;p&gt;"Proposals that would let people sell their information seem empowering—but they aren’t."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://slate.com/technology/2020/04/sell-your-own-data-bad-idea.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What are people for? Employment and the real existential threat of AI&lt;/h2&gt;
&lt;p&gt;"When people choose to pretend that the machines themselves have agency, they allow the people who programmed them, designed them, sold them, bought them, determined how to use them, operated them, or were supposed to regulate and check them – all these people are let off the hook from having done their jobs appropriately, if we allow anyone to say that the machine is the one that acted"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://joanna-bryson.blogspot.com/2020/04/employment-and-real-existential-threat.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Want to get better at sports? Listen to the Machines&lt;/h2&gt;
&lt;p&gt;Companies are now "...using the pattern-recognizing power of machine learning to revolutionize coaching and make advanced analytics available to teams of all kinds."&lt;/p&gt;
&lt;p&gt;Interesting tidbit: &lt;/p&gt;
&lt;p&gt;"Seattle Sports Sciences uses Labelbox, a training data platform that allows Mr. Milton’s data science team in Seattle to work with shifts of workers in India who label data 24 hours a day. “That’s how fast you have to move to compete in modern vision A.I.,” Mr. Milton said. “It’s basically a labeling arms race"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/04/08/technology/ai-sports-athletes-machine-learning.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Prof. Joanna Bryson interviewed on the ethical challenges of digitalization&lt;/h2&gt;
&lt;p&gt;"...we turn to ethics. What is the trade-off between privacy and security? Can we have a thriving society and innovations without a freedom of speech? Does a digital revolution mean that we also need a revolution in governance? Will artificial general intelligence (AGI) substitute doctors and teachers in the future? Joanna Bryson, Professor of Ethics and Technology at the Hertie School of Governance in Berlin, talks about the ethical challenges we face in the process of digitalization – in the time of coronavirus crisis and beyond. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://anchor.fm/anna-litvinenko/episodes/Prof--Joanna-Bryson-about-ethical-challenges-of-digitalization-ecj6fg"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COVID-19 and Digital Contact Tracing&lt;/h2&gt;
&lt;p&gt;This has been a hot topic in the last few weeks, and we've collected a few particularly interesting pieces here. &lt;/p&gt;
&lt;p&gt;Since the TraceTogether App has been a major inspiration for DCT, this article helps contextualize the contact tracing efforts in South Korea. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ophrp.org/journal/view.php?doi=10.24171/j.phrp.2020.11.1.09"&gt;Contact Transmission of COVID-19 in South Korea: Novel Investigation Techniques for Tracing Contacts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here's a post from the product lead of TraceTogether, that helps give further background on their work:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.gds-gov.tech/automated-contact-tracing-is-not-a-coronavirus-panacea-57fb3ce61d98?gi=775d620650c5"&gt;Automated contact tracing is not a coronavirus panacea&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There's a nice roundup of the privacy concerns with contact tracing here:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.washingtonpost.com/news/powerpost/paloma/the-cybersecurity-202/2020/04/14/the-cybersecurity-202-privacy-experts-fear-a-boom-in-coronavirus-surveillance/5e94901988e0fa101a7615be/"&gt;The Cybersecurity 202: Privacy experts fear a boom in coronavirus surveillance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And also from Merve Hickok:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@MerveHickok/ethical-ai-big-data-in-times-of-pandemic-df10bee77fc8"&gt;Ethical AI &amp;amp; Big Data In Times Of Pandemic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are a core set of privacy concerns with DCT that we might address through technical means, and legal protections. &lt;/p&gt;
&lt;p&gt;Here's some model legislation that illustrates the sort of legal protections we might require:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://osf.io/preprints/lawarxiv/yc6xu/"&gt;The Coronavirus (Safeguards) Bill 2020: Proposed protections for digital interventions and in relation to immunity certificates&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here's a thread from one of the people behind &lt;a href="https://github.com/DP-3T/documents"&gt;Decentralized Privacy-Preserving Proximity Tracing&lt;/a&gt; that helps explain some of the best techincal tools we have available to protect privacy:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/mikarv/status/1246124667355660291"&gt;A bluetooth COVID proximity tracing system that works at scale, where the server learns nothing about individuals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cansu Canca argues that to make privacy-preserving work equitably, it must be mandatory. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@cansucanca/why-mandatory-privacy-preserving-digital-contact-tracing-is-the-ethical-measure-against-covid-19-a0d143b7c3b6"&gt;Why ‘Mandatory Privacy-Preserving Digital Contact Tracing’ is the Ethical Measure against COVID-19&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At the core of many worries about DCT is a concern that whatever temporary concessions we make to help combat COVID-19 will become permanent. &lt;/p&gt;
&lt;p&gt;Jathan Sadowski draws an analogy with 9/11, which spurred emergency measures that reduced freedom from surveillance that remain in effect today.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://reallifemag.com/the-authoritarian-trade-off/"&gt;Exchanging privacy rights for public health is a false compromise&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ryan Calo, in evidence to the US House Committee on Commerce, Science, and Transportation, summarizes some of the reasons why we should be cautious about DCT:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.commerce.senate.gov/2020/4/enlisting-big-data-in-the-fight-against-coronavirus"&gt;Enlisting Big Data in the Fight Against Coronavirus&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This short essay by Dr. Hannah C. McLane reminds us that "If we strictly adhere to ‘save the most lives’ principle, we will be treating more white people, more men, more wealthy people"&lt;/p&gt;
&lt;p&gt;&lt;a href="https://whyy.org/articles/a-disturbing-medical-consensus-is-growing-heres-what-it-could-mean-for-black-patients-with-coronavirus/"&gt;A disturbing medical consensus is growing. Here’s what it could mean for Black patients with coronavirus.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We've got a piece of our own up from &lt;a href="https://twitter.com/andrewbuzzell"&gt;@andrewbuzzell&lt;/a&gt; on using DCT to track COVID-19 &lt;a href="https://ethicalintelligence.co/covid19-dct-wcgw.html"&gt;COVID-19 Digital Contact Tracing - Launch it fast and debug it live. What could go wrong?&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>COVID-19 Digital Contact Tracing - Launch it fast and debug it live. What could go wrong?</title><link href="/covid19-dct-wcgw.html" rel="alternate"></link><published>2020-04-14T07:20:00+02:00</published><updated>2020-04-14T07:20:00+02:00</updated><author><name>Andrew Buzzell</name></author><id>tag:None,2020-04-14:/covid19-dct-wcgw.html</id><summary type="html">&lt;p&gt;COVID-19 Digital Contact Tracing - Launch it fast and debug it live. What could go wrong?&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;tldr; We can patch many of the issues with DCT that pose direct ethical problems, but not the deeper misalignment between the technology and the goals we are using it to pursue.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;Recently, there has been a huge amount of enthusiasm for adopting digital contact tracing (DCT), like the TraceTogether app used in Singapore, as part of a test and trace strategy to help relax physical distancing regulations and reopen economies that have been shut down to suppress the spread of COVID-19. Discussion in the AI and technology ethics community has largely divided into what can be described as compatibilist and incompatibilist positions on the possibility of implementing DCT ethically. I think the incompatibilists are right, but that there are better ways of stating the position. The primary incompatibility isn't between DCT and specific ethical challenges familiar to AI ethics, such as protecting privacy. Rather, it's a deeper tension between the limits of the technological tool and the requirements DCT would have to meet to function usefully as part of a test and trace strategy. &lt;/p&gt;
&lt;p&gt;DCT is being developed rapidly, and we will be highly motivated to make it work - billions of dollars will change hands, much of it from public coffers to private corporations, and lives really do hang in the balance. Moreover, as we learn more about the populations most likely to experience the more severe health and economic outcomes from COVID-19, it is apparent that costs and benefits will not be shared equally. &lt;/p&gt;
&lt;p&gt;The AI and technology ethics community has played a prominent role in the development and discussion of the DCT approach, and numerous proposals have been carefully developed in collaborative efforts between technologists and ethicists. Increasingly sophisticated measures to implement contact tracing anonymously such as &lt;a href="https://github.com/DP-3T/documents/blob/master/DP3T%20White%20Paper.pdf"&gt;Decentralized Privacy-Preserving Proximity Tracing&lt;/a&gt; have earned the approval of some ethicists, such as Cansu Canca, the Executive Director of AI Ethics Labs, who recently argued that there should be &lt;a href="https://medium.com/@cansucanca/why-mandatory-privacy-preserving-digital-contact-tracing-is-the-ethical-measure-against-covid-19-a0d143b7c3b6"&gt;mandatory adoption of these systems&lt;/a&gt;, in an approach she calls MPP-DCT (Mandatory Privacy Preserving Digital Contact Tracing). &lt;/p&gt;
&lt;h2&gt;Can DCT be deployed ethically?&lt;/h2&gt;
&lt;p&gt;Ethicists have been very effective bringing concerns about surveillance and privacy into the mainstream of the work on digital contact tracing, and views amongst AI and tech ethicists have largely divided into two camps. There's an incompatibilist outlook, which argues that there is no way to implement digital contact tracing that is compatible with privacy rights, and a compatibilist view, that by using technical and regulatory tools we can in fact deploy DCT in an ethical way. &lt;/p&gt;
&lt;p&gt;The compatibilists have received significant and much deserved airtime within organizations developing DCT apps, and in the broader policy discussion. This is a welcome acceptance of the essential role that ethical oversight must play in the technology development cycle as we slowly absorb the litany of research showing extensive problems with AI systems with hidden biases, unpredictable behaviour, and unanticipated social and personal and economic costs. &lt;/p&gt;
&lt;p&gt;It's not surprising that the incompatibilists receive comparatively less attention and exert less influence. Not only are some of the richest corporations on earth involved in the digital contact tracing project, but they are portrayed as an essential part of plans that could allow governments to relax public health initiatives that have substantially impeded the economic and personal freedom of millions of people. &lt;/p&gt;
&lt;p&gt;Discussion of the incompatibilist position on digital contact tracing seems to focus on a particular kind of "slippery slope" argument. If we allow them to track us to cope with the COVID-19 challenge, what will stop them from expanding this in other contexts? Technologists and ethicists have argued for years that the "If you have nothing to hide, you've got nothing to fear" attitude is problematic, and that privacy tradeoffs tend to end up a lot worse than the initial bargain reveals. This has fallen on deaf ears, in part because the problem with these tradeoffs is that there is a deep information asymmetry. We think we know what we are giving up, and it tends to be pretty innocuous, some photos we post, some basic personal info, some anonymized digital fingerprints. What could go wrong? But what we don't know, and sometimes can't know, is how they will ultimately become part of systems that cause harm, because those systems might not yet exist, or the conditions that make them harmful are not yet in place. &lt;/p&gt;
&lt;p&gt;Slippery slope arguments are vivid, appealing, and easy to devise. But they tend to be speculative and vague, and invite compelling rebuttals. Countering a slippery slope argument is easy - for any imagined bad landing, one might show that there are regulatory, economic, or technical protections that can be put in place, and then the dialectic tends to head towards a back and forth between potential pitfalls and possible solutions. As this goes on, the ground can shift, towards a higher level objection to the slippery slope, that we have to be realistic, we have to deal with the problems we are facing now, and that the argument is ultimately paranoid, contrarian, or unreasonable.&lt;/p&gt;
&lt;p&gt;The compatibilists have excellent answers to many of the slippery slope arguments against DCT. Proposals such as MPP-DCT show that tracking apps can be highly anonymous. The system proposed by Google and Apple also implements technical measures that protect individual privacy, and further innovations will doubtlessly improve these. Emergency public health legislation in effect in most countries with COVID-19 outbreaks have strict limitations that we should, in most jurisdictions, trust to function as intended. Crytopgraphic techniques to create ephemeral device fingerprints can prevent future misuse. &lt;/p&gt;
&lt;h2&gt;First Wave and Second Wave AI and Technology Ethics&lt;/h2&gt;
&lt;p&gt;Frank Pasquale, an expert on the law of artificial intelligence, algorithms, and machine learning,  recently argued that we can distinguish between what he calls &lt;a href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;"First Wave" and "Second Wave" algorithmic accountability&lt;/a&gt;. First Wave research and policy focussed on fixing the immediate emergencies created by our reliance on systems which don't do what we expect, or don't do it fairly. Second Wave AI ethics asks deeper questions about how we use these systems in the first place - whether a proposed technology can be aligned with the values and norms that matter within the domain in question, whether it should be used at all, rather than whether or not we are doing it fairly. We can fix a lot of First Wave problems for a system which is still fundamentally, morally and pragmatically misaligned. For instance, we might focus on addressing First Wave problems with AI video hiring systems by working to ensure that the data they are trained with is unbiased and used with consent, implementing technical measures to generate interpretable models, and ensuring compliance with labour and data protection regulations. With all of these First Wave problems fixed, we might still find that there is a fundamental mismatch between what the system can measure, and what actually matters when choosing a person for the job. It might just be a technological solution that can't align with our interests in the problem. &lt;/p&gt;
&lt;p&gt;The AI ethics debate about digital contact tracing has been overwhelmingly focused on First Wave problems that concern implementation challenges, such as protecting privacy, addressing data protection and ownership issues, and avoiding off-label reuse of the data collected. What are the Second Wave questions we should be asking?&lt;/p&gt;
&lt;p&gt;Instead of a slippery slope, where we worry about what might come after digital contact tracing, we should be worried about a trojan horse - what comes along with it that we haven't seen? If we incur significant sunk costs to rapidly launch DCT, what other investments will we have to make to fix it at sea, and what policies, technologies, and behaviours will we need to accept? &lt;/p&gt;
&lt;p&gt;It is important to note the economic and epidemiological viability of the test and trace strategy remains in question. For example, researchers at &lt;a href="http://www.centerforhealthsecurity.org/our-work/pubs_archive/pubs-pdfs/2020/a-national-plan-to-enable-comprehensive-COVID-19-case-finding-and-contact-tracing-in-the-US.pdf"&gt;Johns Hopkins estimate&lt;/a&gt; that this approach would require 3.6 billion dollars and 100,000 dedicated employees dedicated to maintain the “human-in-the-loop” capacity that is essential to making DCT signals actionable (indeed, TraceTogether was human-led, and the app itself merely augmented conventional contact tracing). Nobel-Laureate Paul Romer estimates that DCT would require almost &lt;a href="https://paulromer.net/covid-sim-part1/"&gt;22 million tests per day&lt;/a&gt; in the United States, a country with limited access to testing and highly variable access to health care resources. As of April 13, the United States has conducted less than &lt;a href="https://covidtracking.com/data/"&gt;3 million tests in total.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is uncertainty about the effectiveness of diagnostic and antibody tests, and the significant gaps in our understanding of the conditions of transmission. The basic reproduction number of COVID-19 is thought to be R3, which means that for every person that is infected, there will in turn be three more infected people. We contact a lot more than three people in the timespan where we would potentially transmit the virus. We know the virus can persist on surfaces and be transmitted through indirect contact. Unlike other infectious diseases where contact tracing has been used effectively, COVID-19 is contagious long before symptoms appear, enlarging the pool of contacts that must be searched. Digital contact tracing works by using low energy bluetooth signals from mobile phones, which can reach up to 20 feet, through walls, outside vehicles, and which are an imprecise proxy for contact that could lead to infection. Conventional contact tracing, in contrast, is a process conducted by experts with access to a broad spectrum of data about an individual's activities. &lt;/p&gt;
&lt;p&gt;We can look at the broad range of attack patterns familiar in the cybersecurity domain and find analogical threats to DCT, such as the denial of service and wardriving. A digital fingerprint that has been flagged as positive could be deliberately or accidentally  associated with many other devices. (For a taste of some of the vulnerabilities we can expect, see &lt;a href="https://eprint.iacr.org/2020/399"&gt;https://eprint.iacr.org/2020/399&lt;/a&gt;) Cybersecurity has historically been attentive to human factors as well as technical ones, and DCT is vulnerable to social engineering attacks and the tendency of users to quickly discover the dark logics of the systems they are forced to interact with. It won't be long before users learn to manipulate quirks in the system, for instance, it might be beneficial to have been infected so as to be flagged as such in the app. There is now doubt that antibody tests are reliable, and worries that the virus can re-activate after a period of dormancy. &lt;/p&gt;
&lt;p&gt;The reliance of DCT on non-causal predictors will drive iteration of the technology that will demand either the investment significant investment in human oversight, or the aggregation of the baseline proximity signals with other data corpora. This will raise substantial ethical and safety issues. &lt;/p&gt;
&lt;p&gt;Ethical analysis of technology should anticipate the trajectory that development of the system will take after deployment. Identifying bugs and failure modes that will require remediation within the sociotechnical paradigm of the system is a way to form hypothesis about the direction this development will take. &lt;/p&gt;
&lt;h2&gt;Even if DCT has flaws, isn’t it good enough to try?&lt;/h2&gt;
&lt;p&gt;If we launch DCT, these are problems that we will have to debug, and those solutions may have many of the features the incompatibilists are worried about. Will tech platforms need to tie the anonymous bluetooth data to their broader data holdings in order to narrow down contacts that are more likely to correlate with transmission? Will the apps need to have access to other health data in order to ensure that self-reported data is accurate, or make AI assisted medical inferences about users from other proxy signals they collect? Many of our contacts, such as meeting a delivery driver, or pumping gasoline, or running, typically happen without our mobile phones. Perhaps we'll find that we are obligated, morally, legally, or practically, to make sure we are emitting and collecting data at all times. &lt;/p&gt;
&lt;p&gt;As the limits of the actual diagnostic capacity are revealed, the apps might still have pragmatic value as a sort of security theatre, acting as a throttle on access to public spaces even when we know the mechanism isn't particularly reliable. At best it will act as quarantine-roulette, at worse it will exhibit biases that reproduce or amplify existing inequities. &lt;/p&gt;
&lt;p&gt;This isn't a slippery slope argument, it's an argument about the implication we are committed to when we adopt the logic of this technological perspective on the problem. When you build a piece of software, there's a blurred line between fixing bugs and adding features. If we want to automate the analysis of proxy signals to determine who is eligible for relaxed physical distancing requirements, using the technologies and platforms we have today, these are some of the implications that come along with it. &lt;/p&gt;
&lt;p&gt;Digital contact tracing might be a useful source of aggregate data, for macro-scale epidemiological modelling, but there are good reasons to doubt that we can use them as a reliable way of making decisions about individuals. This is not an uncommon problem with the use of big data and artificial intelligence, where we gain the capacity to generate knowledge that we often lack the resources to act on in an ethical or effective way. And yet, if we build the system, we acquire ethical and sometimes legal obligations to take responsible action with the results. It's not clear that we can with DCT, and the solutions available to try to remediate that are problematic. &lt;/p&gt;
&lt;p&gt;One might object that this overlooks the positive contribution even imperfect DCT can make. Even though we know that non-medical masks aren't reliable at preventing transmission, we endorse their use because they still help. The difference is of course the downstream commitment - with masks there are few, but with DCT they are substantial. Masks have a measurable impact on transmission, and the commitments that go with them depend on regulatory decisions about mandate and enforcement. It is true that in some places masks have been mandatory where as access to them is poor, and handing out masks would be a better exercise of government power than writing fines. But on the whole, there aren’t a lot of troubling implicated commitments with policies that encourage or mandate mask usage. For DCT, on the other hand, they are significant commitments which will be technically and politically costly to solve, and which pose substantial ethical and practical challenges. Even if one believes that efficacy has presumptive justificatory force, the degree of efficacy would need to be correspondingly high to support the full social and economic costs we would incur to implement even a plausibly effective DCT test and trace strategy. &lt;/p&gt;
&lt;h2&gt;The problems with DCT are deeper than the sort of privacy issues current proposals focus on solving.&lt;/h2&gt;
&lt;p&gt;AI and tech ethicists should embrace these Second Wave questions about the appropriate use of technology. Often First Wave issues are more tractable, and easier to solve within the institutional mandates technology ethicists operate with as part of collaborative efforts with diverse stakeholders, often spanning private and public interests. Second Wave perspectives are more likely to cast doubt fundamental assumptions about the viability of projects as a whole and the likelihood of success. We risk ethics-washing when we focus too closely on patching issues on the edges of systems that have deeper tensions with our goals and values. There was controversy this winter when the &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;GermEval 2020 machine learning contest&lt;/a&gt; included a task to develop AI that makes inferences about the “intellectual ability” of the authors of text samples. Some ethicists warned that even if we solved ethical problems at the periphery, there are fundamental ethical issues baked into problem definition itself, that should cause us to object to building software for the problem at all. Sometimes we can fix the bugs, but deeper problems with the problem definition or the limits of our tools mean we still can’t perform the task ethically. Second Wave algorithmic accountability challenges us to pay attention to these cases. &lt;/p&gt;
&lt;p&gt;The implementation of DCT will require a significant exercise of political will and economic investment that could perhaps be deployed more effectively in other ways with more empirically demonstrable likelihood of having positive impacts, such as improving access to health care services, tackling known social determinants of poor outcomes, addressing the economic costs of the distancing, and putting into action the well-established practices and protocols that will help respond to future pandemics. Of course, it’s a poor argument to claim that we can’t fix these things and also implement DCT, but in practice, political and economic capital for public health initiatives is a precious and limited resource, and we should be cautious about how we spend it. “It won’t work” isn’t just a practical problem when the measures we’ll need to take to make it work have serious ethical challenges, and even more so when some of the bugs carry serious moral risks. &lt;/p&gt;
&lt;p&gt;We might still be optimistic that AI and big data could be part of the solutions to these problems, even if we are skeptical about the value of implementing DCT. &lt;/p&gt;
&lt;p&gt;This point bears repeating. In the United States &lt;a href="https://www.propublica.org/article/early-data-shows-african-americans-have-contracted-and-died-of-coronavirus-at-an-alarming-rate"&gt;evidence is mounting&lt;/a&gt; that the heaviest impacts of COVID-19 will be on the most vulnerable populations. &lt;a href="https://www.nytimes.com/2020/04/05/opinion/coronavirus-social-distancing.html"&gt;"What the vulnerable portion of society looks like varies from country to country, but in America, that vulnerability is highly intersected with race and poverty."&lt;/a&gt;. &lt;a href="https://thestarphoenix.com/opinion/columnists/cuthand-covid-19-situation-on-u-s-reserves-is-cautionary-tale/"&gt;"The rate of COVID-19 infection is eight times higher on the Navajo Nation and the death rate is 16 times higher than the rest of New Mexico."&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In Canada, where I live, we've had a heartbreaking series of news stories about terrible conditions in senior care homes with extreme levels of COVID-19 infection, highly limited capacity for treatment and care, and terrible outcomes. I recently had a chance to hear first hand from a RN at senior care facility how dire this situation is, in a part of our society that is in many ways hidden. &lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://whyy.org/articles/a-disturbing-medical-consensus-is-growing-heres-what-it-could-mean-for-black-patients-with-coronavirus/"&gt;thoughtful essay on managing scarce health care resources&lt;/a&gt;, Dr. Hannah C. McLane reminds us that saving the most lives involves value-laden judgements. Using tools like DCT to accelerate efforts to relax measures that are currently slowing the rate of infection will amplify existing inequities in our allocation of public goods. You might think that's a distal problem, an unfortunate reality, and one that shouldn't weigh too heavily against broadly utilitarian calculations that are inescapable in &lt;a href="https://www.ncchpp.ca/docs/2016_eth_frame_upshur_En.pdf"&gt;public health ethics&lt;/a&gt;. But we should pay close attention to the extent to which the technological paradigms with which we approach problems constrain these choices, tilting the scale so that making decisions that save the most lives with the least cost just happen to favour one kind of life, and risk another. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: March 31 - April 7</title><link href="/ai-ethics-news-roundup-apr7-2020.html" rel="alternate"></link><published>2020-04-07T09:20:00+02:00</published><updated>2020-04-07T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-04-07:/ai-ethics-news-roundup-apr7-2020.html</id><summary type="html">&lt;p&gt;@ds_wats0n and @Floridi on interpretable #machinelearning, @christianmunthe on moral agency, @_KarenHao AI predicting life outcomes for children, @gabrielazanfir on data protection, @AFCEA on intelligent weapons, @VincentCMueller with an SEP entry on AI ethics + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;How artificial intelligence and machine learning are used in hiring and recruiting&lt;/h2&gt;
&lt;p&gt;"Job seekers interact more with advancing tech than they realize as more companies turn to automated tools in talent acquisition."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.zdnet.com/article/how-artificial-intelligence-and-machine-learning-are-used-in-hiring-and-recruiting/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI in the headlines: the portrayal of the ethical issues of artificial intelligence in the media&lt;/h2&gt;
&lt;p&gt;"This paper expands upon previous research by systematically analyzing and categorizing the media portrayal of the ethical issues of AI to better understand how media coverage of these issues may shape public debate about AI. Our results suggest that the media has a fairly realistic and practical focus in its coverage of the ethics of AI, but that the coverage is still shallow. A multifaceted approach to handling the social, ethical and policy issues of AI technology is needed, including increasing the accessibility of correct information to the public in the form of fact sheets and ethical value statements on trusted webpages (e.g., government agencies), collaboration and inclusion of ethics and AI experts in both research and public debate, and consistent government policies or regulatory frameworks for AI technology."&lt;/p&gt;
&lt;p&gt;There's also a &lt;a href="https://news.ncsu.edu/2020/04/how-we-write-about-ai-ethics/"&gt;nice article about this piece&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And &lt;a href="https://www.futurity.org/artificial-intelligence-news-coverage-ethics-2326172/"&gt;an interview&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;"A recent analysis of how journalists deal with the ethics of artificial intelligence suggests that reporters are doing a good job of grappling with a complex set of questions—but there’s room for improvement."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007/s00146-020-00965-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI can’t predict how a child’s life will turn out even with a ton of data&lt;/h2&gt;
&lt;p&gt;"Hundreds of researchers attempted to predict children’s and families’ outcomes, using 15 years of data. None were able to do so with meaningful accuracy."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615434/ai-machine-learning-social-outcome-prediction-study/?utm_medium=tr_social&amp;utm_campaign=site_visitor.unpaid.engagement&amp;utm_source=Twitter#Echobox=1586185183"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why data protection law is uniquely equipped to let us fight a pandemic with personal data&lt;/h2&gt;
&lt;p&gt;"Data protection law is different than “privacy”. We, data protection lawyers, have been complacent recently and have failed to clarify this loud and clear for the general public. Perhaps happy to finally see this field of law taking the front stage of public debate through the GDPR, we have not stopped anyone from saying that the GDPR is a privacy law." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://pdpecho.com/2020/04/06/why-data-protection-law-is-uniquely-equipped-to-let-us-fight-a-pandemic-with-personal-data/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;It's Time to Take Seriously the Machine Ethics of Autonomous and AI Cyber Systems&lt;/h2&gt;
&lt;p&gt;"Many concepts revolve around the law of armed conflict, societal law, ethical dilemmas, psychological concepts and artificially intelligent cyber systems, as well as their relationships among each other. In addition to the delineation of machine ethic guidelines, an ethical life cycle is necessary to account for changes over time in national circumstances and personal beliefs. Just recently, the Defense Innovation Board, which serves as an advisory board to the Pentagon, met and published ethical guidelines in designing and implementing artificially intelligent weapons. Artificial intelligence (AI) systems in the Defense Department must satisfy the conditions of responsibility, equitability, traceability, reliability and governability. The Defense Innovation Board approved five ethical principles."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.afcea.org/content/its-time-take-seriously-machine-ethics-autonomous-and-ai-cyber-systems"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics - From Principles to Practice&lt;/h2&gt;
&lt;p&gt;"We have prepared this report as experts in spheres ranging from computer science, philosophy, and technology impact assessment via physics and engineering to social sciences, and we work together as the AI Ethics Impact Group (AIEI Group). Our paper offers concrete guidance to decision-makers in organisations developing and using AI on how to incorporate values into algorithmic decision-making, and how to measure the fulfilment of values using criteria, observables and indicators combined with a context-dependent risk assessment. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.ai-ethics-impact.org/resource/blob/1961130/c6db9894ee73aefa489d6249f5ee2b9f/aieig---report---download-hb-data.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A Normative Approach to Artificial Moral Agency&lt;/h2&gt;
&lt;p&gt;"This paper proposes a methodological redirection of the philosophical debate on artificial moral agency (AMA) in view of increasingly pressing practical needs due to technological development. This “normative approach” suggests abandoning theoretical discussions about what conditions may hold for moral agency and to what extent these may be met by artificial entities such as AI systems and robots. Instead, the debate should focus on how and to what extent such entities should be included in human practices normally assuming moral agency and responsibility of participants"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.researchgate.net/publication/311196481_A_Normative_Approach_to_Artificial_Moral_Agency"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics of Artificial Intelligence and Robotics&lt;/h2&gt;
&lt;p&gt;A new Stanford Encyclopedia of Philosophy entry in on the way!&lt;/p&gt;
&lt;p&gt;"Artificial intelligence (AI) and robotics are digital technologies that will be of major importance for the development of humanity in the near future. They have raised fundamental questions about what we should do with these systems, what the systems themselves should do, what risks they involve and how we can control these. After the Introduction to the field (1), the main themes of this article are: (2) Ethical issues that arise with AI systems as objects, i.e. tools made and used by humans; here, the main sections are privacy and manipulation, opacity and bias, human-robot interaction, employment, and the effects of autonomy. (3) AI systems as subjects, i.e. when ethics is for the AI systems themselves in machine ethics and artificial moral agency. (4) The problem of a possible future AI superintelligence leading to a ‘singularity’. For each section within these themes, we provide a general explanation of the ethical issues, we outline existing positions and arguments, then we analyse how this plays out with current technologies and finally what policy consequences may be drawn"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://philarchive.org/archive/MLLEOA-4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The explanation game: a formal framework for interpretable machine learning&lt;/h2&gt;
&lt;p&gt;"We propose a formal framework for interpretable machine learning. Combining elements from statistical learning, causal interventionism, and decision theory, we design an idealised explanation game in which players collaborate to find the best explanation(s) for a given algorithmic prediction. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007/s11229-020-02629-9?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst&amp;utm_source=ArticleAuthorOnlineFirst&amp;utm_medium=email&amp;utm_content=AA_en_06082018&amp;ArticleAuthorOnlineFirst_20200404"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Mar 24 - Mar 30</title><link href="/ai-ethics-news-roundup-mar24-mar30-2020.html" rel="alternate"></link><published>2020-03-30T09:20:00+02:00</published><updated>2020-03-30T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-30:/ai-ethics-news-roundup-mar24-mar30-2020.html</id><summary type="html">&lt;p&gt;@SweeLengHarris-data protection, @ShannonVallor-AI and #covid19, @PartnershipAI-responsible disclosure, @SandraWachter5 on fairness, @akapczynski on informational capitalism, @MarcelloIenca-data &amp;amp; covid19&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Coronavirus Pandemic Could Elevate ESG Factors&lt;/h2&gt;
&lt;p&gt;"Environmental, social and governance investing was growing in popularity before the virus began to circulate, as investors flocked to companies that have taken steps to manage nonfinancial risks related to matters such as climate change, board diversity or human rights issues in the supply chain.&lt;/p&gt;
&lt;p&gt;But the pandemic has demonstrated on a large scale the importance of other factors that are paramount to ESG investors. Among them: disaster preparedness, continuity planning and employee treatment through benefits such as paid sick leave as companies direct employees to work from home."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wsj.com/articles/coronavirus-pandemic-could-elevate-esg-factors-11585167518"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI can help with the COVID-19 crisis - but the right human input is key&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence (AI) has the potential to help us tackle the pressing issues raised by the COVID-19 pandemic. It is not the technology itself, though, that will make the difference but rather the knowledge and creativity of the humans who use it. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.weforum.org/agenda/2020/03/covid-19-crisis-artificial-intelligence-creativity/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data Protection Impact Assessments as rule of law governance mechanisms&lt;/h2&gt;
&lt;p&gt;"This article explores how Data Protection Impact Assessments (DPIAs) could provide a mechanism for improved rule of law governance of data processing systems developed and used by government for public purposes in civil and administrative areas. Applying rule of law principles to two case studies provides a sketch of the issues and concerns that this article’s proposals for DPIAs seek to address. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cambridge.org/core/journals/data-and-policy/article/data-protection-impact-assessments-as-rule-of-law-governance-mechanisms/3968B2FBFE796AA4DB0F886D0DBC165D#.XoL9tjyutqQ.twitter"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Shannon Vallor on AI and Covid-19&lt;/h2&gt;
&lt;p&gt;"Thoughts on the growing debate over whether COVID-19 illustrates the moral necessity of using AI and other tech for more expansive and intrusive forms of public health surveillance: a thread"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://threadreaderapp.com/thread/1242194076293890048.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Adversarial Perturbations Fool Deepfake Detectors&lt;/h2&gt;
&lt;p&gt;Hope that the deep fake problem can be solved technologically might be misplaced, instead we might have an arms race which further undermines the digital epistemic environment. &lt;/p&gt;
&lt;p&gt;"This work uses adversarial perturbations to enhance deepfake images and fool common deepfake detectors.The DIP defense achieved 95 perturbed deepfakes that fooled the original detector, while retaining 98 accuracy in other cases on a 100 image subsample."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/adversarial-perturbations-fool-deepfake-detectors"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Publication Norms for Responsible AI&lt;/h2&gt;
&lt;p&gt;Some have argued that dangerous AI technology like deep fake generators should be protected from disclosure to the public. The partnership on AI is working on new standards for publication, and is looking for comments.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.partnershiponai.org/case-study/publication-norms/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI&lt;/h2&gt;
&lt;p&gt;"In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in AI and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Recommendations on privacy and dataprotection in the fight against COVID-19&lt;/h2&gt;
&lt;p&gt;"Governments, companies, NGOs, and individuals alike have a responsibility to do their part tomitigate the consequences of COVID-19 and to show solidarity and respect for each other. Inthis paper, we will provide ​privacy and data protection recommendations forgovernments​ to fight against COVID-19 in a rights-respecting manner."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.accessnow.org/cms/assets/uploads/2020/03/Access-Now-recommendations-on-Covid-and-data-protection-and-privacy.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;‘Trustworthy AI’ is a framework to help manage unique risk&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence (AI) technology continues to advance by leaps and bounds and is quickly becoming a potential disrupter and essential enabler for nearly every company in every industry. At this stage, one of the barriers to widespread AI deployment is no longer the technology itself; rather, it’s a set of challenges that ironically are far more human: ethics, governance, and human values."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615400/trustworthy-ai-is-a-framework-to-help-manage-unique-risk/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Law of Informational Capitalism&lt;/h2&gt;
&lt;p&gt;"I construct an account of the “law of informational capitalism,” with particular attention to the law that undergirds platform power. Once we come to see informational capitalism as contingent upon specific legal choices, we can begin to consider how democratically to reshape it. Though Cohen does not emphasize it, some of the most important legal developments—specifically, developments in the law of takings, commercial speech, and trade—are those that encase private power from democratic revision. Today’s informational capitalism brings a threat not merely to our individual subjectivities but to equality and our ability to self-govern. Questions of data and democracy, not just data and dignity, must be at the core of our concern."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.yalelawjournal.org/review/the-law-of-informational-capitalism"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Vulnerable robots positively shape human conversational dynamics in a human–robot team&lt;/h2&gt;
&lt;p&gt;"In this work, we explore how a social robot influences team engagement using an experimental design where a group of three humans and one robot plays a collaborative game. Our analysis shows that a robot’s social behavior influences the conversational dynamics between human members of the human–robot group, demonstrating the ability of a robot to significantly shape human–human interaction."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.pnas.org/content/117/12/6370"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Empathy Machine: Humans Communicate Better after Robots Show Their Vulnerable Side&lt;/h2&gt;
&lt;p&gt;"“While other work has focused on how to more easily integrate robots into teams, we focused instead on how robots might positively shape the way that people react to each other,” says Sarah Sebo, a graduate student at Yale University and co-author of the research, published this month in Proceedings of the National Academy of Sciences USA. To measure these changes in reactions, researchers at Yale and Cornell University assigned participants to teams of four—consisting of three people and one small humanoid robot—and had them play a collaborative game on Android tablets. In some groups, the robots were programmed to act “vulnerable.” These machines performed actions such as apologizing for making mistakes, admitting to self-doubt, telling jokes, sharing personal stories about their “life,” and talking about how they were “feeling.” In control groups, the human participants teamed up with robots that made only neutral statements or remained entirely silent."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.scientificamerican.com/article/empathy-machine-humans-communicate-better-after-robots-show-their-vulnerable-side/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Federal Court Rules ‘Big Data’ Discrimination Studies Do Not Violate Federal Anti-Hacking Law&lt;/h2&gt;
&lt;p&gt;"The ACLU challenged a provision of the CFAA that the government argues makes it a crime to violate a website’s terms of service. Those terms, which are unilaterally set by individual sites and can change at any time, often prohibit researchers and journalists from creating tester online identities or recording what content is served up to those identities. These practices were used by, for example, investigative journalists who exposed that advertisers were using Facebook’s ad-targeting algorithm to exclude users from receiving job, housing, or credit ads based on race, gender, age, or other classes protected from discrimination in federal and state civil rights laws."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.aclu.org/press-releases/federal-court-rules-big-data-discrimination-studies-do-not-violate-federal-anti"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;On the responsible use of digital data to tackle the COVID-19 pandemic&lt;/h2&gt;
&lt;p&gt;"Large-scale collection of data could help curb the COVID-19 pandemic, but it should not neglect privacy and public trust. Best practices should be identified to maintain responsible data-collection and data-processing standards at a global scale."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/s41591-020-0832-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: March 17 - March 24</title><link href="/ai-ethics-news-roundup-mar17-mar24-2020.html" rel="alternate"></link><published>2020-03-23T09:20:00+01:00</published><updated>2020-03-23T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-23:/ai-ethics-news-roundup-mar17-mar24-2020.html</id><summary type="html">&lt;p&gt;@rachelcoldicutt on the ethics of contact tracking, @StanfordHAI online conference on Covid19 &amp;amp; AI, @carissaveliz on Data, Privacy, and the Individual, @LydNicholas on responsible tech in a pandemic, @lawfare on using data to track covid19, @stevevosloo on child-friendly AI + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Open Letter: Contact Tracking and NHSX&lt;/h2&gt;
&lt;p&gt;"During this global emergency, technology and data-driven decisions have a vital role in saving lives by delivering essential information, building communities and managing capacity across the NHS. But they are not a magic bullet to solve unsolvable problems."&lt;/p&gt;
&lt;p&gt;"As responsible technologists, we call upon the NHSX leadership and the Secretary of State for Health and Social Care to ensure new technologies used in the suppression of Coronavirus follow ethical best practice"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@rachelcoldicutt/open-letter-contract-tracking-and-nhsx-e503325b2703"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COVID-19 and AI: A Virtual Conference&lt;/h2&gt;
&lt;p&gt;Topics to be addressed include: AI applications in diagnostics and treatment, epidemiological tracking and forecasting of the spread of the virus, information and disinformation, and the broader human impact of COVID-19 and pandemics in general on economies, culture, government, and human behavior. Through timely, insightful presentations and interactive sessions, this event will serve to unite a global community toward solutions to benefit all of humanity.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hai.stanford.edu/events/covid-19-and-ai-virtual-conference/overview?utm_source=twitter&amp;utm_medium=social&amp;utm_content=UComm_twitter_StanfordHAI_202003221000_sf119533070&amp;utm_campaign=&amp;sf119533070=1"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data, Privacy, and the Individual&lt;/h2&gt;
&lt;p&gt;"The aim of the research project Data, Privacy, and the Individual is to contribute to a better understanding of the ethics of privacy and of differential privacy. The outcomes of the project are seven research papers on privacy, a survey, and this final report, which summarises each research paper, and goes on to offer a set of reflections and recommendations to implement best practices regarding privacy."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://philpapers.org/archive/VLIPM.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithmic systems: the consent is in the detail?&lt;/h2&gt;
&lt;p&gt;Applications of algorithmically informed decisions are becoming entrenched in society, with data processing being their main process and ingredient. While these applications are progressively gaining momentum, established data protection and privacy rules have struggled to incorporate the particularities of data-intensive information societies. It is a truism to point out the resulting misalignment between algorithmic processing of personal data and the data protection regulatory frameworks that strive for meaningful control over personal data. However, the challenges to the (traditional) role and concept of consent are particularly manifest. This article examines the transformation of consent models in order to assess how the concept and the applied models of consent can be reconciled in order to correspond not only to the current regulatory landscapes but also to the exponential growth of algorithmic processing technologies.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://policyreview.info/articles/analysis/algorithmic-systems-consent-detail"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Israeli Emergency Regulations for Location Tracking of Coronavirus Carriers&lt;/h2&gt;
&lt;p&gt;Surveillance and big data are an area of tech ethics that doesn't always overlap with AI ethics, but this is a reminder of the huge amount of data that we create, and that is retained, which AI might use for unexpected purposes down the road. &lt;/p&gt;
&lt;p&gt;"The ISA Emergency Coronavirus Regulations (“Authorizing the Israel Security Agency to Assist in the National Efforts to Reduce the Spread of the Novel Coronavirus”) authorize the ISA to receive, collect and process “technological data” for the purpose of assisting the Ministry of Health in conducting epidemiological investigations to reduce and prevent the spread of the novel coronavirus. The data should be used to identify the location data and movement routes of coronavirus carriers in the 14 days preceding their diagnosis as carriers, along with the identity of individuals who came into close contact with them."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.lawfareblog.com/israeli-emergency-regulations-location-tracking-coronavirus-carriers"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Soap Box as a Black Box: Regulating transparency in social media recommender systems&lt;/h2&gt;
&lt;p&gt;"Social media recommender systems play a central role in determining what content is seen online, and what remains hidden. As a point of control for media governance, they are subject to intense controversy and, increasingly, regulation by European policymakers. A recurring theme in such efforts is transparency, but this is an ambiguous concept that can be implemented in various ways depending on the types of accountability one envisages. This paper maps and critiques the various efforts at regulating social media recommendation transparency in Europe, and the types of accountability they pursue."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://osf.io/preprints/lawarxiv/uhxcv"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How can we build child-friendly AI that empowers children in Africa?&lt;/h2&gt;
&lt;p&gt;We missed this last week!&lt;/p&gt;
&lt;p&gt;"These reflections were gathered at an AI and Children African regional consultation that took place in Cape Town, South Africa in February 2020. It is the third in a series of regional workshops designed to develop global AI policy guidance that protects child rights."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.unicef.org/globalinsight/stories/how-can-we-build-child-friendly-ai-empowers-children-africa"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;We still need responsible tech for care in a global pandemic&lt;/h2&gt;
&lt;p&gt;"We spent the last eighteen months researching and redesigning care systems with people on the front line of care – people who give care, who work in care, who receive care, who organise care and who fight for better care, and those who do several or even all of the above. &lt;/p&gt;
&lt;p&gt;We came up with three key principles that were necessary prerequisites for technology developed for care to be responsible"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.doteveryone.org.uk/2020/03/we-still-need-responsible-tech-for-care-in-a-global-pandemic/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ada Lovelace Institute JUST AI network would like to hear from you.&lt;/h2&gt;
&lt;p&gt;Our humanities-led JUST AI network would like to hear from you...&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/AdaLovelaceInst/status/1242146621242122246"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: March 10 - March 17</title><link href="/ai-ethics-news-roundup-mar10-mar17-2020.html" rel="alternate"></link><published>2020-03-10T09:20:00+01:00</published><updated>2020-03-10T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-10:/ai-ethics-news-roundup-mar10-mar17-2020.html</id><summary type="html">&lt;p&gt;@datainnovation on AI and #covid19, @Brave challenges Google via the #GDPR, @rcalo &amp;amp; @daniellecitron on automated government, @jake_bittle on AI and deception, @MichaelEOHanlon on AI and National Security, @dmonett comments on the JAGI #AI issue, @datarobot on AI bias, @RadicalAIPod on the dangers of AI and #covid19&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;How Artificial Intelligence is Aiding the Fight Against Coronavirus&lt;/h2&gt;
&lt;p&gt;"The coronavirus (COVID-19) has dominated the global news and resulted in travel restrictions, school closures, market panic, and many more disruptions as health organizations work to contain the spread of the virus. With over 125,000 confirmed cases and over 4,500 deaths worldwide and over 1,600 cases and 41 deaths in the U.S., and the World Health Organization (WHO) officially labeling the virus as a pandemic, the global health community is relying on new tools and technologies to stay ahead. Artificial intelligence (AI) has proven especially valuable every step of the way, from detecting the first coronavirus outbreak to measuring the disease’s economic impact."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/03/how-artificial-intelligence-is-aiding-the-fight-against-coronavirus/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bias In Recruitment Software To Be ‘Illegal’ in New York, Vendors Will Need Bias Audit&lt;/h2&gt;
&lt;p&gt;"A new law by New York’s City Council will in effect make algorithmic bias in recruitment software illegal. It demands that vendors of such candidate filtering tools submit to a bias audit, with Civil penalties for those who fail to conduct an ‘impartial evaluation‘. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.artificiallawyer.com/2020/03/12/bias-in-recruitment-software-to-be-illegal-in-new-york-vendors-will-need-bias-audit/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The emergence of the professional AI risk manager&lt;/h2&gt;
&lt;p&gt;"We are now beset by data breaches and data privacy scandals, and regulators around the world have responded with data regulations. GDPR is the current role model, but I expect a global group of regulators to expand the rules to cover AI more broadly and set the standard on how to manage it. The UK ICO just released a draft but detailed guide on auditing AI. The EU is developing one as well. Interestingly, their approach is very similar to that of the Basel standards: specific AI risks should be explicitly managed. This will lead to the emergence of professional AI risk managers."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://venturebeat.com/2020/03/14/the-emergence-of-the-professional-ai-risk-manager/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI is an Ideology, Not a Technology&lt;/h2&gt;
&lt;p&gt;"A clear alternative to “AI” is to focus on the people present in the system. If a program is able to distinguish cats from dogs, don’t talk about how a machine is learning to see. Instead talk about how people contributed examples in order to define the visual qualities distinguishing “cats” from “dogs” in a rigorous way for the first time. There's always a second way to conceive of any situation in which AI is purported. This matters, because the AI way of thinking can distract from the responsibility of humans."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wired.com/story/opinion-ai-is-an-ideology-not-a-technology/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Formal GDPR complaint against Google’s internal data free-for-all&lt;/h2&gt;
&lt;p&gt;Brave has filed a formal GDPR complaint against Google for infringing the GDPR “purpose limitation” principle. Enforcement would be tantamount to a functional separation of Google’s business. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://brave.com/google-internal-data-free-for-all/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Responsible AI Moves Into Focus at Microsoft's Data Science and Law Forum&lt;/h2&gt;
&lt;p&gt;"Last week, Microsoft gathered experts from academia, civil society, policy making and more to discuss one of the most important topics in tech at the moment: responsible AI (RAI)."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cmswire.com/information-management/responsible-ai-moves-into-focus-at-microsofts-data-science-and-law-forum/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;An AI Ethics Checklist for COVID-19 Healthcare Technology&lt;/h2&gt;
&lt;p&gt;"On March 13th Forbes published an article entitled &lt;a href="https://www.forbes.com/sites/bernardmarr/2020/03/13/coronavirus-how-artificial-intelligence-data-science-and-technology-is-used-to-fight-the-pandemic/#43242df45f5f"&gt;Coronavirus: How Artificial Intelligence, Data Science And Technology Is Used To Fight The Pandemic&lt;/a&gt; written by Bernard Marr. In this article Marr lists 10 ways in which AI, ML, and DS are being used to address the pandemic. I would like to use Marr’s list as a framework to propose questions (3 questions per item) that folks designing and using this tech to engage with the virus might ask themselves to better ensure ethical creation and application. You can think of it as an AI ethics checklist"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.radicalai.org/blog/an-ai-ethics-checklist-for-covid-19-healthcare-technology"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Evolution of Artificial Intelligence and Future of National Security&lt;/h2&gt;
&lt;p&gt;"...Is AI likely to be all it’s cracked up to be? We think the answer is complex and that a modest dose of cold water should be thrown on the subject. In fact, many of the AI systems being envisioned today will take decades to develop. Moreover, AI is often being confused with things it is not. Precision about the concept will be essential if we are to have intelligent discussions about how to research, develop, and regulate AI in the years ahead."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://nationalinterest.org/feature/evolution-artificial-intelligence-and-future-national-security-133032"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Lie detectors have always been suspect. AI has made the problem worse.&lt;/h2&gt;
&lt;p&gt;An in-depth investigation into artificial-intelligence-based attempts to recognize deception.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615336/ai-lie-detectors-polygraph-silent-talker-iborderctrl-converus-neuroid/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A great thread from Dagmar Monett on the JAGI AI Issue&lt;/h2&gt;
&lt;p&gt;Dagmar Monett runs through abstracts and adds some commentary to the JAGI special issue on the concept of Artificial Intelligence. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/dmonett/status/1238791188565630977"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Automated Administrative State: A Crisis of Legitimacy&lt;/h2&gt;
&lt;p&gt;We missed this last week... &lt;/p&gt;
&lt;p&gt;"In recent decades, state and federal agencies have embraced a novel mode of operation: automation. Agencies rely more and more on software and algorithms in carrying out their delegated responsibilities. The automated administrative state, however, is demonstrably riddled with concerns. Legal challenges regarding the denial of benefits and rights—from travel to disability—have revealed a pernicious pattern of bizarre and unintelligible outcomes."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3553590"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial intelligence isn’t as smart as it thinks&lt;/h2&gt;
&lt;p&gt;"...the technology is limited to narrow tasks that require human oversight and mountains of data which is often skewed in ways that lead to unexpected, or even biased, results. The holy grail — so-called general artificial intelligence that can flit between various jobs mimicking human behavior — is still more a myth than a reality, with more than half of almost 400 AI experts recently surveyed saying it will be at least 2060, if not later, for such technology to become feasible."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://www.politico.eu/article/artificial-intelligence-drawbacks-isnt-as-smart-as-it-thinks"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Exploring Gender Imbalance in AI: Numbers, Trends, and Discussions&lt;/h2&gt;
&lt;p&gt;To highlight the contributions of women in the AI industry, Synced introduces the Women in AI special project this month and invites female researchers from the field to share their recent research works and the stories behind the idea. Join our conversation by clicking here.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/syncedreview/exploring-gender-imbalance-in-ai-numbers-trends-and-discussions-33096879bd54?_branch_match_id=768431107131880217"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Do You Define Unfair Bias in AI?&lt;/h2&gt;
&lt;p&gt;"In this blog post, rather than defining a unique solution, I will list the four key questions that you need to answer in order to derive a definition of unfair bias that matches your particular needs."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://blog.datarobot.com/how-do-you-define-unfair-bias-in-ai"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Mar 3 - Mar 10</title><link href="/ai-ethics-news-roundup-mar3-mar10-2020.html" rel="alternate"></link><published>2020-03-10T09:20:00+01:00</published><updated>2020-03-10T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-10:/ai-ethics-news-roundup-mar3-mar10-2020.html</id><summary type="html">&lt;p&gt;@Floridi &amp;amp; @agstrait ethical analysis, @carolinejmolloy: NHS and tech, @misslivirose: consent and virtuality, @jacobsonjenna: cybervetting, @mckelveyf: the public good, @timleberecht: AI, nature, and ethics, @NathalieSmuha: human rights, @SandraWachter5: fairness, @Sarah_Brayne: crime prediction + more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Ethical Foresight Analysis: What it is and Why it is Needed?&lt;/h2&gt;
&lt;p&gt;A new piece from Luciano Floridi and Andrew Strait:&lt;/p&gt;
&lt;p&gt;"An increasing number of technology firms are implementing processes to identify and evaluate the ethical risks of their systems and products. A key part of these review processes is to foresee potential impacts of these technologies on different groups of users. In this article, we use the expression Ethical Foresight Analysis (EFA) to refer to a variety of analytical strategies for anticipating or predicting the ethical issues that new technological artefacts, services, and applications may raise. This article examines several existing EFA methodologies currently in use. It identifies the purposes of ethical foresight, the kinds of methods that current methodologies employ, and the strengths and weaknesses of each of these current approaches. The conclusion is that a new kind of foresight analysis on the ethics of emerging technologies is both feasible and urgently needed."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007%2Fs11023-020-09521-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Robots Are Coming: Ethics, Politics, and Society in the Age of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;The late Kenneth Taylor's previously unpublished essay on the impact of AI and Robotics just came out in the Boston Review:&lt;/p&gt;
&lt;p&gt;"If we cannot stop or reverse the robot invasion of the built human world, we must turn and face them. We must confront hard questions about what will and should become of both them and us as we welcome ever more of them into our midst. Should we seek to regulate their development and deployment? Should we accept the inevitability that we will lose much work to them? If so, perhaps we should rethink the very basis of our economy. Nor is it merely questions of money that we must face. There are also questions of meaning. What exactly will we do with ourselves if there is no longer any economic demand for human cognitive labor? How shall we find meaning and purpose in a world without work?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://bostonreview.net/science-nature-philosophy-religion/kenneth-taylor-robots-are-coming"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithmic Legal Metrics&lt;/h2&gt;
&lt;p&gt;This is a fascinating paper that in places runs very much against the grain of current thinking. &lt;/p&gt;
&lt;p&gt;"In this paper I link the sociological and legal analysis of AI, highlighting the reflexive social processes that are engaged by algorithmic metrics. This paper examines these overlooked social effects of predictive legal algorithms, and contributes to the literature a vital fundamental but missing critique of such analytics. Specifically, this paper shows how the problematic social effects of algorithmic legal metrics extend far beyond the concerns about accuracy that have thus far dominated critiques of such metrics. Second, it demonstrates that corrective governance mechanisms such as enhanced due process or transparency will be inadequate to remedy such corrosive effects, and that some such remedies, such as transparency, may actually exacerbate the worst effects of algorithmic governmentality. Third, the paper shows that the application of algorithmic metrics to legal decisions aggravates the latent tensions between equity and autonomy in liberal institutions, undermining democratic values in a manner and on a scale not previously experienced by human societies. Illuminating these effects casts new light on the inherent social costs of AI metrics, particularly the perverse effects of deploying algorithms in legal systems. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3537337"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How ‘Smart Tech’ Masks an Emerging Era of Corporate Control&lt;/h2&gt;
&lt;p&gt;"Harvesting data requires the technical ability and social authority to probe things, people, and places. Control systems are fueled by data, which allows for more granular, more effective, and more instantaneous command over those same things, people, and places. Smart tech is the offspring of both imperatives."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://onezero.medium.com/how-smart-tech-masks-an-emerging-era-of-corporate-control-779c96b05f85"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Cybervetting job applicants on social media: the new normal?&lt;/h2&gt;
&lt;p&gt;"Our research, using an online survey with 482 participants, investigates young people’s concerns with their publicly available social media data being used in the context of job hiring. Grounded in stakeholder theory, we analyze the relationship between young people’s concerns with social media screening and their gender, job seeking status, privacy concerns, and social media use. We find that young people are generally not comfortable with social media screening. A key finding of this research is that concern for privacy for public information on social media cannot be fully explained by some “traditional” variables in privacy research. The research extends stakeholder theory to identify how social media data ethics should be inextricably linked to organizational practices. The findings have theoretical implications for a rich conceptualization of stakeholders in an age of social media and practical implications for organizations engaging in cybervetting."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007%2Fs10676-020-09526-2"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Before Adopting New Technologies, We Must Define the Common Good&lt;/h2&gt;
&lt;p&gt;"Canada (and the rest of the world) faces an opportunity: to avoid (or halt) the harmful effects of digital control and techno-social engineering, regulators must garner more public interest in the ways that technologies shape public life. Wrestling with the effects of technology, its wide reach and impact provides opportunity to foster more inclusive and engaged consultations about the environmental, health, social, cultural and democratic implications of powerful technologies such as AI and digital platforms. Here, there is excitement as much as risk in searching for the right formats to find, define and debate the common good."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cigionline.org/articles/adopting-new-technologies-we-must-define-common-good"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Artificial Intelligence Can Turn Us Into Bigots&lt;/h2&gt;
&lt;p&gt;An intriguing thought: AI can devleop it's own pseudoscience. &lt;/p&gt;
&lt;p&gt;"While the computing involved in AI is often incomprehensibly vast and fast, it always rests upon foundations of simplifying, quantifying, and generalising. And that’s okay: it’s how engineers traditionally solve problems, by breaking them down into simple parts, simplifying where necessary, and building in safety margins to account for inaccuracies."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://10daily.com.au/views/a200304obwdr/how-artificial-intelligence-can-turn-us-into-bigots-20200306"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The case for an AI that puts nature and ethics first, not humans&lt;/h2&gt;
&lt;p&gt;"...concern is growing that we are surrendering to a paradigm of “forced reductionism” (to borrow a term from former MIT Media Lab director Joi Ito), shoehorning ourselves into a purely mechanistic, utilitarian model of technology. As AI becomes more and more powerful and invasive, it may inevitably change our world to align with these very design principles. The consequence might be a world full of “monochrome societies,” as Infineon CEO Dr. Reinhard Pless puts it."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thenextweb.com/neural/2020/03/07/the-case-for-an-ai-that-puts-nature-and-ethics-first-not-humans-syndication/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;This Company Is Using Racially-Biased Algorithms to Select Jurors&lt;/h2&gt;
&lt;p&gt;"Momus Analytics' predictive scoring system is using race to grade potential jurors on vague qualities like "leadership" and "personal responsibility.""&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.vice.com/en_us/article/epgmbw/this-company-is-using-racially-biased-algorithms-to-select-jurors"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Beyond a Human Rights-based approach to AI Governance: Promise, Pitfalls, Plea&lt;/h2&gt;
&lt;p&gt;This paper discusses the establishment of a governance framework to secure the development and deployment of “good AI”, and describes the quest for a morally objective compass to steer it. Asserting that human rights can provide such compass, this paper first examines what a human rights-based approach to AI governance entails, and sets out the promise it propagates. Subsequently, it examines the pitfalls associated with human rights, particularly focusing on the criticism that these rights may be too Western, too individualistic, too narrow in scope and too abstract to form the basis of sound AI governance. After rebutting these reproaches, a plea is made to move beyond the calls for a human rights-based approach, and start taking the necessary steps to attain its realisation. It is argued that, without elucidating the applicability and enforceability of human rights in the context of AI; adopting legal rules that concretise those rights where appropriate; enhancing existing enforcement mechanisms; and securing an underlying societal infrastructure that enables human rights in the first place, any human rights-based governance framework for AI risks falling short of its purpose. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3543112"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Will Big Tech save the NHS – or eat it alive?&lt;/h2&gt;
&lt;p&gt;"Technophile politicians and tech companies are making big promises. But the broader impact of the digital transformation of our health and public services has been too little examined." &lt;/p&gt;
&lt;p&gt;"Digitalisation is a tool to ‘tailor’ services, we’re told. But in a context of austerity, the biggest concern is that ‘tailoring’ becomes a fancy word for ‘cutting’ – that it provides the cover for cuts, privatisation, co-payments, and the loss of communal space, public accountability and social connection."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.opendemocracy.net/en/ournhs/will-big-tech-save-the-nhs-or-eat-it-alive/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI&lt;/h2&gt;
&lt;p&gt;"This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automat-ed fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as “contextual equality.”"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;It’s Time To Develop A Consent Framework For Virtual Beings&lt;/h2&gt;
&lt;p&gt;The rise of virtual beings leads to new questions regarding the rights of digital representations.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://vrscout.com/news/consent-framework-for-virtual-beings/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI In Policing: Better Than ‘A Knife Through The Chest?’&lt;/h2&gt;
&lt;p&gt;"The U.K. police are rolling out AI crime fighting with little regard to the societal risks. In an impassioned speech about police use of AI, the commissioner of the Metropolitan police, Cressida Dick, criticised privacy advocates. She intoned that concern over the use of Live Face Recognition (LFR) on law abiding citizens, “feels much, much smaller than my and the public’s vital expectation to be kept safe from a knife through the chest.” "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/noelsharkey/2020/03/06/ai-in-policing-better-than-a-knife-through-the-chest/#e1361f3548e4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Technologies of Crime Prediction: The Reception of Algorithms in Policing and Criminal Courts&lt;/h2&gt;
&lt;p&gt;"The number of predictive technologies used in the U.S. criminal justice system is on the rise. Yet there is little research to date on the reception of algorithms in criminal justice institutions. We draw on ethnographic fieldwork conducted within a large urban police department and a midsized criminal court to assess the impact of predictive technologies at different stages of the criminal justice process. '&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://academic.oup.com/socpro/advance-article-abstract/doi/10.1093/socpro/spaa004/5782114"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;JAGI Special Issue “On Defining Artificial Intelligence”&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href="https://content.sciendo.com/view/journals/jagi/11/2/jagi.11.issue-2.xml"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Deployment of AI Systems: “Assistive” Technology</title><link href="/03-04-2020-assistive-tech.html" rel="alternate"></link><published>2020-03-04T07:20:00+01:00</published><updated>2020-03-04T07:20:00+01:00</updated><author><name>Sam Meeson-Frizelle</name></author><id>tag:None,2020-03-04:/03-04-2020-assistive-tech.html</id><summary type="html">&lt;p&gt;AI &amp;amp; Disability: Challenges of developing "Assistive" technology&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Conversations about AI and Disability tend to focus on the ways in which AI is used in the deployment of technological tools that help disabled people to perform tasks they otherwise would not be able to carry out. Recently, this idea of help has seeped into common usage with the term “assistive” technology - a notion that is burdened with assumptions about disability which shed light on deep rooted social injustices that I will later explore.&lt;/p&gt;
&lt;p&gt;Deployment of so-called assistive tech is widespread and there are many cases where they allow disabled people to participate more fully in society. The National Theatre, for example, offers a pair of ‘Smart Caption Glasses’ for deaf people. These glasses are essentially a speech recognition tool that uses a natural language processing (NLP) AI model and provides live captioning for what is in the visual field so that deaf people can enjoy a performance. &lt;/p&gt;
&lt;p&gt;This is just one example but such language processing models are used for tools that are deployed more widely and at scale to help persons - whose disability affects their audio/visual skills - perform necessary everyday tasks.&lt;/p&gt;
&lt;h2&gt;Ways in which Assistive Technology Benefits Society&lt;/h2&gt;
&lt;p&gt;It’s clear that assistive technologies have a profound impact on the daily life of users. On Scope UK’s website, a disability equality charity, you see users of assistive technology talk about the instrumental role certain tools play in their lives. Raisa, whose disability means she cannot physically type with any proficiency, talks of her reliance on Apple’s voice recognition software to perform her “most important job” of “dictating and replying to emails”. For her, such assistive technology is paramount and she believes that it can “help you live the life you chose to live.”&lt;/p&gt;
&lt;p&gt;Assistive tech is not limited to physical disabilities. “&lt;a href="http://noisolation.com"&gt;No Isolation&lt;/a&gt;”, a Norwegian health-tech start-up, deploys what we might call an assistive tool to help address social isolation and loneliness. According to No Isolation’s research, two of the most vulnerable groups are young children with long-term illness and those who are over 80 years of age. No Isolation addresses this vulnerability for young children, by deploying it’s AV1 robot to assist those who cannot physically participate at school. The robot attends classes by proxy, employing NLP and machine vision AI systems that allow the child to engage with the teacher and with the other children.  &lt;/p&gt;
&lt;h2&gt;Challenging The Notion of “Assistive” Tech - The Language of Assistance&lt;/h2&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;Whilst such technologies appear to be taking a positive step to bring about equality and opportunity for disabled people, we might want to challenge the common usage of the term “assistive”. &lt;/p&gt;

&lt;p&gt;As Richard Ladner from the University of Washington points out, the term “assistive” technology is in some respects redundant. It’s hard to think of an example in which technology is not assistive in some way. Technology, generally, makes certain tasks possible or easier to do. So what is the distinction between  “assistive” tech and plain old tech?&lt;/p&gt;
&lt;p&gt;This is not immediately clear. When we think about people who have “Correctable Vision” - a condition not technically considered a disability - the glasses or contact lenses that help them to improve their vision are not commonly described as “assistive technology”. Yet surely glasses or lenses are in fact assistive. This begs the question which Ladner asks: “Why is it that people with disabilities have assistive technology while the rest of us just have technology?”&lt;/p&gt;
&lt;p&gt;Given the redundancy of word assistive, the term assistive technology seems to indicate that disabled people require lots of extra help - evoking a sense of dependence and a lack of capability.  As Ladner points out, it seems inherently paternalistic to say that disabled people receive assistance and it fundamentally challenges their identity, freedom and agency humans - ultimately it diminishes the autonomy individuals have over their actions.&lt;/p&gt;
&lt;p&gt;Moreover, the idea of assistive technology highlights a certain “quick fix” attitude to technology which Mara Mills, Associate Professor of Media, Culture and Communication at NYU, claims ignores important advances of “education, community support and social change”.&lt;/p&gt;
&lt;p&gt;These concerns raise some weighty concerns for disability and AI. Notably, the idea that the deployment of assistive tech carries a serious threat of power asymmetry between those who design and deploy AI systems and those for whom it is made.&lt;/p&gt;
&lt;p&gt;This opens the door to further scrutiny around the design of AI systems that fuel these tools - time to get the magnifying glass out.&lt;/p&gt;
&lt;h2&gt;Design and Disability&lt;/h2&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;When we think about the design stage of AI systems, specifically those that apply machine learning methods, it’s important to consider the nature of the dataset that an AI model is being trained on. &lt;/p&gt;

&lt;p&gt;In general being excluded from the training data in an AI model can cause problems. Concretely, if an AI system is, for example, trained on a data set that has no images of bald people, then bald people will be missing from the AI model. As a result, the AI system won’t recognise bald people - which would, let’s say, make it almost impossible for a machine vision AI tool to hunt down a Jason Statham or Bruce Willis. &lt;/p&gt;
&lt;p&gt;Moreover, the datasets that many AI systems use unfairly represent a number of groups - most commonly on the grounds of race, gender or disability. Unfair representation is a reflection of how people in these groups have been subject to historic marginalisation and discrimination.&lt;/p&gt;
&lt;p&gt;These historic patterns are imprinted in the AI model datasets, which in turn are used to train an AI system - resulting in a so-called “algorithmic bias”. Ultimately, this bias leads to yet more unfair outcomes for people in these groups - pouring ever more fuel on the discriminatory fire. &lt;/p&gt;
&lt;p&gt;AI Now, a NYU research body addressing the societal impacts of technology, calls this vicious circle  “discriminatory logics” - that is to say: “those who have borne discrimination in the past are most at risk of harm from biased and exlusionary AI in the present”. This ongoing pattern reveals an inherent toxicity in AI systems for disabled people that has been at the forefront of wider AI bias concerns.&lt;/p&gt;
&lt;p&gt;Moreover, it is particularly worrisome that the AI systems which use these encoded “discriminatory logics” carry a certain unchallengeable authority. There are countless cases of disabled individuals being discriminated against by AI systems in high-stakes decision making. &lt;/p&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;For example, as Kathryn Zsykowski observes with Amazon Mechanical Turk, a crowdsourcing marketplace for outsourcing virtual tasks, certain disabled clickworkers are unable to pass the CAPTCHA (a common reverse Turing Test that helps to prove an individual's humanity) or cannot complete work quickly enough so they are consequently rejected by the platform.&lt;/p&gt;

&lt;p&gt;We need to be really careful about how we assign authority to technology. In the case of decision-making AI systems, we might think their authority often hinges tenuously on the fact that they are the product of powerful companies who employ clever people to create technology that very few people actually understand. &lt;/p&gt;
&lt;p&gt;Immediately, this raises concerns of explainability and accountability around AI: we need to direct our attention to the companies that produce these tools and a) demand an explanation of the decisions that the system makes but, more importantly, b) hold them accountable when the decisions are discriminatory. &lt;/p&gt;
&lt;p&gt;The idea that misplaced authority leads to discrimination against disabled persons reveals a deeper social injustice in the AI realm. That is: there is an imbalance in power between those who design and deploy AI systems and those who are “classified, ranked and assessed by these systems”. Ultimately, as I will suggest, to address this imbalance we need to look at concerns of AI bias and disability in tandem with these deep rooted social injustices. &lt;/p&gt;
&lt;p&gt;In the wider tech ethics arena most of the discussion and many of the headlines are focussed on the axes of gender and racial bias, with comparably little literature discussing the treatment of disabled persons in the face of algorithmic bias. &lt;p style="font-size: 18px;font-weight: bold"&gt;As AI Now points out in its report: “disability has been largely omitted from the AI bias conversation”. &lt;/p&gt;&lt;/p&gt;
&lt;p&gt;By and large, this statement is justified. Perhaps more importantly though, even when disabled persons seem to be part of the conversation, they are not properly included and the problems they face are not addressed in the right way. Most commonly, disabled persons are not included in the right way because of exclusion and “unfair representation”.&lt;/p&gt;
&lt;h2&gt;Issues of Exclusion and Unfair Representation&lt;/h2&gt;
&lt;p&gt;One example in recent years where exclusion from training data had a fatal impact involved an autonomous Uber vehicle that hit and killed Elaine Gerzberg in 2018 as she pushed her bike across the road. &lt;/p&gt;
&lt;p&gt;In this case, the system’s training data clearly did not include enough images of a person pushing a bike, which lead to confusion for Uber's pedestrian recognition system. Not having enough representations in this case echoes the Jason Statham example above in the way that it is seemingly “unfair”. &lt;/p&gt;
&lt;p&gt;If able bodied pedestrians are at risk from misrecognition due to exclusion and unfair representation in a dataset, it’s vital that we ask ourselves how we can avoid this happening for disabled people in wheelchairs or mobility scooters?&lt;/p&gt;
&lt;p&gt;One solution might be to concentrate our design efforts on fairly representing disabled persons in the training data. We might suggest that a fair representation of disable persons equates to something like comprehensive representation  - which fully and correctly classifies all kinds of disability. &lt;/p&gt;
&lt;p&gt;Clearly this is easier said than done.&lt;/p&gt;
&lt;p&gt;Disability is such a fluid and vast concept that encompasses an almost immeasurable range of physical and mental health conditions, and that can come and go throughout time. This means there are so many outliers and often no two disabilities are the same. As Dr. Stephen Shore famously said: “if you’ve met one person with autism, you’ve met one person with autism”. &lt;/p&gt;
&lt;p&gt;Such fluidity is in direct conflict with the rigidity of AI systems. What this means is it is hard to concretely account for the multitude and variation of disabilities, which makes mapping disabilities onto AI model classifications seemingly impossible.&lt;/p&gt;
&lt;p&gt;Moreover, as AI Now points out, even if we could account for such fluidity, to achieve fair representation “may require increased surveillance and invasion of privacy in the process” - opening another can of ethically questionable worms. &lt;/p&gt;
&lt;h2&gt;Can We Move Towards Inclusion and Fair Representation?&lt;/h2&gt;
&lt;p&gt;One option for solving the AI bias issue is to take a technical approach: classifying people into a single variable, for example race or gender, and then testing the system by applying a number of methods to see if it works across a variety of people.&lt;/p&gt;
&lt;p&gt;This common technical approach is limited and simply won’t do for disability. Firstly, for the reasons of fluidity mentioned above. Secondly, and more importantly, if we take a social model of disability in which we understand it as a “product of disabling environments and thus an identity that can only be understood in relation to a given social and material context” we see that it’s very difficult to find technical fixes for social problems. &lt;/p&gt;
&lt;p&gt;Another way  to mitigate the issues of systemic algorithmic misrepresentation is to collect more data representing disabled people. However, simply augmenting the data collection process raises issues again of how this data is collected and also how it is classified. &lt;/p&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;For example, there are a number of grassroot collection efforts within the disability community to gather data in the hope of better understanding and addressing their health. But in such cases where healthcare isn’t guaranteed, it is difficult, as AI Now reports, to “ensure that such data won’t be reused in ways that could cause harm” - even if that was not the original intention. &lt;/p&gt;

&lt;p&gt;Moreover, the workshop, which the AI Now report emerged from, revealed that much of the effort to increase the data representation of disabled persons is performed by “clickworkers” who “label data as being from people who are disabled based on what is effectively a hunch”. Paradoxically here, the effort to include more disabled people is stunted by the classification of categories that “effectively exclude many of those they are meant to represent”. &lt;/p&gt;
&lt;h2&gt;Going Back to the Drawing Board&lt;/h2&gt;
&lt;p&gt;Although the development of technological tools that help disabled people to participate in society is seemingly beneficial and perhaps life changing, the impact on the wider disability movement is momentary.&lt;/p&gt;
&lt;p&gt;As I looked, in this article, at AI and disability through the critical lens of philosophy - I saw the issues of “assistance” and bias that reflected deeply rooted social concerns.&lt;/p&gt;
&lt;p&gt;To properly address the issue of bias and disability, specifically addressing the concerns of algorithmic marginalisation and discrimination, we need to focus on disability in it’s social and environmental context. This is no small task and probably beyond the scope of this article. That said, there is slowly more and more work being done to take steps in the right direction.&lt;/p&gt;
&lt;p&gt;The AI Now paper cited in this article is a great resource for an overview of the challenges that face the disability community with respect to AI. It is also particularly constructive in the way it draws our attention to the vital need to include disabled persons in the design stage of tech development. &lt;p style="font-size: 18px;font-weight: bold"&gt;The resounding message from AI Now is that disabled persons should be designing “with, not for” the mantra of the disability movement: “Nothing About Us Without Us”.&lt;/p&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 25 - Mar 3</title><link href="/ai-ethics-news-roundup-feb25-mar3-2020.html" rel="alternate"></link><published>2020-03-03T09:20:00+01:00</published><updated>2020-03-03T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-03:/ai-ethics-news-roundup-feb25-mar3-2020.html</id><summary type="html">&lt;p&gt;@TC_IntLaw discusses a GDPR decision on #facialrecognition at @AI_Regulation, @oshdzieza on AI in the workplace, @math_rachel on selling our own data, @willknight on fooling AI, and much more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;First Ever Decision of a French Court Applying GDPR to Facial Recognition&lt;/h2&gt;
&lt;p&gt;A French court canceled a decision by the South-Est Region of France to undertake a series of tests using facial recognition at the entrance of two High schools considering that this would be illegal. This is the first decision ever by a French Court applying the General Data Protection Regulation (GDPR) on Facial Recognition Technologies (FRTs).&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ai-regulation.com/first-decision-ever-of-a-french-court-applying-gdpr-to-facial-recognition/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A Framework for Responsible Limits on Facial Recognition&lt;/h2&gt;
&lt;p&gt;"The World Economic Forum’s Framework for the Responsible use of facial recognition technology seeks to address the need for a set of concrete guidelines to ensure the trustworthy and safe use of this technology. This framework enables Governments to protect citizens from various harms potentially caused by facial recognition technology while supporting beneficial applications. It also enables industry actors to demonstrate that they have implemented robust risk mitigation processes through an independent audit of their systems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.weforum.org/whitepapers/a-framework-for-responsible-limits-on-facial-recognition-use-case-flow-management"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Crowdsourcing Moral Machines&lt;/h2&gt;
&lt;p&gt;"... We believe bringing about accountable intelligent machines that embody human ethics requires an interdisciplinary approach. First, engineers build and refine intelligent machines, and tell us how they are capable of operating. Second, scholars from the humanities—philosophers, lawyers, social theorists—propose how machines ought to behave, and identify hidden moral hazards in the system. Third, behavioral scientists, armed with tools for public engagement and data collection like the MM, provide a quantitative picture of the public's trust in intelligent machines, and of their expectations of how they should behave.b Finally, regulators monitor and quantify the performance of machines in the real world, making this data available to engineers and citizens, while using their enforcement tools to adjust the incentives of engineers and corporations building the machines."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://cacm.acm.org/magazines/2020/3/243030-crowdsourcing-moral-machines/fulltext"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bad news for explainability?&lt;/h2&gt;
&lt;p&gt;"One of the strangest mysteries in AI is that you can average two models and get a result superior to either model alone."&lt;/p&gt;
&lt;p&gt;An interesting twitter discussion on a surprising way to improve ML, the exact characterization of which is not quite certain, but perhaps by mitigating overfitting. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/theshawwn/status/1234310915895353344"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How hard will the robots make us work?&lt;/h2&gt;
&lt;p&gt;"In warehouses, call centers, and other sectors, intelligent machines are managing humans, and they’re making work more stressful, grueling, and dangerous"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theverge.com/2020/2/27/21155254/automation-robots-unemployment-jobs-vs-human-google-amazon"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;As humanity’s relationship with AI grows, experts call for protective framework&lt;/h2&gt;
&lt;p&gt;"Imperial College London researchers have suggested a new regulatory framework with which governments can minimise unintended consequences of our relationship with technology. The comment piece is published in Nature Machine Intelligence."&lt;/p&gt;
&lt;p&gt;"The proposed framework, known as the Human Impact Assessment for Technology (HIAT), would be designed to predict and evaluate the impact that new digital technologies have on society and individual wellbeing. This, they argue, should focus on ethical considerations like individual privacy, wellbeing and autonomy."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.imperial.ac.uk/news/195615/as-humanitys-relationship-with-ai-grows/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Can you sell your own data and therefore consent to how it will be used downstream?&lt;/h2&gt;
&lt;p&gt;This is an important question in light of regulations like the CCPA and calls for users to be compensated for the data extracted from them. A nice thread from Rachel Thomas.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/math_rachel/status/1233109858079166470"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;In Coronavirus Fight, China Gives Citizens a Color Code, With Red Flags&lt;/h2&gt;
&lt;p&gt;"As China encourages people to return to work despite the coronavirus outbreak, it has begun a bold mass experiment in using data to regulate citizens’ lives — by requiring them to use software on their smartphones that dictates whether they should be quarantined or allowed into subways, malls and other public spaces.&lt;/p&gt;
&lt;p&gt;But a New York Times analysis of the software’s code found that the system does more than decide in real time whether someone poses a contagion risk. It also appears to share information with the police, setting a template for new forms of automated social control that could persist long after the epidemic subsides."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/03/01/business/china-coronavirus-surveillance.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Adversarial Attacks Could Destabilize Military AI Systems&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence and robotic technologies with semi-autonomous learning, reasoning, and decision-making capabilities are increasingly being incorporated into defense, military, and security systems. Unsurprisingly, there is increasing concern about the stability and safety of these systems. In a different sector, runaway interactions between autonomous trading systems in financial markets have produced a series of stock market “flash crashes,” and as a result, those markets now have rules to prevent such interactions from having a significant impact"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/adversarial-attacks-and-ai-systems#disqus_thread"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;More on Adversarial AI and The risks of algorithmic (il)literacy on healthcare platforms&lt;/h2&gt;
&lt;p&gt;We missed this one last week, a very nice discussion of the use of machine learning in health care and some of the ethical problems this raises, in particular, the problem of expertise that we must trust, but that we cannot engage with. &lt;/p&gt;
&lt;p&gt;We also missed a related piece from Wired on &lt;a href="https://www.wired.com/story/technique-uses-ai-fool-other-ais/"&gt;how easily algorithms can be fooled&lt;/a&gt;, for example, when assessing a medical claim. &lt;/p&gt;
&lt;p&gt;Even more further reading on deception from IEEE, &lt;a href="https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/ai-deception-when-your-ai-learns-to-lie"&gt;AI Deception: When Your Artificial Intelligence Learns to Lie&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And there's even a conference coming up: &lt;a href="https://sites.google.com/view/deceptecai2020"&gt;1st International Workshop on Deceptive AI @ECAI2020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/the-risks-of-algorithmic-il-literacy-on-healthcare-platforms-130914"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Can YouTube Quiet Its Conspiracy Theorists?&lt;/h2&gt;
&lt;p&gt;The extreme and radicalizing nature of Youtube's algorithm has been a topic of significant discussion, an now "A new study examines YouTube’s efforts to limit the spread of conspiracy theories on its site, from videos claiming the end times are near to those questioning climate change."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/interactive/2020/03/02/technology/youtube-conspiracy-theory.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 17 - Feb 25</title><link href="/ai-ethics-news-roundup-feb17-feb25-2020.html" rel="alternate"></link><published>2020-02-25T09:20:00+01:00</published><updated>2020-02-25T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-25:/ai-ethics-news-roundup-feb17-feb25-2020.html</id><summary type="html">&lt;p&gt;@HMRoff on deceptive AI, @dryellowbean &amp;amp; @jud1ths1mon on ethics, @vdignum, @MullerCatelijne and @RecklessCoding on the EU AI Ethics Whitepaper, @Klonick provokes on the definition of AI, new AI ethics map from @aiethicslab&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;First analysis of the EU Whitepaper on AI&lt;/h2&gt;
&lt;p&gt;"This week, Europe took a clear stance on AI; foster the uptake of AI technologies, underpinned by what it calls ‘an ecosystem of excellence’, while also ensuring their compliance with to European ethical norms, legal requirements and social values, ‘an ecosystem of trust’. While the Whitepaper on AI of the European Commission does not propose legislation yet, it announces some bold legislative measures, that will likely materialize by the end of 2020. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://allai.nl/first-analysis-of-the-eu-whitepaper-on-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Deception: When Your Artificial Intelligence Learns to Lie&lt;/h2&gt;
&lt;p&gt;"Understanding the breadth of what “AI deception” looks like, and what happens when it is not a human’s intent behind a deceptive AI, but instead the AI agent’s own learned behavior. These may seem somewhat far-off concerns, as AI is still relatively narrow in scope and can be rather stupid in some ways. To have some analogue of an “intent” to deceive would be a large step for today’s systems. However, if we are to get ahead of the curve regarding AI deception, we need to have a robust understanding of all the ways AI could deceive. We require some conceptual framework or spectrum of the kinds of deception an AI agent may learn on its own before we can start proposing technological defenses."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/ai-deception-when-your-ai-learns-to-lie"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics lab releases AI Princples Map&lt;/h2&gt;
&lt;p&gt;"We decided to create the AI Principles Map to help understand the trends, common threads, and differences among numerous sets of principles published."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://aiethicslab.com/big-picture/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Thinking About ‘Ethics’ in the Ethics of AI&lt;/h2&gt;
&lt;p&gt;"One of the fundamental questions in the ethics of AI, therefore, can be formulated as a problem of value alignment: how can we build autonomous AI that is aligned with societally held values."&lt;/p&gt;
&lt;p&gt;"Our review of the theoretical, technical, and ethical challenges to machine ethics does not intend to be exhaustive or conclusive, and these challenges could indeed be overcome in future research and development of autonomous AI. However, we think that these challenges do warrant a pause and reconsideration of the prospects of building ethical AI. In fact, we want to advance a more fundamental critique of machine ethics before exploring another path for answering the value alignment problem."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://revistaidees.cat/en/thinking-about-ethics-in-the-ethics-of-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethical Dimensions of Using Artificial Intelligence in Health Care&lt;/h2&gt;
&lt;p&gt;"An artificially intelligent computer program can now diagnose skin cancer more accurately than a board-certified dermatologist.1 Better yet, the program can do it faster and more efficiently, requiring a training data set rather than a decade of expensive and labor-intensive medical education. While it might appear that it is only a matter of time before physicians are rendered obsolete by this type of technology, a closer look at the role this technology can play in the delivery of health care is warranted to appreciate its current strengths, limitations, and ethical complexities."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://journalofethics.ama-assn.org/article/ethical-dimensions-using-artificial-intelligence-health-care/2019-02"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Would you rather a human or a black box AI perform surgery on you? Would you want the AI to be illegal?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/geoffreyhinton/status/1230592238490615816"&gt;This tweet&lt;/a&gt; sparked a wide ranging discussion this week. Some objected to the framing, arguing that the presumption of superior outcomes from the AI smuggled in a lot of assumptions regarding whom, exactly, and in what cases, might expect to benefit. Others argued that the question of illegality is an orthogonal one, and ought not be considered directly alongside a claim to superior performance. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/geoffreyhinton/status/1230592238490615816"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What Artificial Intelligence Is Not&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence is not one thing." - a short provocative piece from Kate Klonick on the many different thigns that we can mean when we talk about "AI".  &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://blog.lareviewofbooks.org/provocations/artificial-intelligence/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI and the bottom line: 15 examples of artificial intelligence in finance&lt;/h2&gt;
&lt;p&gt;An updated rundown of AI applications in finance. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://builtin.com/artificial-intelligence/ai-finance-banking-applications-companies"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Forensic Architecture Founder Says United States Prevented His Visit Via Algorithm&lt;/h2&gt;
&lt;p&gt;"Eyal Weizman, the director of the investigative group, said an embassy official in London told him an algorithm had identified a security threat that was related to him."&lt;/p&gt;
&lt;p&gt;“Associative algorithms, triangulating algorithms, that look at patterns that look at relations between actions and movement, between people and places,” he said. “We need to gear up to be able identify and monitor those.”&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/02/19/arts/design/forensic-architecture-founder-says-us-denied-him-visa.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 12 - Feb 18</title><link href="/ai-ethics-news-roundup-feb2-feb18-2020.html" rel="alternate"></link><published>2020-02-18T09:20:00+01:00</published><updated>2020-02-18T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-18:/ai-ethics-news-roundup-feb2-feb18-2020.html</id><summary type="html">&lt;p&gt;History of algorithms w/ @RiederB,@silvertjeand,@_mstevenson, @dorotheabaur finds a dilemma for companies ethics-washing facial recognition, @_KarenHao on OpenAI, @chengela and @hannahdev on reading emotion, @poppynoor on medtech, and @emilymbender tweeting from AAAS2020&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Emotion AI researchers say overblown claims give their work a bad name&lt;/h2&gt;
&lt;p&gt;There are no strong, peer-reviewed studie proving that analyzing body posture or facial expressions can help pick the best workers or students (in part because companies are secretive about their methods). As a result, the hype around emotion recognition, which is projected to be a $25 billion market by 2023, has created a backlash from tech ethicists and activists who fear that the technology could raise the same kinds of discrimination problems as predictive sentencing or housing algorithms for landlords deciding whom to rent to.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615232/ai-emotion-recognition-affective-computing-hirevue-regulation-ethics/?utm_source=newsletters&amp;utm_medium=email&amp;utm_campaign=the_algorithm.unpaid.engagement"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why we should hope that corporate claims about the ethics of facial recognition are pure marketing&lt;/h2&gt;
&lt;p&gt;"If we acknowledge the non-commercial, or rather the effectively ‘far-beyond-commercial’ dimensions of facial recognition technology, we in fact severely restrict our abilities to demand accountability for the claims regarding their ‘ethical qualities’."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@dorotheabaur/why-we-should-hope-that-corporate-claims-about-the-ethics-of-facial-recognition-are-pure-marketing-d8b2fea83319"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The historical trajectories of algorithmic techniques: an interview with Bernhard Rieder&lt;/h2&gt;
&lt;p&gt;"In this interview, Michael Stevenson (MS) and Anne Helmond (AH) talk to Bernhard Rieder (BR) about his forthcoming book entitled Engines of Order: A Mechanology of Algorithmic Techniques (University of Amsterdam Press, 2020). In particular, Rieder discusses how the practice of software-making is “constantly faced with the ‘legacies’ of previous work” and how the past continues to operate into present algorithmic techniques."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.tandfonline.com/doi/full/10.1080/24701475.2020.1723345"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;We know ethics should inform AI. But which ethics?&lt;/h2&gt;
&lt;p&gt;The ethical standards for assessing AI and its associated technologies are still in their infancy. Companies need to initiate internal discussion as well as external debate with their key stakeholders about how to avoid being caught up in difficult situations.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.gigabitmagazine.com/ai/we-know-ethics-should-inform-ai-which-ethics?utm_content=117506893&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-2797602696"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Navigating AI’s expanding role in the world of HR&lt;/h2&gt;
&lt;p&gt;As the use of artificial intelligence continues to skyrocket within the HR realm, related ethics issues represent real risk if not handled correctly, and early. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hrexecutive.com/navigating-ais-expanding-role-in-the-world-of-hr/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The messy, secretive reality behind OpenAI’s bid to save the world&lt;/h2&gt;
&lt;p&gt;Karen Hao spent half a year digging into @OpenAI, the SF-based AI research lab, originally founded by @elonmusk. I started with a few simple questions: Who are they? What are their goals? How do they work? After nearly three dozen interviews, I found so much more.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;"AI systems claiming to 'read' emotions pose discrimination risks&lt;/h2&gt;
&lt;p&gt;There are no strong, peer-reviewed studie proving that analyzing body posture or facial expressions can help pick the best workers or students (in part because companies are secretive about their methods). As a result, the hype around emotion recognition, which is projected to be a $25 billion market by 2023, has created a backlash from tech ethicists and activists who fear that the technology could raise the same kinds of discrimination problems as predictive sentencing or housing algorithms for landlords deciding whom to rent to.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/feb/16/ai-systems-claiming-to-read-emotions-pose-discrimination-risks"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Can we trust AI not to further embed racial bias and prejudice?&lt;/h2&gt;
&lt;p&gt;Heralded as an easy fix for health services under pressure, data technology is marching ahead unchecked But is there a risk it could compound inequalities? Poppy Noor investigates&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.bmj.com/content/368/bmj.m363"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Great twitter thread from the Ethics and AI panel panel at American Association for the Advancement of Science 2020&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/emilymbender/status/1228749166622334976"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>The Ethics of Data Processing: Comparing the CCPA and GDPR</title><link href="/02-12-gdpr-ccpa.html" rel="alternate"></link><published>2020-02-12T07:20:00+01:00</published><updated>2020-02-12T07:20:00+01:00</updated><author><name>Annie Valentine</name></author><id>tag:None,2020-02-12:/02-12-gdpr-ccpa.html</id><summary type="html">&lt;p&gt;GDPR Lite? A comparison of the CCPA and GDPR&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;Why the California Consumer Privacy Act, why now?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://ccpa-info.com/california-consumer-privacy-act-full-text/"&gt;California’s CCPA&lt;/a&gt; came into force January 1st 2020, becoming fully enforceable penalties and all, July 1st 2020. &lt;/p&gt;
&lt;p&gt;The act has been endearingly referred to as &lt;em&gt;GDPR Lite&lt;/em&gt;, a more laid-back version of the GDPR that could be passed through California’s legislative process with more minimal protest from stakeholders. The CCPA applies against a backdrop of diverse businesses processing personal data. &lt;/p&gt;
&lt;p&gt;In anticipation of getting a piece of the new “sharing economy,” countless businesses have cropped up across California which capitalise on the opportunity to cut out the middle-men of their industry. &lt;/p&gt;
&lt;p&gt;Business models in the sharing economy bring goods and services directly to consumers, without the stress and liability of managing the aforementioned goods or services themselves. Think Airbnb, which is known as the “The world’s largest hotel which owns no property.” Or Uber, which is the largest ride-sharing service in the world, yet owns no cars and (&lt;a href="https://www.theverge.com/2019/3/12/18261755/uber-driver-classification-lawsuit-settlement-20-million"&gt;so far&lt;/a&gt;) employs no drivers. &lt;/p&gt;
&lt;p&gt;Companies participating in the sharing economy rely on data-driven methodologies to maintain healthy profit margins. Facebook can afford to provide you with a “free” service because they profit from the data you generate while engaging with their services. FB’s overall value increases as you post content. California’s Silicon Valley culture has sent a resounding message – &lt;em&gt;data is the gift that keeps on giving&lt;/em&gt;. With a sea of new start-ups relying on data to bolster fragile business models, it’s no wonder that the first US state to enact its own privacy legislation is California. The privacy risk consumers face has increased exponentially. &lt;/p&gt;
&lt;p&gt;&lt;b&gt;59%&lt;/b&gt; &lt;em&gt;of companies believe that data and data analytics are vital to their organisation&lt;/em&gt;&lt;br&gt;
&lt;b&gt;29%&lt;/b&gt; &lt;em&gt;of businesses surveyed believed the “big data” allowed them to generate new revenue from existing products or services&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://eiuperspectives.economist.com/technology-innovation/business-data-0"&gt; The Economist, The Business of Data, Intelligence Report, The Economist Intelligence Unit (EIU) 2016&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The dangers of the digital economy&lt;/h2&gt;
&lt;p&gt;While most digital consumers are wary the moment they input a credit card or phone number online, they may not be aware of the technicalities of data processing, understand nuances of the legal framework, or be familiar with technical terms like “shadow profiling.” &lt;/p&gt;
&lt;p&gt;A shadow profile is made up of data harvested from a multitude of sources, collected as you engage with websites, apps and other digital services. Data is combined by linking matching data points. Accurate estimates can then be made about who you are – your gender, health, interests, socio-economic status, and even your criminal background may be accurately determined.  &lt;/p&gt;
&lt;p&gt;We are also surrounded by a multitude of sensors day-to-day that may be collecting data. Your phone, personal assistant, discounted TV nabbed in the Black Friday sale, and even your refrigerator or vacuum cleaner may contain sensors. &lt;/p&gt;
&lt;p&gt;The consequences of loss of personal data have devastating effects, including, as &lt;a href="https://www.eclypses.com/wp-content/uploads/2019/10/CCPA_Regulations-SRIA-DOF.pdf"&gt;California’s DOJ remarks&lt;/a&gt; “financial fraud, identity theft, unnecessary costs to personal time and finances… reputational damage, emotional stress, and even potential physical harm.” The DOJ also cited that there is a pressing need for enhanced consumer privacy rights, as “neither state nor federal law have kept pace with these [technological] developments in ways that enable consumers to exert control over the collection, use, and protection of their personal information.” &lt;/p&gt;
&lt;p&gt;One of the biggest concerns for consumers is how to prevent data from being sold to and used in unpredictable ways by third, fourth, and fifth parties, as it’s sold on ad infinitum. Chances are, if you are not paying a subscription for your software-heavy product, your data is being used to fill the profit-gaps. The CCPA is intended to provide traceability for this very reason.  &lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://oag.ca.gov/sites/all/files/agweb/pdfs/privacy/ccpa-isor-appendices.pdf"&gt;statement of reasons&lt;/a&gt; for the CCPA, the California legislature notes that “although California has been a leader in privacy protection, the law has not kept pace with rapid technological developments and the proliferation of personal information that fuels the internet economy. As a result, consumers are largely unable to control or even understand the collection and use of their personal information by a myriad of businesses and organizations.” &lt;/p&gt;
&lt;h2&gt;Isn’t the CCPA just another GDPR? Think again...&lt;/h2&gt;
&lt;p&gt;The intention of the CCPA echoes that of the GDPR. The GDPR states in its preamble that “[technological] developments require a strong and more coherent data protection framework in the Union...natural persons should have control of their own personal data.”&lt;/p&gt;
&lt;p&gt;How do the rights granted to consumers in the GDPR compare to those under the CCPA? 
On the face of it, the rights granted to California’s consumers mirror those enumerated in the GDPR. The CCPA also includes the right to be informed about what data is being collected, the right to be informed about how and why your personal data is being used and sold, the right to request deletion of your data, the right to request that companies do not sell personal data, and protections for consumers against discrimination for exercising these rights among other rights. &lt;a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB1121"&gt;You can read the full act here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are other similarities. The European Data Protection Supervisory (or EDPS) has remarked that access to a service must not be made conditional upon the individual being forced to ‘consent’ to being tracked and prevents discrimination. Both the GDPR and CCPA explicitly prohibit discriminatory practices against consumers who choose to exercise their privacy rights. &lt;/p&gt;
&lt;p style="font-size: 18px"&gt;&lt;b&gt;However, being compliant with the GDPR does not mean you are automatically compliant with the CCPA. &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Prior to July of this year, businesses will need to prepare on the ground by, for example, reviewing their site designs and updating their records of data processing. Consumers who expect the CCPA to protect them to the same degree as the GDPR will need to manage high expectations.&lt;/p&gt;
&lt;p&gt;The CCPA is much more narrow in scope than the GDPR, excluding certain types of data such as health information, information related to vehicles, or information related to credit worthiness. &lt;/p&gt;
&lt;p&gt;Additionally, the CCPA applies only to businesses that meet certain thresholds – the most notable being that 50% or more of a business’ revenue must be derived from selling personal information. &lt;em&gt;“Selling” data doesn’t just cover cash in exchange for data, but is wide enough to cover other processes deriving value from data.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;Unlike the GDPR, which is more free-form in its requirements as to how businesses establish opt-in consent (e.g. opt-ins can be a tick-box, or an “I agree” button), the &lt;em&gt;CCPA requires that all websites have a link or logo reading “do not sell my personal information.”&lt;/em&gt; This logo may look jarring to consumers, as it makes it obvious that websites are selling consumer data. &lt;/p&gt;
&lt;h2&gt;The culture of privacy; is data privacy the same in the USA as in Europe?&lt;/h2&gt;
&lt;p&gt;The EU and USA have markedly different privacy cultures. &lt;a href="https://www.asser.nl/asserpress/books/?rId=12915"&gt;Experts argue&lt;/a&gt; that the EU relies more heavily on written laws to set privacy standards, the legislature pushing specific risk-management rules down onto businesses who view privacy compliance as a legal jargon, an administrative burden. &lt;/p&gt;
&lt;p&gt;By contrast, and “particularly in the United States, there is a surprising deep chasm between privacy law in the books and privacy practice on the ground…the focus is on a reputation-based approach.” In the US, privacy is more closely related to corporate responsibility than litigious risk-management. Regulators push the onus of responsibility onto businesses to determine and drive the appropriate privacy protections within their organizations.    &lt;/p&gt;
&lt;p&gt;This can be further demonstrated via the “data as a commodity” argument, which is gaining steam in the USA. This is the idea that data should be viewed as an asset you own and license others to use. Prohibited in earlier versions of the act, the CCPA allows &lt;a href="https://oag.ca.gov/sites/all/files/agweb/pdfs/privacy/ccpa-isor-appendices.pdf"&gt;financial incentives or price differentials to compensate&lt;/a&gt; for profit gaps. If a consumer doesn’t want to consent to having their data used, they can just pay money to avoid compromising their data.&lt;/p&gt;
&lt;h2&gt;How American business empires were built on consumer data and loose terms of privacy&lt;/h2&gt;
&lt;p&gt;The payment model is currently being marketed as a potential legal “solution” for businesses who have built fragile business models that mostly rely on data to generate profit. A great example of a business with such a model is Google. Google offers a free service directly to the consumer – you can Google something, use services like Gmail, Google Maps and Google docs without a monetary charge. Their business model is fragile because it relies on data, and without data there is no business.&lt;/p&gt;
&lt;p&gt;Google can afford to stay on the forefront of technological development, have brick-and-mortar locations globally employing thousands of people, and provide consumers with top-tier services and ongoing support by using the data they collect from you. In 2018, &lt;a href="https://www.statista.com/statistics/266206/googles-annual-global-revenue/"&gt;85% of Google’s revenue&lt;/a&gt; came from ads. Let’s look at targeted ads as an example. &lt;/p&gt;
&lt;p&gt;Targeted ads work by collecting, analyzing, and processing consumer data. This data processing is used to generate ads that are relevant to you. Data may be disclosed or sold to third parties. If the legal privacy landscape changes significantly, prohibiting Google from using and selling personal data as it can be done today, the business model collapses.  &lt;/p&gt;
&lt;p&gt;A great example of what would happen if the tide were to turn in favor of stronger privacy regulation can be demonstrated by the recent senatorial hearings with Facebook, following the &lt;a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal"&gt;Cambridge Analytica data scandal&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;During the hearings, &lt;a href="https://www.washingtonpost.com/news/the-switch/wp/2018/04/10/transcript-of-mark-zuckerbergs-senate-hearing/"&gt;Senator Grassley asked Mark Zuckerburg&lt;/a&gt; “Are you actually considering having Facebook users pay for you not to use the information?” He replied “I think what Sheryl &lt;a href="https://www.wsj.com/articles/facebooks-mark-zuckerberg-hints-at-possibility-of-paid-service-1523399467"&gt;Facebook’s COO&lt;/a&gt; was saying was that, in order to not run ads at all, we would still need some sort of business model.” &lt;/p&gt;
&lt;p&gt;Mark Zuckerburg stated that FB’s business model would fall apart if the company was prohibited from running ads. He implied that the burden of making up the loss in profits could go to consumers, perhaps by FB’s adopting of a “data as a commodity” approach.  &lt;/p&gt;
&lt;h2&gt;What snail beer can teach us about the ethical obligations of data processing&lt;/h2&gt;
&lt;p&gt;I can imagine Grassley admired the &lt;em&gt;chutzpah&lt;/em&gt; – no other industry would have the tenacity to argue such a point. On the face of it, it seems like a reasonable business solution. But let’s put it in the context of consumer protection law in food standards, from one of the best-known Scots law cases – &lt;a href="https://www.bailii.org/uk/cases/UKHL/1932/100.html"&gt;Donaghue vs Stevenson&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Mrs. Donaghue was treated to a ginger-beer by her friend in Wellmeadow Café in Paisley, one afternoon in 1928. Much to the horror of everyone involved, a lone decomposed snail was found at the bottom of her beer, eventually resulting in Donaghue’s hospitalization from gastroenteritis and shock. The case was fundamentally about negligence, as calculable harm was caused to Mrs. Donaghue. Donaghue vs Stevenson set off a chain-reaction that eventually resulted in the modern concept of “duty of care” toward consumers.   &lt;/p&gt;
&lt;p&gt;Now let’s say the “&lt;em&gt;ode-d’escargot&lt;/em&gt;” ginger-beer represents Facebook processing data negligently, such as selling data to third parties like Cambridge Analytica without appropriate controls. Mrs. Donaghue represents the consumer, harmed by the detrimental data processing.  &lt;/p&gt;
&lt;p&gt;The equivalent would be like if the drinks manufacturer, Mr. “Facebook” Stevenson, built his entire business model on the continuous sale of snail beer, maintaining a predilection for not telling his customers about the snails. Imagine if Stevenson testified in court that “snails” were his thing, and not only was he not prepared to stop selling snail beer, but henceforth intended to charge all of his customers who didn’t want a snail in their ginger beer extra for the privilege. &lt;/p&gt;
&lt;p&gt;After all, how else was he going to pay to keep the Café doors open after &lt;em&gt;all of this negative press&lt;/em&gt;? &lt;/p&gt;
&lt;h2&gt;CCPA: A step in the right direction, but not a complete solution&lt;/h2&gt;
&lt;p&gt;Propositions for a payment model have raised a number of &lt;a href="https://www.eclypses.com/wp-content/uploads/2019/10/CCPA_Regulations-SRIA-DOF.pdf"&gt;equity concerns&lt;/a&gt; for the DOJ, as “low-income groups may be more likely to give up their personal information in exchange for services while high-income groups will pay the service fee to protect their data.” Big data is arguing for “&lt;a href="https://www.economist.com/the-world-if/2018/07/07/data-workers-of-the-world-unite"&gt;Technofeudalism&lt;/a&gt;.” To continue with Facebook as the provocative example…&lt;/p&gt;
&lt;p&gt;Facebook represents the crown: the technology creator and controller. Since Facebook determines access to, use of and service conditions for their platform, they also represent the nobility in this example. &lt;/p&gt;
&lt;p&gt;FB users upload their own content to the platform, a process which NY times bestseller &lt;a href="http://www.roughtype.com/?p=634"&gt;Nicholas Carr&lt;/a&gt; cleverly named “digital sharecropping.” Creating content increases the value of Facebook overtime– generating profits for the company that consumers will never see. Ultimately, consumers pay for access to the platform with their data (and maybe in the future, with cash or check too). The consumer ultimately represents the peasantry, and always gets the short-end of the stick. &lt;/p&gt;
&lt;p&gt;If differential pricing models become an industry standard, the CCPA could unintentionally contribute to a ‘privacy class system,’ where higher socio-economic groups are able to pay to protect their personal information and disadvantaged groups have no choice but to allow their data to be used. &lt;/p&gt;
&lt;p&gt;Still, these services are so engrained into our education system, jobs, and daily lives that any move to regulate them has to navigate the complex short and long-term consequences diligently. Some of these technologies are so pervasive that to not have access to them puts consumers at a measurable social or financial disadvantage. &lt;/p&gt;
&lt;p&gt;While the CCPA is a step toward greater transparency and moves some power back into the hands of the consumer, there is still work to be done. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Remember, terms and conditions may change.&lt;/em&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 5 - Feb 11</title><link href="/ai-ethics-news-roundup-feb5-feb11-2020.html" rel="alternate"></link><published>2020-02-11T09:20:00+01:00</published><updated>2020-02-11T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-11:/ai-ethics-news-roundup-feb5-feb11-2020.html</id><summary type="html">&lt;p&gt;@alexsablay and @hjegou on tracking training data, @dorotheabaur on facial recognition, @lindakinstler on tech ethicists, @benzevenbergen and @drzimmermann on ethical pitfalls, SyRI from @SaschaSchendel, @datainnovation 2019 review, @PublicStandards report on AI&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;European parliament says it will not use facial recognition tech&lt;/h2&gt;
&lt;p&gt;"The European parliament has insisted it has no plans to introduce facial recognition technology after a leaked internal memo discussing its use in security provoked an outcry."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/feb/05/european-parliament-insists-it-will-not-use-facial-recognition-tech"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The SyRI case: a landmark ruling for benefits claimants around the world&lt;/h2&gt;
&lt;p&gt;"In NJCM cs/ De Staat der Nederlanden (NJCM vs the Netherlands), also known as the SyRI case, the court considered the legality of the System Risk Indication (SyRI), a system designed by the Dutch government to process large amounts of data collected by various Dutch public authorities to identify those most likely to commit benefits fraud."&lt;/p&gt;
&lt;p&gt;There's a &lt;a href="https://twitter.com/SaschaSchendel/status/1225094322510536705"&gt;great twitter thread on SyRI too&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://privacyinternational.org/news-analysis/3363/syri-case-landmark-ruling-benefits-claimants-around-world?PageSpeed=noscript"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Using ‘radioactive data’ to detect if a data set was used for training&lt;/h2&gt;
&lt;p&gt;"We have developed a new technique to mark the images in a data set so that researchers can determine whether a particular machine learning model has been trained using those images. This can help researchers and engineers to keep track of which data set was used to train a model so they can better understand how various data sets affect the performance of different neural networks."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/2002.00937"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Opposing facial recognition — why focusing on accuracy misses the point&lt;/h2&gt;
&lt;p&gt;"These are important and very worrying findings that emphasize the overall problem with algorithmic discrimination. But what does it imply when we argue against facial technology based on empirical arguments such as overall low accuracy and particular bias with regards to certain ethnic groups? In the worst case, proponents of facial recognition could use this argument to their advantage and take it to underline the need to train the technology better to make sure it improves its accuracy — as apparently done by Google last year. Better training means collecting more data. And collecting more data often means intruding into people’s privacy. We might open the floodgates for even larger scale data collection."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@dorotheabaur/opposing-facial-recognition-why-focusing-on-accuracy-misses-the-point-9b96ea3f864b"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;An Algorithm That Grants Freedom, or Takes It Away&lt;/h2&gt;
&lt;p&gt;"Across the United States and Europe, software is making probation decisions and predicting whether teens will commit crime. Opponents want more human oversight."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/02/06/technology/predictive-algorithms-crime.html?referringSource=articleShare
"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethicists were hired to save tech’s soul. Will anyone let them?&lt;/h2&gt;
&lt;p&gt;"While some tech firms have taken concrete steps to insert ethical thinking into their processes, Catherine Miller, interim CEO of the ethical consultancy Doteveryone, says there's also been a lot of "flapping round" the subject."&lt;/p&gt;
&lt;p&gt;"Critics dismiss it as "ethics-washing," the practice of merely kowtowing in the direction of moral values in order to stave off government regulation and media criticism. The term belongs to the growing lexicon around technology ethics, or "tethics," an abbreviation that began as satire on the TV show "Silicon Valley," but has since crossed over into occasionally earnest usage. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.protocol.com/ethics-silicon-valley"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society&lt;/h2&gt;
&lt;p&gt;"As AI (and associated AI-hype) grows more pervasive in our lives, its impact on society is ever more significant, raising ethical concerns and challenges regarding issues such as privacy, safety and security, surveillance, inequality, data handling and bias, personal agency, power relations, effective modes of regulation, accountability, sanctions, and workforce displacement. Only a multidisciplinary effort can find the best ways to address these concerns, including experts from various disciplines, such as ethics, philosophy, economics, sociology, psychology, law, history, politics, interaction design, informatics, social studies of science and technology, communication and media studies, and political science, as well as those with lived experience in relation to the impacts of AI systems. In order to address these issues in a scientific context, AAAI and ACM joined forces in 2018 to start the AAAI/ACM Conference on AI, Ethics, and Society. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/proceedings/10.1145/3375627#sec1"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics: Seven Traps&lt;/h2&gt;
&lt;p&gt;"The question of how to ensure that technological innovation in machine learning and artificial intelligence leads to ethically desirable—or, more minimally, ethically defensible—impacts on society has generated much public debate in recent years. Most of these discussions have been accompanied by a strong sense of urgency: as more and more studies about algorithmic bias have shown, the risk that emerging technologies will not only reflect, but also exacerbate structural injustice in society is significant."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://freedom-to-tinker.com/2019/03/25/ai-ethics-seven-traps/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence and Public Standards: Committee publishes report&lt;/h2&gt;
&lt;p&gt;“Our message to government is that the UK’s regulatory and governance framework for AI in the public sector remains a work in progress and deficiencies are notable. The work of the Office for AI, the Alan Turing Institute, the Centre for Data Ethics and Innovation (CDEI), and the Information Commissioner’s Office (ICO) are all commendable. But on transparency and data bias in particular, there is an urgent need for practical guidance and enforceable regulation."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.gov.uk/government/news/artificial-intelligence-and-public-standards-committee-publishes-report"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Most Significant AI Policy Developments in the United States in 2019&lt;/h2&gt;
&lt;p&gt;"2019 was a monumental year for artificial intelligence (AI) policy in the United States. The federal government took several important steps that prioritized AI development and deployment and positioned the United States to strengthen its global AI leadership, beginning with President Trump’s “Executive Order on Maintaining American Leadership in Artificial Intelligence,” which set the tone for the rest of the year."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/02/the-most-significant-ai-policy-developments-in-the-united-states-in-2019/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Interview with Will Griffin, VP of Ethics &amp; Diversity in AI, Hypergiant Industries</title><link href="/02-08-2020-interview-hypergiant.html" rel="alternate"></link><published>2020-02-08T07:20:00+01:00</published><updated>2020-02-08T07:20:00+01:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2020-02-08:/02-08-2020-interview-hypergiant.html</id><summary type="html">&lt;p&gt;Some thoughts about the interface between programmers and tech ethics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p style="float: right;margin-left: 20px;margin-bottom: 15px;"&gt;&lt;img alt="Alt Text" src="/images/will.jpg"&gt;&lt;/p&gt;

&lt;p&gt;Hypergiant Industries builds AI systems for enterprises and governments around the world. We were fortunate to have the chance to interview Will Griffin, Hypergiant’s VP of Ethics and Diversity, to talk about the challenges of bringing ethical awareness to the day to day operations of a major AI systems developer. &lt;/p&gt;
&lt;div style="border-bottom: 1px solid #666;margin-bottom: 30px;padding-bottom 20px;width: 50px"&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;As the VP of Ethics and Diversity in AI at Hyper Giant, you have joined a small but growing number of professionals with ‘ethics’ in their title. What led you to taking this position with a growing tech startup?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Our Co-Founder and CEO, Ben Lamm’s vision of “delivering on the future we were promised”, inspired me. Ben is an uber-successful serial entrepreneur with multiple exits to his credit. I know Hypergiant will be successful and, combined with a heavy emphasis on ethics; we have the potential to re-shape the way technology companies think about their obligations to society.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Going back even before your current position, what aspect of AI Ethics first caught your attention, and how has that shaped your view of technology?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Over my career I have primarily worked at the intersection of media and technology (including launching the first original content on the PlayStation Network and the first video on demand channel on cable with Comcast). There was very little industry regulation and the goal was always to be first.  We followed the Silicon Valley mantra of “go fast and break things.” Unfortunately, we have seen the negative impacts of that approach – especially on privacy. Since AI has the capacity to transform society in ways not previously imagined, I feel an obligation to be a part of making sure AI innovation works for the benefit of humanity.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What has surprised you most about your work as the VP of Ethics?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I have been surprised by two things. 1) The amount of emphasis and demands the public and policymakers are now placing on ethics in technology  and 2) the number of technologists willing to admit how little they know about embedding ethics in their workflows and the products they make.&lt;/p&gt;
&lt;h2&gt;AI Code of Ethics&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Hyper Giant has a heavy emphasis on the use of ethics in AI development, and even has an AI Code of Ethics, which is something rare to see in a tech startup. What led to the decision in the first place to create this code, and what was it like developing it?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The decision to create the AI Code of Ethics came from the top, Ben Lamm. I was brought in specifically to ensure that we stay at the forefront of best practices in operationalizing ethics in our AI workflows and educating our team, clients, and partners on the importance of embedding ethical reasoning at every step of the process. Developing our ethical code and framework has been a lot of fun. As part of the process I visit, and vet our approach, with some of the top thought leaders on ethics in AI. Sangeeta Mudnal (Group Project Manager, for Microsoft’s Responsible AI initiative) is on our Board of Advisors and is very helpful. Frank Buytendijk , head of Ethics at Gartner Research is a great sounding board and thought partner. The professors at Stanford’s AI Lab have been gracious in sharing their approach to ethics education with their computer science students. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Can you give an example of a time where you needed to make technical compromises to align with your ethical code?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To date our AI uses cases have not caused us to make technical compromises. Instead, we have to make changes to our AI development workflows. In the old days (pre-2019), AI developers would be presented with a business problem and immediately begin crafting a technical solution and the thought of ethics (or compliance) was an afterthought. Now we incorporate ethical reasoning at the outset and it forces the team to create a range of technical solutions to a given problem and choose the solution with the least ethical challenges and most people-centered beneficial outcomes. In the end, it makes the team more creative because we have to think of more solutions to client problems.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;There are many accusations that an ethics code only leads to ethics washing, as there is no proof that a company actually follows such a code. How do you ensure that this is not the case with Hyper Giant?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The AI Code of Ethics has been helpful, but the mindset that created it is the secret sauce. Every company should (and likely will) develop a statement for public relations reasons, but the companies that embed ethical reasoning into their workflows and culture are the ones who will own the future. We have found that becoming well-versed into ethical reasoning has increased our overall creativity and led our product teams to develop more robust AI solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How has having an AI Code of Ethics helped shape the work HyperGiant is doing?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Boards of Directors and C-suite executives are well-aware of the well-publicized cases of ethical failures in the technology industry. The image of Mark Zuckerberg sitting in front of Congress being taken to task for Facebook’s myriad ethical failures, including billions of dollars in fines, has prompted introspection on the part of senior leaders. This introspection has increased the calls for companies with a well-defined set of values and tangible experience embedding ethics into the products they create. This has increased the number of pitches we have invited into, but more importantly demonstrating ethics has allowed us to expand our scope of work and amount of projects with our current clients.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Can you describe an example where it was hard work to help your client’s adjust their expectations so that they would be compatible with your ethics code?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I would not want to identify a specific client, but I can speak to issues that apply to every client. Embedding ethics into AI development workflows is challenging at first. The first step is getting the CEO and other responsible C-suite executives to mandate compliance with the AI Code of Ethics. The second step is traing the relevant stakeholders in our ethical reasoning model and vetting all proposed use cases through the model. Once clients see the creativity unleashed and the number of technical solutions they are able to create, they began to embrace the model as a value-add to their overall workflow.&lt;/p&gt;
&lt;h2&gt;Ethics in Technology&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What do you believe is the biggest challenge we are facing in AI Ethics?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The biggest challenge is the prevailing Silicon Valley conventional wisdom that ethical mishaps are just the price to be paid for being first (“move fast, break things”). With the prediction that AI leadership will be worth trillions of dollars, it is easy to see why many companies will not want to change their approach to product development and deployment. We view our role as educating the industry on the economic value of ethics in AI and the trust it will create within the marketplace. We believe the trust generated is what will be required to compete and gain market leadership in the AI-driven world.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;At Ethical Intelligence, we are working to change the narrative around AI Ethics from ethics being a blocker in technology to ethics being an asset to long-term innovation. What inspires you about the field of AI Ethics, and what good do you see if doing in the world of AI?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I agree wholeheartedly with Ethical Intelligence’s approach. I am inspired by all of the discussion (and research) in industry, academia, and the media to raise awareness of the importance of ethics in AI. If privacy had this much attention at the early stages of the internet, I believe social media and other platforms would have evolved with many more protections built-in. The growth of AI gives us another chance to get ethics in technology right, and I am excited to be a part of that process.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anything further you would like to share?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ethics equals trust. Trust equals sustained economic value creation.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 28 - Feb 4</title><link href="/ai-ethics-news-roundup-jan28-feb4-2020.html" rel="alternate"></link><published>2020-02-04T09:20:00+01:00</published><updated>2020-02-04T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-04:/ai-ethics-news-roundup-jan28-feb4-2020.html</id><summary type="html">&lt;h1&gt;AIEthics news for Jan 28 - Feb 4 featuring health care and drug discovery (@exscientialtd and a great twitter thread from @DorotheaBaur), privacy (@BernardMarr), facial recognition (@castrotech @itifdc @mclaughlintech), impact assessment (@Rafael_A_Calvo), comment on EU AI regs from @datainnovation and more!&lt;/h1&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Would you take a drug discovered by artificial intelligence?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/DorotheaBaur/status/1223549933506191363"&gt;@DorotheaBaur sparked an interesting reaction on Twitter&lt;/a&gt;, where in a climate of frequent AI news that raises ethical concerns, she asked if we might agree that this was a good example an applicaiton of AI that was not ethically problematic. &lt;/p&gt;
&lt;p&gt;"The British startup Exscientia claims it has developed the first medication created using artificial intelligence that will be clinically tested on humans. The medication, which is meant to treat obsessive-compulsive disorder, took less than a year from conception to trial-ready capsule. Human trials are set to begin in March, but would you take a drug designed using artificially intelligent software?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.techjuice.pk/exscientia-an-oxford-based-biotech-company-is-going-to-test-the-ai-designed-medicine-on-humans/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence Is Not Ready For The Intricacies Of Radiology&lt;/h2&gt;
&lt;p&gt;"...while much of the theoretical basis for AI in the practice of radiology is extremely exciting, the reality is that the field has not yet fully embraced it. The most significant issue is that the technology simply isn’t ready, as many of the existing systems have not yet been matured to compute and manage larger data sets or work in more general practice and patient settings, and thus, are not able to perform as promised. Other issues exist on the ethical aspects of AI. Given the sheer volume of data required to both train and perfect these systems, as well as the immense data collection that these systems will engage in once fully mainstream, key stakeholders are raising fair concerns and the call for strict ethical standards to be put into place, simultaneous to the technological development of these systems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/saibala/2020/02/03/artificial-intelligence-is-not-ready-for-the-intricacies-of-radiology/#312ac17867eb"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Advancing impact assessment for intelligent systems&lt;/h2&gt;
&lt;p&gt;"We discuss how the EIA provides a partial blueprint for what we call a Human Impact Assessment for Technology (HIAT) and how more recent algorithmic and data protection impact assessment initiatives can contribute. We also discuss how ethical frameworks for such a human impact assessment could draw on recently established AI ethics principles. We argue that this approach will help build trust in an industry facing increasing criticism and scrutiny."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.repository.cam.ac.uk/handle/1810/300852"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What Is A Data Passport: Building Trust, Data Privacy And Security In The Cloud&lt;/h2&gt;
&lt;p&gt;"Data passports allow you to extend the encryption technology that used to be only available on a physical mainframe to cloud computing. Each piece of data in the cloud has a passport assigned to it, and with the passport, you can verify if the data is misused, if the passport is still valid, etc. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/bernardmarr/2020/01/31/what-is-a-data-passport-building-trust-data-privacy-and-security-in-the-cloud/#76c416f75843"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How the EU Should Revise its AI White Paper Before it is Published&lt;/h2&gt;
&lt;p&gt;"The European Commission is planning to release a white paper to support the development and uptake of artificial intelligence (AI). Early drafts of this white paper suggest that the Commission may call for additional AI regulations that would make it more expensive and more difficult for European businesses to use AI systems in many areas of the economy. Given the EU’s desire to be a leader in AI, and to use AI to bolster its global competitiveness, the Commission should avoid heavy-handed rules that would slow adoption of this emerging technology."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/02/how-the-eu-should-revise-its-ai-white-paper-before-it-is-published/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Critics Were Wrong: NIST Data Shows the Best Facial Recognition Algorithms Are Neither Racist Nor Sexist&lt;/h2&gt;
&lt;p&gt;"NIST assessed the false positive and false-negative rates of algorithms using four types of images, including mugshots, application photographs from individuals applying for immigration benefits, visa photographs, and images taken of travelers entering the United States. NIST’s report reveals that: &lt;/p&gt;
&lt;p&gt;a) The most accurate identification algorithms have “undetectable” differences between demographic groups
b) The most accurate verification algorithms have low false positives and false negatives across most demographic groups
c) Algorithms can have different error rates for different demographics but still be highly accurate"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://itif.org/publications/2020/01/27/critics-were-wrong-nist-data-shows-best-facial-recognition-algorithms"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COR-GAN: Correlation-Capturing Convolutional Neural Networks for Generating Synthetic Healthcare Records&lt;/h2&gt;
&lt;p&gt;"In this paper, we propose a novel framework called correlation-capturing Generative Adversarial Network (corGAN), to generate synthetic healthcare records. In corGAN we utilize Convolutional Neural Networks to capture the correlations between adjacent medical features in the data representation space by combining Convolutional Generative Adversarial Networks and Convolutional Autoencoders"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/cor-gan-correlation-capturing-convolutional-neural-networks-for-generating-synthetic-healthcare-records"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;New surveillance AI can tell schools where students are and where they’ve been)&lt;/h2&gt;
&lt;p&gt;"Not all AI being used by schools is facial recognition. That doesn’t mean the tech doesn’t come with privacy risks. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.vox.com/recode/2020/1/25/21080749/surveillance-school-artificial-intelligence-facial-recognition"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Connected cots, talking teddies, and the rise of the algorithmic child&lt;/h2&gt;
&lt;p&gt;"Digital technologies are now a ubiquitous part of our daily lives. And questions remain as to how these technologies are reshaping how we experience the world around us, and how the world around us is being reshaped. One area this is being played out is in the family – in changing the experience of not only childhood, but what constitutes good parenting."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://keyahconsulting.com/connected-cots-talking-teddies-and-the-rise-of-the-algorithmic-child/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 21 - Jan 28</title><link href="/ai-ethics-news-roundup-jan21-jan28-2020.html" rel="alternate"></link><published>2020-01-28T09:20:00+01:00</published><updated>2020-01-28T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-28:/ai-ethics-news-roundup-jan21-jan28-2020.html</id><summary type="html">&lt;p&gt;Latest AIEthics news: AI in health (@jessRmorley @Floridi), ethics-bashing (@Elibietti), value-alignment (@IasonGabriel), bias (@SimonHegelich @SciOrestis @thyjuancarlos @FabienneMarco), explainability from(@MaribelLopez) and more from @LloydDanzig, @alexcengler and @LizzieGibney!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;An ethically mindful approach to AI for health care&lt;/h2&gt;
&lt;p&gt;"Health-care systems worldwide face increasing demand, a rise in chronic disease, and resource constraints. At the same time, the use of digital health technologies in all care settings has led to an expansion of data. These data, if harnessed appropriately, could enable health-care providers to target the causes of ill-health and monitor the effectiveness of preventions and interventions. For this reason, policy makers, politicians, clinical entrepreneurs, and computer and data scientists argue that a key part of health-care solutions will be artificial Intelligence (AI), particularly machine learning."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)32975-7/fulltext"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy&lt;/h2&gt;
&lt;p&gt;"The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, "ethics" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called "ethics washing" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in "ethics bashing." This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/abs/10.1145/3351095.3372860"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence, Values and Alignment&lt;/h2&gt;
&lt;p&gt;"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bias in Word Embeddings&lt;/h2&gt;
&lt;p&gt;"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372843"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Road to Artificial Intelligence: An Ethical Minefield&lt;/h2&gt;
&lt;p&gt;"There are a number of ethical dilemmas woven inextricably into the field of Artificial Intelligence, many of which are often overlooked, even within the engineering community. Even the best intentions are often not enough to guarantee solutions free from unintended or undesired results, as humans can accidentally encode biases into AI engines and malicious actors can exploit flaws in models. In the short-term, accountability and transparency on behalf of tech companies is critical, as is vigilance on behalf of consumers."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.infoq.com/articles/algorithmic-integrity-ethics"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Preparing For AI Ethics And Explainability In 2020&lt;/h2&gt;
&lt;p&gt;"People distrust artificial intelligence and in some ways this makes  sense.  With the desire to create the best performing AI models, many organizations have prioritized complexity over the concepts of explainability and trust. As the world becomes more dependent on algorithms for making a wide range of decisions, technologies and business leaders will be tasked with explaining how a model selected its outcome. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/maribellopez/2020/01/21/preparing-for-ai-ethics-and-explainability-in-2020/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The battle for ethical AI at the world’s biggest machine-learning conference&lt;/h2&gt;
&lt;p&gt;"Bias and the prospect of societal harm increasingly plague artificial-intelligence research — but it’s not clear who should be on the lookout for these problems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/d41586-020-00160-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Ethical Upside to Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;"if correctly designed, AI should clarify and amplify the ethical frameworks that U.S. military leaders already bring to war. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://warontherocks.com/2020/01/the-ethical-upside-to-artificial-intelligence/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The case for AI transparency requirements&lt;/h2&gt;
&lt;p&gt;"As AI technologies quickly and methodically climb out of the uncanny valley, customer service calls, website chatbots, and interactions on social media and in virtual reality may become progressively less evidently artificial."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/research/the-case-for-ai-transparency-requirements/amp/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 15 - Jan 21</title><link href="/ai-ethics-news-roundup-jan21-2020.html" rel="alternate"></link><published>2020-01-21T09:20:00+01:00</published><updated>2020-01-21T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-21:/ai-ethics-news-roundup-jan21-2020.html</id><summary type="html">&lt;p&gt;Our weekly news roundup for Jan 15 - Jan 21: Including AI in Finance, political campaigns, social profiling, the workplace, the problem of identifying health data, and some pieces on explainability and governance.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Everything We Do Tells Something About Our Health - Including Our Taste in Music&lt;/h2&gt;
&lt;p&gt;Much of our data exhaust is rich enough that it can be used to generate accurate inferences about us in domains unrelated to context of collection. Machine learning analysis of large sets of data about our music listening habits can reveal information about our physical and mental health. This information, if directly present, would be subject to regulatory and ethical norms. &lt;/p&gt;
&lt;p&gt;"A crucial question is: what exactly is health data? Does it comprise the data from medical treatment and investigation, or is it more than that? In his research, Hooghiemstra refers to medical investigations, also mentioning wearables and apps. But where, precisely, is the line between what is and what is not health data?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dataethics.eu/health-data-can-include-your-taste-in-music/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI – Enabled Innovation, Part 1: Regulatory Intervention &amp;amp; AI in Financial Services&lt;/h2&gt;
&lt;p&gt;The first of a two-part series on AI in Finance: &lt;/p&gt;
&lt;p&gt;"Financial services regulators are promoting principles and giving guidance through public statements to set the expectation that firms need to ensure their governance model is fit for purpose when applied to AI enabled innovation. In time, these regulators will become more proactive in asking firms to demonstrate they fully understand their data assets and to explain how that data is exploited and how the associated risk is mitigated when using AI – enabled technologies. Financial services firms should develop a coherent AI strategy now in a way that anticipates how they will answer that question when it inevitably comes."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyslegaledge.com/2020/01/ai-enabled-innovation-part-1-regulatory-intervention-ai-in-financial-services/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Towards ethical and socio-legal governance in AI&lt;/h2&gt;
&lt;p&gt;"Many high-level ethics guidelines for AI have been produced in the past few years. It is time to work towards concrete policies within the context of existing moral, legal and cultural values, say Andreas Theodorou and Virginia Dignum."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/s42256-019-0136-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial intelligence: DeepMind unlocks secrets of human brain using AI learning technique&lt;/h2&gt;
&lt;p&gt;Researchers at Google-owned DeepMind discovered that a recent development in computer science regarding reinforcement learning could be applied to how the brain’s dopamine system works.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/artificial-intelligence-deepmind-ai-human-brain-neuroscience-a9286661.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Digital Political Ethics: Aligning Principles with Practice&lt;/h2&gt;
&lt;p&gt;"This report is the fruit of a bipartisan report to identify areas of agreement among key stakeholders concerning ethical principles and best practices in the conduct of digital campaigning in the United States. Although many have raised concerns about the potential for digital technologies to weaken or undermine democracy, the voices of digital political practitioners are largely absent from this discussion." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://citapdigitalpolitics.com/?page_id=1911"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithms at Work: The New Contested Terrain of Control&lt;/h2&gt;
&lt;p&gt;"We find that algorithmic control in theworkplace operates through six main mechanisms, which we call the“6Rs”—employers can use algorithms to direct workers by restrictingand recommending, evaluate workers by recording and rating, and discipline workers by replacing and rewarding. We also discuss several key insights regarding algorithmic control."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://www.angelechristin.com/wp-content/uploads/2020/01/Algorithms-at-Work_Annals.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Black-Boxed Politics:Opacity is a Choice in AI Systems&lt;/h2&gt;
&lt;p&gt;"There are many myths and misconceptions about AI, but in cases where these systems are being used in sensitive, high-risk scenarios such as public health and criminal justice, arguably the most damaging msconception is that these systems are ‘black boxes’ about which we simply cannot know anything."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@szymielewicz/black-boxed-politics-cebc0d5a54ad"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The US just released 10 principles that it hopes will make AI safer&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Booker beware: Airbnb can scan your online life to see if you’re a suitable guest&lt;/h2&gt;
&lt;p&gt;"Details have emerged of its “trait analyser” software built to scour the web to assess users’ “trustworthiness and compatibility” as well as their “behavioural and personality traits” in a bid to forecast suitability to rent a property. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.standard.co.uk/tech/airbnb-software-scan-online-life-suitable-guest-a4325551.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Beyond Bias: Contextualizing “Ethical AI” Within the History of Exploitation and Innovation in Medical Research&lt;/h2&gt;
&lt;p&gt;This is from late December, but it just reached us and is worth a read: "It’s time for us to move beyond “bias” as the anchor point for our efforts to build ethical and fair algorithms."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.media.mit.edu/articles/beyond-bias-contextualizing-ethical-ai-within-the-history-of-exploitation-and-innovation-in-medical-research/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 1 - Jan 15</title><link href="/ai-ethics-news-roundup-jan1-jan15-2020.html" rel="alternate"></link><published>2020-01-15T09:20:00+01:00</published><updated>2020-01-15T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-15:/ai-ethics-news-roundup-jan1-jan15-2020.html</id><summary type="html">&lt;p&gt;Happy New Year! This roundup includes the Artificial Intelligence Video Interview Act, AI in diagnostic systems, Deep Fakes, How to build an ethical career, and more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;Happy New Year! &lt;/p&gt;
&lt;h2&gt;The use of AI in job search processes and tools&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.vox.com/recode/2020/1/1/21043000/artificial-intelligence-job-applications-illinios-video-interivew-act"&gt;Artificial Intelligence Video Interview Act&lt;/a&gt; takes effect Jan 1 2020. Many video interview tools now incorporate some kind of AI screening to generate reports on candidates, a practice which raises several AI ethics and safety concerns. &lt;/p&gt;
&lt;p&gt;A new article at the WSJ discusses some of these: &lt;a href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;How Job Interviews Will Transform in the Next Decade&lt;/a&gt;. "Recruiters using AI and virtual-reality simulations may hire based on a candidate’s behaviour, personality traits and physiological responses—no resumes needed"&lt;/p&gt;
&lt;p&gt;And you can already purchase countermeasures, if you can afford it. &lt;a href="https://www.scmp.com/news/asia/east-asia/article/3045795/south-korean-job-applicants-are-learning-trick-ai-hiring-bots"&gt;South Korean job applicants are learning to trick AI hiring bots that use facial recognition tech&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a topic EI has been thinking deeply about for a few months, and we have a detailed blog article in the works ... stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Comparison of the GoogleHealth breast AI paper against the RSNA Editorial Board recommendations for Assessing Radiology Research on Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;Are we holding AI diagnostic tools to the right standards? A great tweet from &lt;a href="http://twitter.com/DrHughHarvey"&gt;@DrHughHarvey&lt;/a&gt;. In the replies there's another good piece from October &lt;a href="https://www.nature.com/articles/s41598-019-51503-3"&gt;Using artificial intelligence to read chest radiographs for tuberculosis detection: A multi-site evaluation of the diagnostic accuracy of three deep learning systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/DrHughHarvey/status/1213548573071204352"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Technology Can't Fix Algorithmic Injustice&lt;/h2&gt;
&lt;p&gt;We need greater democratic oversight of AI not just from developers and designers, but from all members of society.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://bostonreview.net/science-nature-politics/annette-zimmermann-elena-di-rosa-hochan-kim-technology-cant-fix-algorithmic"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data and Justice in 2019 — Who can afford big tech, and who can live without it?&lt;/h2&gt;
&lt;p&gt;"What we see is that while you may not have access to the cloud, you can still be tracked and controlled by your government’s AI. This increase in the reach of data and analytics is even more noticeable for those who don’t have a country to call home. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://globaldatajustice.org/2020-01-01-data-and-justice-2019/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Deepfakes: The Looming Threat Of 2020&lt;/h2&gt;
&lt;p&gt;Deepfakes have been lurking on the internet for years now. But in 2020 the AI technology will become a powerful weapon for misinformation, fraud, and other crimes.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.designnews.com/artificial-intelligence/deepfakes-looming-threat-2020/109800999062105"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How will we remain USEFUL HUMANS? A longer post on the future of work, jobs, education and training&lt;/h2&gt;
&lt;p&gt;"As human intelligence (HI) encounters AI, will humans really become useless? Will all this progress be heaven (working only four hours per day, four days a week, but for the same money), or will it be hell (50% unemployment, rampant inequality and global civil unrest)? Or will it be both i.e. a kind of #hellven? Let’s have a look!"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thefuturesagency.com/2020/01/03/how-will-we-remain-useful-humans-a-longer-post-on-the-future-of-work-jobs-education-and-training/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Building an Ethical Career&lt;/h2&gt;
&lt;p&gt;Not an AI ethics piece, but some interesting reflections on how to build ethical awareness into our professional development. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hbr.org/2020/01/building-an-ethical-career"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The US just released 10 principles that it hopes will make AI safer&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Human-like robots spark fear in users according to researchers&lt;/h2&gt;
&lt;p&gt;"Japanese researcher Masahiro Mori’s “uncanny valley” theory, which he developed in the 1970s, states that we react positively to robots if they have physical features familiar to us -but they disturb us if they start looking too much like us."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.techspark.co/blog/2020/01/02/human-like-robots-spark-fear-in-users-according-to-researchers/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Tweeting from the Afterlife: Exploring the Deaths of Social Network Members and the Birth of Online Remembrance</title><link href="/tweeting-from-the-afterlife.html" rel="alternate"></link><published>2020-01-08T07:20:00+01:00</published><updated>2020-01-08T07:20:00+01:00</updated><author><name>Robert Seddon</name></author><id>tag:None,2020-01-08:/tweeting-from-the-afterlife.html</id><summary type="html">&lt;p&gt;Tweeting from the Afterlife: Exploring the Deaths of Social Network Members and the Birth of Online Remembrance&lt;/p&gt;</summary><content type="html">&lt;p&gt;Social networks such as Twitter and Facebook are frequent targets of criticism for handling personal data in ways that undermine the privacy and dignity of their users. Twitter recently found itself in the unusual position of receiving criticism for an effort to protect the privacy of user data, by announcing plans to initiate a cull of abandoned accounts on December 11 2019 as part of an effort to comply with EU data privacy regulations.&lt;/p&gt;
&lt;p&gt;Who would miss accounts that had been disused for ages? Quite a few people, it turned out. It wasn’t because lapsed users thought they might start using their accounts again. The accounts most at stake will never tweet again, because they fell silent when their owners died.&lt;/p&gt;
&lt;p&gt;By leaving their thoughts behind on the social network, people had created a presence for themselves that persisted long after death. Twitter suddenly faced objections from people who would visit the preserved thoughts of a departed &lt;a href="https://www.inc.com/jason-aten/twitter-said-it-would-delete-unused-accounts-then-it-realized-some-of-them-belong-to-people-we-want-to-remember.html"&gt;father&lt;/a&gt;, &lt;a href="https://www.bbc.co.uk/news/newsbeat-50584688"&gt;boyfriend&lt;/a&gt; or other loved ones.&lt;/p&gt;
&lt;p&gt;For a site that promotes itself as the place to go to discover ‘what’s happening in the world and what people are talking about right now’ this came as a surprise. In fact, it highlights one of the significant changes in online culture since use of the Internet became widespread. A decade ago, Friendster was a major social network, but usage declined until the site eventually closed, an event foreshadowed by an article in the satirical publication The Onion, which &lt;a href="https://www.theonion.com/internet-archaeologists-find-ruins-of-friendster-civili-1819594871"&gt;spoofed Friendster&lt;/a&gt; as an archaeological site marking a lost civilisation. Abandoned accounts on a social network aren’t just a liability, they reflect the health of the platform, and can be an undesirable signal that platforms might wish to conceal. It’s very natural for a business to value current and potential customers, and lost customers that might yet be regained, but much less to value those that are deceased. How should a social network value users that can no longer use its product? &lt;/p&gt;
&lt;h2&gt;Read-Only Memorials&lt;/h2&gt;
&lt;p&gt;As more and more of us have started living significant parts of our lives online, however, an increasing amount of the content on social platforms has been created by people who are no longer alive. Because of this, we can all expect to have ‘digital afterlives’.&lt;/p&gt;
&lt;p&gt;The sheer scale is remarkable. Take Facebook as an example, which already has a memorialisation feature for deceased users. &lt;a href="https://journals.sagepub.com/doi/full/10.1177/2053951719842540"&gt;Carl Öhman and David Watson project&lt;/a&gt; that billions of Facebook users will have passed away before 2100, by which time ‘the dead may well outnumber the living’. Öhman’s research with Luciano Floridi examines a whole &lt;a href="https://link.springer.com/article/10.1007/s11023-017-9445-2"&gt;digital afterlife industry&lt;/a&gt; dealing with &lt;a href="https://ora.ox.ac.uk/objects/uuid:c059841d-a702-4218-8c1b-39ff49dc6c65/download_file?safe_filename=Ohman_Floridi_R1_edited.pdf&amp;amp;file_format=application%252Fpdf&amp;amp;type_of_work=Journal+article"&gt;online remains&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Firms such as Eterni.me and Replica now offer consumers online chat bots, based on one’s digital footprint, which continue to live on after users die, enabling the bereaved to “stay in touch” with the deceased. This new phenomenon has opened up opportunities for commercial enterprises to monetise the digital afterlife of Internet users. As a consequence the economic interests of these firms are increasingly shaping the presence of the online dead.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In large part this is about how those still living can be helped to feel better and cope with loss—and that’s no small thing. Technology can honour the past as well as building the future. When London’s underground railway upgraded its automatic announcement system, in the process replacing the old ‘Mind the gap’ recording, it emerged that the actor who made it had left behind a widow who still listened for her husband’s voice at her station. Railway staff worked to digitise and restore the recording, securing emotional succour for one woman and over &lt;a href="https://twitter.com/garius/status/1204795961731629058"&gt;forty thousand likes on Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another form of benefit for those still living can come from the rich historical resource which all these online posters have cumulatively created. Öhman and Watson emphasise this aspect, describing the aggregate contributions of social media users as a form of cultural heritage which is of value both to historians and ‘to future generations as part of their record and self-understanding’. They advocate ‘a multi-stakeholder approach’ to the maintenance of this record: a commercial platform like Facebook has an obvious economic interest in the running of its own services, but other interested parties might include ‘states, NGOs, universities, libraries, museums’ and the like. Historic data might someday be handled like historic buildings, as assets that come with special responsibilities for their owners.&lt;/p&gt;
&lt;h2&gt;Mortal Obligations?&lt;/h2&gt;
&lt;p&gt;To say that preserving data of this sort benefits the living, however, isn’t to say that we should understand the value of their digital artifacts entirely in relation to the interests of living people’s wants and needs. We may care not only about doing good for the living, but also about doing the rightly respectful thing for the departed themselves.&lt;/p&gt;
&lt;p&gt;If we believe they are departed, though, to either oblivion or an otherworldly afterlife, then we may be perplexed about how we might be able to treat them well or badly: how they could be, in the philosophical jargon, moral patients. Explanations we might give for moral responsibilities towards sentient beings—explanations involving the capacity to suffer, for example—seem doubtfully applicable towards those who have gone to rest in peace.&lt;/p&gt;
&lt;p&gt;The problem isn’t that we can’t conceive of how there could be any kind of moral patient besides a living, thinking, feeling being. Some philosophers do believe there are other kinds of moral patient, and quite possibly you do as well: if you care about ‘the environment’ then you care for something that, though it incorporates various kinds of sentient organism, isn’t reducible to any of them. The problem is that the conceptual toolkit I’d use in asking, say, what could be wrong with wantonly destroying a fossilised ammonite isn’t a toolkit one can simply go ahead and apply to things left behind by human beings. We don’t relate to that long-dead organism as we do to a dead person.&lt;/p&gt;
&lt;p&gt;So questions are explored about what could make dead people, as a class, qualify as moral patients. Is it possible to harm the dead? Do practices like writing and honouring wills imply that obligations towards the dead person who is are disguised duties to the living one who was, or does that merely restate the paradox in another form?&lt;/p&gt;
&lt;h2&gt;Social Media Absence&lt;/h2&gt;
&lt;p&gt;Twitter was forced to consider dead people as a class among its account-holders, but that’s because of some users’ very specific connections with particular people they’ve lost: with parents and spouses and lovers and friends, people with names and personal histories. Parts of those individual histories linger online. When we remember people through mementos—a portrait, say—our treatment of the objects expresses attitudes towards the people themselves. If you see a social media profile in this way, like a portrait you’d hang in a place of honour, then the point of view from which it’s obsolete clutter in the database is going to be far from what you can personally endorse.&lt;/p&gt;
&lt;p&gt;Öhman and Floridi suggest that we should literally regard the digital material people leave behind as a form of human remains—‘not merely regarded as a chattel or an estate, but as something constitutive of one’s personhood’—and should draw on archaeological ethics to identify the principles behind respectful display of them. It’s unclear how far existing archaeological ethics will take us towards working out whether anyone has an obligation to fund the ongoing display, however; the Internet has nothing analogous to reburial.&lt;/p&gt;
&lt;p&gt;Some writers on the ethics of heritage and human remains contrast a materialist, empiricist West, which thinks of being dead as being gone, with different societies in which the dead are understood to have an ongoing presence in the life of a community. &lt;a href="http://www.piotrbienkowski.co.uk/"&gt;Piotr Bienkowski&lt;/a&gt;, for example, has written that scepticism about connections with people from former times arises from a view of the world that regards the dead ‘as no longer existing or having personhood in any sense’. If that’s how the West truly thinks, though, then perhaps our technological society is developing so that we start to think differently.&lt;/p&gt;
&lt;p&gt;It is now common for some of our most significant relationships to take place entirely online. If we encounter people through their online presence while they live to update it, maybe it’s not so strange a notion that something of them remains present while it persists on the servers. We should bear in mind, though, that for many people, their  online personae are not authoritative self-portraitures but often impressionistic or playful takes on themselves, sometimes multiple masques that relate only part of their personalities to specific audiences. We should perhaps be careful about these subtleties when memorializing online self-expression, and not take it more seriously in death than the mind behind it did in life. Nevertheless, even an outright parody account can be fondly regarded as part of a community. It leaves a hole in that community when the posting suddenly ceases, and can retain its place of honour in the ‘social graph’ of friend or follower relationships.&lt;/p&gt;
&lt;h2&gt;Last Words&lt;/h2&gt;
&lt;p&gt;In ten years we’ve gone from seeing Friendster’s decline spoofed as an archaeological dig to Öhman and Watson’s serious proposal that Internet hosts are custodians of a form of human cultural heritage. And as stories about bereavement go, Twitter’s experience carries a heartwarming moral: what seemed to be a load of disused data was in fact a memorial to the dead and is actually giving living people reasons to keep coming back to the site.&lt;/p&gt;
&lt;p&gt;Twitter was taken aback by the discovery that deceased people can still be part of its community. This is not the only form online remembrance can take—you may even have had a secret memorial sent to you in &lt;a href="https://xclacksoverhead.org/home/about"&gt;HTTP headers&lt;/a&gt; — but it’s a form that underlines how those content posters who’ve passed on can still have significant social and therefore ethical relationships with living people: relationships that matter to those people in ways that foster a deep commitment to keeping them going.&lt;/p&gt;
&lt;p&gt;Digital afterlives are a recent and growing source of questions for industry and policy—and Twitter will still be working out how best to square them with EU privacy rules. These are the early stages of working out how to handle personal grief on the public Internet: who shall bear the costs, and when it’s acceptable for the ‘digital afterlife industry’ to explore the opportunities. Will this lead to the involvement of religious bodies, such as the Vatican, when it comes to the preservation of digital afterlives? For tech companies already under the spotlight, approaching these questions with ethical sensitivity will be part of securing their own survival. For regulators and ethicists, this is an important issue that problematises recent approaches to privacy and data rights. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 17-24</title><link href="/ai-ethics-news-roundup-dec17-dec24.html" rel="alternate"></link><published>2019-12-24T09:20:00+01:00</published><updated>2019-12-24T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-24:/ai-ethics-news-roundup-dec17-dec24.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 17 - Dec 24&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;A great tweet from Arvind Narayanan&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/random_walker"&gt;Arvind Narayanan&lt;/a&gt; writes:&lt;/p&gt;
&lt;p&gt;"If you think there's too much yelling about algorithmic bias, here's an analogy. By the mid 90s the privacy community knew there was a huge problem. But it took two decades of yelling and a million privacy disasters before the public and policy makers started taking it seriously."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/random_walker/status/1208050796476215296"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Persons yet Unknown: Animals, Chimeras, Artificial Intelligence and Beyond&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/grok_"&gt;Kate Darling&lt;/a&gt; and &lt;a href="https://twitter.com/PKathrani"&gt;Paresh Kathrani&lt;/a&gt; in an fascinating discussion of robotics and AI and The Animal Law Conference. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.youtube.com/watch?v=dEFI05Gtalc"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Implementing Ethics Into Artificial Intelligence: A Contribution, From A Legal Perspective, To The Development Of An Ai Governance Regime&lt;/h2&gt;
&lt;p&gt;This is a new article added to a great issue of the Duke Law and Technology Review that came out in August, a &lt;a href="https://scholarship.law.duke.edu/dltr/vol18/iss1/"&gt;Symposium for John Perry Barlow&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;"This Article advocates for the need to conduct in-depth risk-benefit-assessments with regard to the use of AI and autonomous systems. This Article points out major concerns in relation to AI and autonomous systems such as likely job losses, causation of damages, lack of transparency, increasing loss of humanity in social relationships, loss of privacy and personal autonomy, potential information biases and the error proneness, and susceptibility to manipulation of AI and autonomous systems. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://scholarship.law.duke.edu/dltr/vol18/iss1/17/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Researchers were about to solve AI’s black box problem, then the lawyers got involved&lt;/h2&gt;
&lt;p&gt;When things go wrong and AI runs amok, the lawyers will be there to tell us the most company-friendly version of what happened. Most importantly, they’ll protect companies from having to share how their AI systems work.&lt;/p&gt;
&lt;p&gt;We’re trading a technical black box for a legal one. Somehow, this seems even more unfair.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thenextweb.com/artificial-intelligence/2019/12/17/researchers-were-about-to-solve-ais-black-box-problem-then-the-lawyers-got-involved/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Explainability and Adversarial Robustness for RNNs&lt;/h2&gt;
&lt;p&gt;"Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a common notion of adversarial robustness, and show that an adversarial training procedure can significantly and successfully reduce the attack surface."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/explainability-and-adversarial-robustness-for-rnns"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;This AI researcher is trying to ward off a reproducibility crisis&lt;/h2&gt;
&lt;p&gt;Joelle Pineau doesn’t want science’s reproducibility crisis to come to artificial intelligence (AI).&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/d41586-019-03895-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics In AI: Why Values For Data Matter&lt;/h2&gt;
&lt;p&gt;An argument for better corporate governance around AI and data. Corporations should "treat data as an asset .... the same way organizations treat inventory, fleet, and manufacturing assets. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/sap/2019/12/18/ethics-in-ai/#2d7dd5285af4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Virtuous Circle of Trusted AI: Turning Ethical and Transparent AI Into a Competitive Advantage&lt;/h2&gt;
&lt;p&gt;"Most large organizations today across the United States and Europe are talking about “duty of care” and AI (i.e. the duty to take care to refrain from causing another person injury or loss). We also hear a lot about the need for clear normative frameworks in areas such as driverless cars, drones, facial recognition, and algorithmic decisionmaking guidelines in public-facing services such as banking or retail. I shall be surprised if we will have this conversation again in two years’ time and legislation hasn’t already been seriously discussed or put in place."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.capgemini.com/research/the-virtuous-circle-of-trusted-ai-turning-ethical-and-transparent-ai-into-a-competitive-advantage-luciano-floridi/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 10-17</title><link href="/ai-ethics-news-roundup-dec10-dec17.html" rel="alternate"></link><published>2019-12-17T09:20:00+01:00</published><updated>2019-12-17T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-17:/ai-ethics-news-roundup-dec10-dec17.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 10 - Dec 17&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Index 2019 Report&lt;/h2&gt;
&lt;p&gt;An independent initiative within Stanford University’s Human-Centered Artificial Intelligence Institute, the report is in its third year and is the result of a collaborative effort led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry, in collaboration with more than 35 sponsoring partners and data contributors. The purpose of the project is to ground the discussion on AI in data, serving practitioners, industry leaders, policymakers and funders, the general public and the media that informs it. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hai.stanford.edu/news/introducing-ai-index-2019-report"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;When should we decline to write code? A small case study.&lt;/h2&gt;
&lt;p&gt;We picked this up on twitter when Emily Bender &lt;a href="https://twitter.com/ethicalai_co/status/1202638293269176321"&gt;tweeted&lt;/a&gt; that there was a task in an AI competition to create an AI that would solve problems involving the  "Prediction of Intellectual Ability and Personality Traits from Text". She's since &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;posted a thoughtful followup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an important problem. Technical and regulatory solutions should be augmented by professional codes of conduct and ethics if we want to ensure the safe and fair development of AI. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Do You Trust Jeff Bezos With Your Life? Tech Giants Like Amazon Are Getting into the Health Care Business&lt;/h2&gt;
&lt;p&gt;Would you trust the Tech Giants with your health data in exchange for more personalized and on-demand healthcare? This article covers the current initiative of telehealth by Amazon and dives into a few key implications that this new commodity would carry for society at large.&lt;/p&gt;
&lt;p&gt;"What health insurance companies, as well as employers who foot the bulk of the U.S.'s health care bill, especially fear from telehealth is that it's so easy to use that people will reach out more often for care. "It creates the risk that every little ache and pain results in a claim that has to be paid out," says the University of Pennsylvania's Asch. "Making people come into the office is health care rationing by inconvenience."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.newsweek.com/amazon-health-care-jeff-bezos-telemedicine-1475154"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A tug-of-war over biased AI&lt;/h2&gt;
&lt;p&gt;"A critical split divides AI reformers. On one side are the bias-fixers, who believe the systems can be purged of prejudice with a bit more math. (Big Tech is largely in this camp.) On the other side are the bias-blockers, who argue that AI has no place at all in some high-stakes decisions."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.axios.com/ai-bias-c7bf3397-a870-4152-9395-83b6bf1e6a67.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Emotion-detecting tech should be restricted by law&lt;/h2&gt;
&lt;p&gt;"The AI Now Institute says the field is "built on markedly shaky foundations".
Despite this, systems are on sale to help vet job seekers, test criminal suspects for signs of deception, and set insurance prices. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.bbc.co.uk/news/technology-50761116"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Would you let a Robot Take Care of Your Mother?&lt;/h2&gt;
&lt;p&gt;AI usage for social care is not a new concept. However, as it becomes more and more of a reality, we are forced to shift our questions from theoretical to personal.&lt;/p&gt;
&lt;p&gt;"Some worry robot care would carry a stigma:the potential of being seen as “not worth human company,” said one participant in a study of potential users with mild cognitive impairments."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2019/12/13/opinion/robot-caregiver-aging.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Owning Intelligence&lt;/h2&gt;
&lt;p&gt;The United States Patent and Trademark Office is trying to answer a very complicated question: who owns artificial intelligence?&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cigionline.org/articles/owning-intelligence"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics for Systemic Issues: A Structural Approach&lt;/h2&gt;
&lt;p&gt;"This paper calls for a "structural" approach to assessing AI’s effects inorder to understand and prevent such systemic risks where no individual can beheld accountable for the broader negative impacts. This is particularly relevantfor AI applied to systemic issues such as climate change and food security whichrequire political solutions and global cooperation. To properly address the widerange of AI risks and ensure ’AI for social good’, agency-focused policies must becomplemented by policies informed by a structural approach."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/48_aisg_neurips2019.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 2 - Dec 9</title><link href="/ai-ethics-news-roundup-dec-9.html" rel="alternate"></link><published>2019-12-09T09:20:00+01:00</published><updated>2019-12-09T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-09:/ai-ethics-news-roundup-dec-9.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 2 - Dec 9&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;On the Legal Compatibility of Fairness Definitions&lt;/h2&gt;
&lt;p&gt;Although the article was on arxiv last week, the author publicized it this week, and it's a good one. There's poor alignment between operationalized definitions of fairness in machine learning and the legal definitions that may in fact apply to the deployment of these systems. &lt;/p&gt;
&lt;p&gt;"Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex socio-technical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts, as well as what both the legal and ML fairness communities can learn from these tensions. We focus this paper on U.S. anti-discrimination law since the ML fairness research community regularly references terms from this body of law."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/1912.00761"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;'Good' isn’t good enough&lt;/h2&gt;
&lt;p&gt;A critical reflection on the problems that arise when the pursuit of good is taken on as a technical objective too hastily, and why sustained and rigorous ethical reflection is a necessary if we want to have any confidence that such efforts will actually succeed. &lt;/p&gt;
&lt;p&gt;"Despite widespread enthusiasm among computer scientists to contribute to “socialgood,” the field’s efforts to promote good lack a rigorous foundation in politicsor social change. There is limited discourse regarding what “good” actuallyentails, and instead a reliance on vague notions of what aspects of society aregood or bad. Moreover, the field rarely considers the types of social changethat result from algorithmic interventions, instead following a “greedy algorithm”approach of pursuing &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.benzevgreen.com/wp-content/uploads/2019/11/19-ai4sg.pdf"&gt;Read More&lt;/a&gt;technology-centric incremental reform at all points."&lt;/p&gt;
&lt;h2&gt;New guidelines on the GDPR Right to Explanation&lt;/h2&gt;
&lt;p&gt;The UK’s Data Protection Authority just issued much-anticipated guidance that clarifies the complicated issue of the GDPR’s ‘right to explanation’. Here is some background on the issue and what the new &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/arthur-ai/uk-gpdr-watchdog-says-explain-your-ai-373ef76d3c"&gt;Read More&lt;/a&gt;information means.&lt;/p&gt;
&lt;h2&gt;Ethics is Objective&lt;/h2&gt;
&lt;p&gt;Here are three arguments for the idea that ethics is subjective, presented with thoughtful rebuttals. This is a theme we took up in our last bog post, where we argued that there is a very large chunk of territory in tech ethics where ethical imperatives can be uncovered and agreed upon by sincere inquiry, &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.linkedin.com/pulse/ethics-objective-reid-blackman-ph-d-/?trackingId=B5NT8Dd1BMx16FH15vHvZQ%3D%3D"&gt;Read More&lt;/a&gt;even by those who disagree on more fundamental ethical and moral questions. &lt;/p&gt;
&lt;h2&gt;Online information of vaccines: information quality is an ethical responsibility of search engines&lt;/h2&gt;
&lt;p&gt;When health-related disinformation is available online, who is responsible? There's a growing backlash against the idea of platforms as "mere tools", but perhaps we should think the same of search engines. We don't usually think that a library is responsible for dangerous information in its books, but should we 
think differently about Google?&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.academia.edu/41168605/Online_information_of_vaccines_information_quality_is_an_ethical_responsibility_of_search_engines"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?&lt;/h2&gt;
&lt;p&gt;Multiple fairness constraints have been proposed in the literature, motivated by a range of concerns about how demographic groups might be treated unfairly by machine learning classifiers. In this work we consider a different motivation; learning from biased training data. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/recovering-from-biased-data-can-fairness-constraints-improve-accuracy"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Datafication&lt;/h2&gt;
&lt;p&gt;In the course of researching and discussing AI ethics challenges, we might run across the claim while the rate and scope of our generation of data has increased, it can be understood on a continuum with the ways in which human activity have always left traces and records. This article on the concept of "datafication" argues against this, and shows several ways to understand what is distinctive about the new systems and actors that collect and use our data. &lt;/p&gt;
&lt;p&gt;"Datafication is not just the making of information, which, in one sense, human beings have been doing since the creation of symbols and writing. Rather, datafication is a contemporary phenomenon which refers to the quantification of human life through digital information, very often for economic value. This process has major social consequences. Disciplines such as political economy, critical data studies, software studies, legal theory, and—more recently— decolonial theory, have considered different aspects of those consequences to be important. Fundamental to all such approaches is the analysis of the &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://policyreview.info/concepts/datafication"&gt;Read More&lt;/a&gt;intersection of power and knowledge. "&lt;/p&gt;
&lt;h2&gt;Amazon ready to cash in on free access to NHS data&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=https://www.thetimes.co.uk/article/amazon-ready-to-cash-in-on-free-access-to-nhs-data-bbzp52n5m""&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Nov 24 - Dec 1</title><link href="/ai-ethics-news-roundup-nov-24-dec-1.html" rel="alternate"></link><published>2019-12-02T09:20:00+01:00</published><updated>2019-12-02T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-02:/ai-ethics-news-roundup-nov-24-dec-1.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Nov 24 - Dec 1&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;The Second Wave of Algorithmic Accountability&lt;/h2&gt;
&lt;p&gt;"... the first wave of algorithmic accountability focuses on improving existing systems, a second wave of research has asked whether they should be used at all—and, if so, who gets to govern them".&lt;/p&gt;
&lt;p&gt;Frank Pasquale argues that we can distinguish the "... first wave of algorithmic accountability research and activism", which has targeted existing systems and helped illuminate urgent ethical concerns in the AI systems already online, from "...an emerging “second wave” of algorithmic accountability has begun to address more structural concerns.". &lt;/p&gt;
&lt;p&gt;"Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology". &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Machine Learning on Encrypted Data Without Decrypting It&lt;/h2&gt;
&lt;p&gt;"Recent breakthroughs in cryptography have made it practical to perform computation on data without ever decrypting it. In our example, the user would send encrypted data (e.g. images) to the cloud API, which would run the machine learning model and then return the encrypted answer. Nowhere was the user data decrypted and in particular the cloud provider does not have access to either the orignal image nor is it able to decrypt the prediction it computed."&lt;/p&gt;
&lt;p&gt;This application of homomorphic encryption systems might mitigate a number of data protection problems, even though it would still be imperative to ensure the data was collected ethically, and that the data itself is free from biases that are not understood and accounted for. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://juliacomputing.com/blog/2019/11/22/encrypted-machine-learning.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Tainted Data Can Teach Algorithms the Wrong Lessons&lt;/h2&gt;
&lt;p&gt;Security problems become ethics problems when vulnerabilities in software systems produce risks in their application that the stakeholders (both users, and the people in the social environment) are unable to understand, assess, and freely and knowingly accept. &lt;/p&gt;
&lt;p&gt;Adversarial input is a particularly powerful way to undermine machine learning systems and to cause them to behave in unexpected and unintended ways. "“Current deep-learning systems are very vulnerable to a variety of attacks, and the rush to deploy the technology in the real world is deeply concerning,” says Cristiano Giuffrida, an assistant professor at VU Amsterdam who studies computer security, and who previously discovered a major flaw with Intel chips affecting millions of computers."&lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://link.springer.com/article/10.1007%2Fs13347-019-00354-x"&gt;recent paper&lt;/a&gt; Luciano Floridi draws our attention to the extension of the practice of ethics dumping, "the export of unethical research practices to countries where there are weaker legal and ethical frameworks" into the digital realm. This is an ethical risk, but it is also a more basic risk to the quality of research. This article argues that there are also security problems with this sort of practice. "... some companies outsource the training of their AI systems, a practice known as machine learning as a service. This makes it far harder to guarantee that an algorithm has been developed securely."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wired.com/story/tainted-data-teach-algorithms-wrong-lessons/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithms, Automation, and News&lt;/h2&gt;
&lt;p&gt;A special issue of the Journal of Digital Journalism has been published on "Algorithms, Automation, and News".&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.tandfonline.com/doi/full/10.1080/21670811.2019.1685395"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Against Ethical AI: Guidelines and Self Interest&lt;/h2&gt;
&lt;p&gt;"In this paper we use the EU guidelines on ethical AI, and the responses to it, as a starting point to discuss the problems with our community's focus on such manifestos, principles, and sets of guidelines. We cover how industry and academia are at times complicit in ‘Ethics Washing’, how developing guidelines carries the risk of diluting our rights in practice, and downplaying the role of our own self interest. We conclude by discussing briefly the role of technical practice in ethics."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.halfwaytothefuture.org/programme/mcmillan-against-ethical-ai-guidelines-and-self-interest"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;5 Q’s for Anne Kao, Senior Technical Fellow at Boeing Research and Technology](https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/)&lt;/h2&gt;
&lt;p&gt;"The Center for Data Innovation spoke with Anne Kao, Senior Technical Fellow at Boeing Research and Technology. Kao discussed how she uses machine learning to analyze maintenance reports and how philosophy influences how she approaches data science."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ukraine denounces Apple for calling Crimea part of Russia in apps](https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O)&lt;/h2&gt;
&lt;p&gt;"Reuters reporters in Moscow who typed the name of the Crimean provincial capital Simferopol into Apple’s Maps and Weather apps on Wednesday saw it displayed as “Simferopol, Crimea, Russia”. Users elsewhere — including in Ukraine’s capital Kiev and in Crimea itself — see locations in Crimea displayed without specifying which country they belong to. "&lt;/p&gt;
&lt;p&gt;One might wonder if the technical specifications in the ticket for the engineering work that was involved discussed the political and ethical implications. It's perhaps difficult to imagine that something of this magnitude wasn't remarked upon, but on the other, so much engineering work in software proceeds as though it happens in at least partial isolation from the downstream social and political environment.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Falsehoods Programmers Believe About Ethics</title><link href="/falsehoods-programmers-believe-about-ethics.html" rel="alternate"></link><published>2019-12-02T07:20:00+01:00</published><updated>2019-12-02T07:20:00+01:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2019-12-02:/falsehoods-programmers-believe-about-ethics.html</id><summary type="html">&lt;p&gt;Some thoughts about the interface between programmers and tech ethics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a Tech Ethicist, I come into contact with programmers almost daily, as my work requires me to navigate the technical objectives and implementation details in addition to the ethical dimensions of the projects I analyze. Through these experiences, I’ve found myself having some of the same conversations over and over, all surrounding the applicability of ethics in the tech industry. There are some persistent misconceptions about both the study of ethics and its application to technical work. In order to help bring clarity to the confusion surrounding ethics in tech, I’m going to briefly discuss three significant misconceptions I've observed. &lt;/p&gt;
&lt;h2&gt;There is one right answer &lt;br /&gt; &lt;em&gt;(or no right answers, if you take a subjective approach)&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Ethics is the study of right and wrong, so shouldn’t there be a right and wrong answer to every ethical question? If there was, then the ethical dilemmas programers and technologists face would have long been answered by general principles with universal applicability across projects, cultures, and domains. Of course, if this were true, the daily news would not include so many stories involving ethical lapses or miscalculations where technological platforms, tools, and solutions cause serious harms. We are all familiar with the headline stories, self-driving cars making errors that human drivers would not, autonomous weapons systems being developed and deployed by militaries, banks and governments using AI driven gatekeepers to control access to credit and social services, and fintech companies making trading decisions at the speed of light, far faster than humans can intervene. These are all headline worthy examples of tech gone wrong due to ethical lapses. But, when we take a step back from these bigger stories, we also discover the points in which software engineers and technologists are at risk for the same ethical lapses, as they make decisions every day in the course of their work that can have real ethical impacts. ‘Do I use a social sign-on SDK, because it’s faster, even though there are privacy tradeoffs? Do we let a test suite languish to save time? Do we optimize for the biggest curve, even if that means missing important edge cases? Is the dataset we are pre-training an algorithm with fair, and ethically sourced?’&lt;/p&gt;
&lt;p&gt;It might seem like there are only two possible solutions to these ethical dilemmas. Either we finally discover this elusive single, straightforward, universal guideline that can be applied across the board, or, if that doesn’t exist, then we must throw up our hands in defeat because ethics is too subjective and we will never arrive at an acceptable single answer.  Why is it that these seem to be our only two possible solutions to solving tech ethics problems? Well, we all come from different backgrounds, which in turn shape our understanding of what constitutes right and wrong. Everything from our culture, our education, and our life experiences can potentially influence how we approach ethical problems. Because of this, being aware of, and finding solutions to the ethical dilemmas raised by technology requires careful navigation of the multiple understandings of right and wrong at play. To complicate matters further, ethics is an emotionally charged topic. We are very attached to our personal ethical frameworks, and are hurt when it is threatened or violated. And yet, there are situations in which two people can have opposing ethical frameworks, and still both arrive at what could be considered a right answer in accordance to those individual frameworks.  The combination of this emotional attachment along with the varying ethical frameworks results in the feeling that the right answer to our ethical dilemmas is impossible to find. &lt;/p&gt;
&lt;p&gt;However, I would argue otherwise, as there are significant overlaps in the ethical frameworks between individuals. This overlap in turn enables us to uncover an understanding of the problems we face and from there make informed decisions. What I'm suggesting is that we don't need to solve the fundamental metaethical dilemmas that philosophers have been arguing about for millennia in order to appreciate and mitigate the ethical risks that technical decisions expose us to. Often an informed ethical awareness is the most significant step towards improving our ability to foresee and to mitigate the ethical risks in our technical practices. &lt;/p&gt;
&lt;h2&gt;It is possible to eliminate ethical bias&lt;/h2&gt;
&lt;p&gt;Algorithmic bias has become a buzzword with a strong negative connotation which results in  often hearing that we need to eliminate bias in our algorithms. However, when it comes to bias in terms of ethical decision making, it is not actually possible to completely eliminate that bias. This is because data is not neutral, it reproduces the biases in the world it comes from, as well as new biases introduced knowingly or accidentally through collection, processing and use. When we try to mitigate bias in data, we must often confront real ethical dilemmas from the world it came from, and make our best efforts to address them. Confronting these dilemmas, or, in other words, attempting to mitigate the biases inherent in the data, involves taking a stance of our own, which is a sort of bias itself. So, in this sense, ethical bias is impossible to fully eliminate, which makes it something we should take on as a serious responsibility.&lt;/p&gt;
&lt;p&gt;Let’s explore this. You’re working for a university, collecting simple data on students such as class attendance, grades, library usage, when you’re asked to develop specific insight on student mental health out of this data. You of course know to clean the data and set controls to mitigate any previous biases as a first step. However, in order to move on to creating the algorithm that will allow you to draw insights into the mental health status of students, you must first take a biased stance on ethics. You either have to decide to create an algorithm that will track student mental health with the hopes of providing care and wellbeing for the greatest amount of people, or decide that this would be a violation of respecting the dignity of an individual, and so not create the algorithm.  In other words, your decision between the two equally as important ethical values depends on your own bias towards those values. Since there is no way to honor both values equally in this situation, a decision must be taken by prioritizing one value over the other, and the ethical bias cannot be avoided. It is also important to note that in this example, the ethical dilemma surfaces only after the info from the data is extracted. By extracting this knowledge on the mental status of students, an ethical imperative to help the identified students is created. This all goes to show, collecting knowledge from datasets is not an ethically neutral task. 
Programmers need to be acutely aware of what ethical values are at play and how they are prone to prioritize these values. It's easy to get lost in the weeds solving architectural and implementation problems, but even these ground level decisions can have ethical impacts we must be aware of. Choices of technologies, datasets, integration partners, and problem definitions all expose the ethical edges of technical development. At the macro level, it's important to take a step back, and make sure we have articulated and examined the values that have guided and shape the formulation of business and technical goals, so that the ways these values interface with other values in the world in which our systems are deployed are understood and made clear. It is therefore essential to to build this kind of ethical awareness into organizational culture and technical decision-making. &lt;/p&gt;
&lt;h2&gt;Ethics is a blocker to innovation&lt;/h2&gt;
&lt;p&gt;Is ethics someone else's problem? Surely the software’s only responsibility is to is to work, not understand ethics. This is a view I often hear when speaking with programmers, as they express an underlying fear of having constraints placed on their previously unencumbered freedom to develop their technology. Ethics can appear to be a blocker if it is viewed as just another piece of paperwork that needs to be filed or the opportunity for someone from a non-technical background who just doesn’t get ‘it’ to kill an interesting project before it’s had the chance to get off the ground. &lt;/p&gt;
&lt;p&gt;However, there’s another perspective. You can think about tech ethics in much the same way programmers are accustomed to thinking about technical debt. Making short term architectural decisions to meet immediate needs involves taking on a notion of "well, it works" that is dangerously myopic. Most programmers are intimately familiar with the exponential accumulation of technical debt, and the astounding difficulty in mitigating it, when working in an environment that doesn’t value longer term thinking. Ethics is the same way. Tiny shortcuts in seemingly insignificant systems can have real effects on people’s lives. In a recent project we looked at, a contractor working on the logged-out view of a particular web application state left in personally identifying information that could be used to unmask users in ways at risk to be that could be abused into pinpointing places and times they had attended particular events. The contractor satisfied the requirements they were given, the test suite didn’t check this, QA didn’t, and a higher level-commitment to recognizing the failure modes that mishandling the unique data this company held had never been made.&lt;/p&gt;
&lt;p&gt;Building in ethical awareness from the ground up is an asset, not a liability. If we go about developing our technology ethically, we are innovating for long-term sustainability, not short-term profit. Whereas, if we “move fast and break things”, without ethical considerations, then it is only a matter of time before consequences arise that are difficult or impossible to unwind. When we break things, people get hurt. There are financial, social, legal, and moral costs involved. Technical innovation needs ethics (and technical advancement likewise allows use to make ethical advances), because at the end of the day, we as humans try to do the right thing, and expect our technology to do the same. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry></feed>