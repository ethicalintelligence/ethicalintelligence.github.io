<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ethical Intelligence - news</title><link href="/" rel="alternate"></link><link href="/feeds/news.atom.xml" rel="self"></link><id>/</id><updated>2020-02-25T09:20:00+01:00</updated><entry><title>Weekly AI Ethics News Roundup: Feb 17 - Feb 25</title><link href="/ai-ethics-news-roundup-feb17-feb25-2020.html" rel="alternate"></link><published>2020-02-25T09:20:00+01:00</published><updated>2020-02-25T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-25:/ai-ethics-news-roundup-feb17-feb25-2020.html</id><summary type="html">&lt;p&gt;@HMRoff on deceptive AI, @dryellowbean &amp;amp; @jud1ths1mon on ethics, @vdignum, @MullerCatelijne and @RecklessCoding on the EU AI Ethics Whitepaper, @Klonick provokes on the definition of AI, new AI ethics map from @aiethicslab&lt;/p&gt;</summary><content type="html">&lt;h2&gt;AI Deception: When Your Artificial Intelligence Learns to Lie&lt;/h2&gt;
&lt;p&gt;"Understanding the breadth of what “AI deception” looks like, and what happens when it is not a human’s intent behind a deceptive AI, but instead the AI agent’s own learned behavior. These may seem somewhat far-off concerns, as AI is still relatively narrow in scope and can be rather stupid in some ways. To have some analogue of an “intent” to deceive would be a large step for today’s systems. However, if we are to get ahead of the curve regarding AI deception, we need to have a robust understanding of all the ways AI could deceive. We require some conceptual framework or spectrum of the kinds of deception an AI agent may learn on its own before we can start proposing technological defenses."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/ai-deception-when-your-ai-learns-to-lie"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;First analysis of the EU Whitepaper on AI&lt;/h2&gt;
&lt;p&gt;"This week, Europe took a clear stance on AI; foster the uptake of AI technologies, underpinned by what it calls ‘an ecosystem of excellence’, while also ensuring their compliance with to European ethical norms, legal requirements and social values, ‘an ecosystem of trust’. While the Whitepaper on AI of the European Commission does not propose legislation yet, it announces some bold legislative measures, that will likely materialize by the end of 2020. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://allai.nl/first-analysis-of-the-eu-whitepaper-on-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics lab releases AI Princples Map&lt;/h2&gt;
&lt;p&gt;"We decided to create the AI Principles Map to help understand the trends, common threads, and differences among numerous sets of principles published."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://aiethicslab.com/big-picture/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Thinking About ‘Ethics’ in the Ethics of AI&lt;/h2&gt;
&lt;p&gt;"One of the fundamental questions in the ethics of AI, therefore, can be formulated as a problem of value alignment: how can we build autonomous AI that is aligned with societally held values."&lt;/p&gt;
&lt;p&gt;"Our review of the theoretical, technical, and ethical challenges to machine ethics does not intend to be exhaustive or conclusive, and these challenges could indeed be overcome in future research and development of autonomous AI. However, we think that these challenges do warrant a pause and reconsideration of the prospects of building ethical AI. In fact, we want to advance a more fundamental critique of machine ethics before exploring another path for answering the value alignment problem."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://revistaidees.cat/en/thinking-about-ethics-in-the-ethics-of-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethical Dimensions of Using Artificial Intelligence in Health Care&lt;/h2&gt;
&lt;p&gt;"An artificially intelligent computer program can now diagnose skin cancer more accurately than a board-certified dermatologist.1 Better yet, the program can do it faster and more efficiently, requiring a training data set rather than a decade of expensive and labor-intensive medical education. While it might appear that it is only a matter of time before physicians are rendered obsolete by this type of technology, a closer look at the role this technology can play in the delivery of health care is warranted to appreciate its current strengths, limitations, and ethical complexities."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://journalofethics.ama-assn.org/article/ethical-dimensions-using-artificial-intelligence-health-care/2019-02"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Would you rather a human or a black box AI perform surgery on you? Would you want the AI to be illegal?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/geoffreyhinton/status/1230592238490615816"&gt;This tweet&lt;/a&gt; sparked a wide ranging discussion this week. Some objected to the framing, arguing that the presumption of superior outcomes from the AI smuggled in a lot of assumptions regarding whom, exactly, and in what cases, might expect to benefit. Others argued that the question of illegality is an orthogonal one, and ought not be considered directly alongside a claim to superior performance. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/geoffreyhinton/status/1230592238490615816"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What Artificial Intelligence Is Not&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence is not one thing." - a short provocative piece from Kate Klonick on the many different thigns that we can mean when we talk about "AI".  &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://blog.lareviewofbooks.org/provocations/artificial-intelligence/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI and the bottom line: 15 examples of artificial intelligence in finance&lt;/h2&gt;
&lt;p&gt;An updated rundown of AI applications in finance. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://builtin.com/artificial-intelligence/ai-finance-banking-applications-companies"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Forensic Architecture Founder Says United States Prevented His Visit Via Algorithm&lt;/h2&gt;
&lt;p&gt;"Eyal Weizman, the director of the investigative group, said an embassy official in London told him an algorithm had identified a security threat that was related to him."&lt;/p&gt;
&lt;p&gt;“Associative algorithms, triangulating algorithms, that look at patterns that look at relations between actions and movement, between people and places,” he said. “We need to gear up to be able identify and monitor those.”&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/02/19/arts/design/forensic-architecture-founder-says-us-denied-him-visa.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 12 - Feb 18</title><link href="/ai-ethics-news-roundup-feb2-feb18-2020.html" rel="alternate"></link><published>2020-02-18T09:20:00+01:00</published><updated>2020-02-18T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-18:/ai-ethics-news-roundup-feb2-feb18-2020.html</id><summary type="html">&lt;p&gt;History of algorithms w/ @RiederB,@silvertjeand,@_mstevenson, @dorotheabaur finds a dilemma for companies ethics-washing facial recognition, @_KarenHao on OpenAI, @chengela and @hannahdev on reading emotion, @poppynoor on medtech, and @emilymbender tweeting from AAAS2020&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Emotion AI researchers say overblown claims give their work a bad name&lt;/h2&gt;
&lt;p&gt;There are no strong, peer-reviewed studie proving that analyzing body posture or facial expressions can help pick the best workers or students (in part because companies are secretive about their methods). As a result, the hype around emotion recognition, which is projected to be a $25 billion market by 2023, has created a backlash from tech ethicists and activists who fear that the technology could raise the same kinds of discrimination problems as predictive sentencing or housing algorithms for landlords deciding whom to rent to.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615232/ai-emotion-recognition-affective-computing-hirevue-regulation-ethics/?utm_source=newsletters&amp;utm_medium=email&amp;utm_campaign=the_algorithm.unpaid.engagement"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why we should hope that corporate claims about the ethics of facial recognition are pure marketing&lt;/h2&gt;
&lt;p&gt;"If we acknowledge the non-commercial, or rather the effectively ‘far-beyond-commercial’ dimensions of facial recognition technology, we in fact severely restrict our abilities to demand accountability for the claims regarding their ‘ethical qualities’."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@dorotheabaur/why-we-should-hope-that-corporate-claims-about-the-ethics-of-facial-recognition-are-pure-marketing-d8b2fea83319"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The historical trajectories of algorithmic techniques: an interview with Bernhard Rieder&lt;/h2&gt;
&lt;p&gt;"In this interview, Michael Stevenson (MS) and Anne Helmond (AH) talk to Bernhard Rieder (BR) about his forthcoming book entitled Engines of Order: A Mechanology of Algorithmic Techniques (University of Amsterdam Press, 2020). In particular, Rieder discusses how the practice of software-making is “constantly faced with the ‘legacies’ of previous work” and how the past continues to operate into present algorithmic techniques."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.tandfonline.com/doi/full/10.1080/24701475.2020.1723345"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;We know ethics should inform AI. But which ethics?&lt;/h2&gt;
&lt;p&gt;The ethical standards for assessing AI and its associated technologies are still in their infancy. Companies need to initiate internal discussion as well as external debate with their key stakeholders about how to avoid being caught up in difficult situations.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.gigabitmagazine.com/ai/we-know-ethics-should-inform-ai-which-ethics?utm_content=117506893&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-2797602696"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Navigating AI’s expanding role in the world of HR&lt;/h2&gt;
&lt;p&gt;As the use of artificial intelligence continues to skyrocket within the HR realm, related ethics issues represent real risk if not handled correctly, and early. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hrexecutive.com/navigating-ais-expanding-role-in-the-world-of-hr/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The messy, secretive reality behind OpenAI’s bid to save the world&lt;/h2&gt;
&lt;p&gt;Karen Hao spent half a year digging into @OpenAI, the SF-based AI research lab, originally founded by @elonmusk. I started with a few simple questions: Who are they? What are their goals? How do they work? After nearly three dozen interviews, I found so much more.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;"AI systems claiming to 'read' emotions pose discrimination risks&lt;/h2&gt;
&lt;p&gt;There are no strong, peer-reviewed studie proving that analyzing body posture or facial expressions can help pick the best workers or students (in part because companies are secretive about their methods). As a result, the hype around emotion recognition, which is projected to be a $25 billion market by 2023, has created a backlash from tech ethicists and activists who fear that the technology could raise the same kinds of discrimination problems as predictive sentencing or housing algorithms for landlords deciding whom to rent to.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/feb/16/ai-systems-claiming-to-read-emotions-pose-discrimination-risks"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Can we trust AI not to further embed racial bias and prejudice?&lt;/h2&gt;
&lt;p&gt;Heralded as an easy fix for health services under pressure, data technology is marching ahead unchecked But is there a risk it could compound inequalities? Poppy Noor investigates&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.bmj.com/content/368/bmj.m363"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Great twitter thread from the Ethics and AI panel panel at American Association for the Advancement of Science 2020&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/emilymbender/status/1228749166622334976"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 5 - Feb 11</title><link href="/ai-ethics-news-roundup-feb5-feb11-2020.html" rel="alternate"></link><published>2020-02-11T09:20:00+01:00</published><updated>2020-02-11T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-11:/ai-ethics-news-roundup-feb5-feb11-2020.html</id><summary type="html">&lt;p&gt;@alexsablay and @hjegou on tracking training data, @dorotheabaur on facial recognition, @lindakinstler on tech ethicists, @benzevenbergen and @drzimmermann on ethical pitfalls, SyRI from @SaschaSchendel, @datainnovation 2019 review, @PublicStandards report on AI&lt;/p&gt;</summary><content type="html">&lt;h2&gt;European parliament says it will not use facial recognition tech&lt;/h2&gt;
&lt;p&gt;"The European parliament has insisted it has no plans to introduce facial recognition technology after a leaked internal memo discussing its use in security provoked an outcry."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/feb/05/european-parliament-insists-it-will-not-use-facial-recognition-tech"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The SyRI case: a landmark ruling for benefits claimants around the world&lt;/h2&gt;
&lt;p&gt;"In NJCM cs/ De Staat der Nederlanden (NJCM vs the Netherlands), also known as the SyRI case, the court considered the legality of the System Risk Indication (SyRI), a system designed by the Dutch government to process large amounts of data collected by various Dutch public authorities to identify those most likely to commit benefits fraud."&lt;/p&gt;
&lt;p&gt;There's a &lt;a href="https://twitter.com/SaschaSchendel/status/1225094322510536705"&gt;great twitter thread on SyRI too&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://privacyinternational.org/news-analysis/3363/syri-case-landmark-ruling-benefits-claimants-around-world?PageSpeed=noscript"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Using ‘radioactive data’ to detect if a data set was used for training&lt;/h2&gt;
&lt;p&gt;"We have developed a new technique to mark the images in a data set so that researchers can determine whether a particular machine learning model has been trained using those images. This can help researchers and engineers to keep track of which data set was used to train a model so they can better understand how various data sets affect the performance of different neural networks."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/2002.00937"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Opposing facial recognition — why focusing on accuracy misses the point&lt;/h2&gt;
&lt;p&gt;"These are important and very worrying findings that emphasize the overall problem with algorithmic discrimination. But what does it imply when we argue against facial technology based on empirical arguments such as overall low accuracy and particular bias with regards to certain ethnic groups? In the worst case, proponents of facial recognition could use this argument to their advantage and take it to underline the need to train the technology better to make sure it improves its accuracy — as apparently done by Google last year. Better training means collecting more data. And collecting more data often means intruding into people’s privacy. We might open the floodgates for even larger scale data collection."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@dorotheabaur/opposing-facial-recognition-why-focusing-on-accuracy-misses-the-point-9b96ea3f864b"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;An Algorithm That Grants Freedom, or Takes It Away&lt;/h2&gt;
&lt;p&gt;"Across the United States and Europe, software is making probation decisions and predicting whether teens will commit crime. Opponents want more human oversight."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/02/06/technology/predictive-algorithms-crime.html?referringSource=articleShare
"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethicists were hired to save tech’s soul. Will anyone let them?&lt;/h2&gt;
&lt;p&gt;"While some tech firms have taken concrete steps to insert ethical thinking into their processes, Catherine Miller, interim CEO of the ethical consultancy Doteveryone, says there's also been a lot of "flapping round" the subject."&lt;/p&gt;
&lt;p&gt;"Critics dismiss it as "ethics-washing," the practice of merely kowtowing in the direction of moral values in order to stave off government regulation and media criticism. The term belongs to the growing lexicon around technology ethics, or "tethics," an abbreviation that began as satire on the TV show "Silicon Valley," but has since crossed over into occasionally earnest usage. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.protocol.com/ethics-silicon-valley"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society&lt;/h2&gt;
&lt;p&gt;"As AI (and associated AI-hype) grows more pervasive in our lives, its impact on society is ever more significant, raising ethical concerns and challenges regarding issues such as privacy, safety and security, surveillance, inequality, data handling and bias, personal agency, power relations, effective modes of regulation, accountability, sanctions, and workforce displacement. Only a multidisciplinary effort can find the best ways to address these concerns, including experts from various disciplines, such as ethics, philosophy, economics, sociology, psychology, law, history, politics, interaction design, informatics, social studies of science and technology, communication and media studies, and political science, as well as those with lived experience in relation to the impacts of AI systems. In order to address these issues in a scientific context, AAAI and ACM joined forces in 2018 to start the AAAI/ACM Conference on AI, Ethics, and Society. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/proceedings/10.1145/3375627#sec1"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics: Seven Traps&lt;/h2&gt;
&lt;p&gt;"The question of how to ensure that technological innovation in machine learning and artificial intelligence leads to ethically desirable—or, more minimally, ethically defensible—impacts on society has generated much public debate in recent years. Most of these discussions have been accompanied by a strong sense of urgency: as more and more studies about algorithmic bias have shown, the risk that emerging technologies will not only reflect, but also exacerbate structural injustice in society is significant."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://freedom-to-tinker.com/2019/03/25/ai-ethics-seven-traps/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence and Public Standards: Committee publishes report&lt;/h2&gt;
&lt;p&gt;“Our message to government is that the UK’s regulatory and governance framework for AI in the public sector remains a work in progress and deficiencies are notable. The work of the Office for AI, the Alan Turing Institute, the Centre for Data Ethics and Innovation (CDEI), and the Information Commissioner’s Office (ICO) are all commendable. But on transparency and data bias in particular, there is an urgent need for practical guidance and enforceable regulation."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.gov.uk/government/news/artificial-intelligence-and-public-standards-committee-publishes-report"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Most Significant AI Policy Developments in the United States in 2019&lt;/h2&gt;
&lt;p&gt;"2019 was a monumental year for artificial intelligence (AI) policy in the United States. The federal government took several important steps that prioritized AI development and deployment and positioned the United States to strengthen its global AI leadership, beginning with President Trump’s “Executive Order on Maintaining American Leadership in Artificial Intelligence,” which set the tone for the rest of the year."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/02/the-most-significant-ai-policy-developments-in-the-united-states-in-2019/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 28 - Feb 4</title><link href="/ai-ethics-news-roundup-jan28-feb4-2020.html" rel="alternate"></link><published>2020-02-04T09:20:00+01:00</published><updated>2020-02-04T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-04:/ai-ethics-news-roundup-jan28-feb4-2020.html</id><summary type="html">&lt;h1&gt;AIEthics news for Jan 28 - Feb 4 featuring health care and drug discovery (@exscientialtd and a great twitter thread from @DorotheaBaur), privacy (@BernardMarr), facial recognition (@castrotech @itifdc @mclaughlintech), impact assessment (@Rafael_A_Calvo), comment on EU AI regs from @datainnovation and more!&lt;/h1&gt;</summary><content type="html">&lt;h2&gt;Would you take a drug discovered by artificial intelligence?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/DorotheaBaur/status/1223549933506191363"&gt;@DorotheaBaur sparked an interesting reaction on Twitter&lt;/a&gt;, where in a climate of frequent AI news that raises ethical concerns, she asked if we might agree that this was a good example an applicaiton of AI that was not ethically problematic. &lt;/p&gt;
&lt;p&gt;"The British startup Exscientia claims it has developed the first medication created using artificial intelligence that will be clinically tested on humans. The medication, which is meant to treat obsessive-compulsive disorder, took less than a year from conception to trial-ready capsule. Human trials are set to begin in March, but would you take a drug designed using artificially intelligent software?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.techjuice.pk/exscientia-an-oxford-based-biotech-company-is-going-to-test-the-ai-designed-medicine-on-humans/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence Is Not Ready For The Intricacies Of Radiology&lt;/h2&gt;
&lt;p&gt;"...while much of the theoretical basis for AI in the practice of radiology is extremely exciting, the reality is that the field has not yet fully embraced it. The most significant issue is that the technology simply isn’t ready, as many of the existing systems have not yet been matured to compute and manage larger data sets or work in more general practice and patient settings, and thus, are not able to perform as promised. Other issues exist on the ethical aspects of AI. Given the sheer volume of data required to both train and perfect these systems, as well as the immense data collection that these systems will engage in once fully mainstream, key stakeholders are raising fair concerns and the call for strict ethical standards to be put into place, simultaneous to the technological development of these systems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/saibala/2020/02/03/artificial-intelligence-is-not-ready-for-the-intricacies-of-radiology/#312ac17867eb"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Advancing impact assessment for intelligent systems&lt;/h2&gt;
&lt;p&gt;"We discuss how the EIA provides a partial blueprint for what we call a Human Impact Assessment for Technology (HIAT) and how more recent algorithmic and data protection impact assessment initiatives can contribute. We also discuss how ethical frameworks for such a human impact assessment could draw on recently established AI ethics principles. We argue that this approach will help build trust in an industry facing increasing criticism and scrutiny."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.repository.cam.ac.uk/handle/1810/300852"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What Is A Data Passport: Building Trust, Data Privacy And Security In The Cloud&lt;/h2&gt;
&lt;p&gt;"Data passports allow you to extend the encryption technology that used to be only available on a physical mainframe to cloud computing. Each piece of data in the cloud has a passport assigned to it, and with the passport, you can verify if the data is misused, if the passport is still valid, etc. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/bernardmarr/2020/01/31/what-is-a-data-passport-building-trust-data-privacy-and-security-in-the-cloud/#76c416f75843"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How the EU Should Revise its AI White Paper Before it is Published&lt;/h2&gt;
&lt;p&gt;"The European Commission is planning to release a white paper to support the development and uptake of artificial intelligence (AI). Early drafts of this white paper suggest that the Commission may call for additional AI regulations that would make it more expensive and more difficult for European businesses to use AI systems in many areas of the economy. Given the EU’s desire to be a leader in AI, and to use AI to bolster its global competitiveness, the Commission should avoid heavy-handed rules that would slow adoption of this emerging technology."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/02/how-the-eu-should-revise-its-ai-white-paper-before-it-is-published/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Critics Were Wrong: NIST Data Shows the Best Facial Recognition Algorithms Are Neither Racist Nor Sexist&lt;/h2&gt;
&lt;p&gt;"NIST assessed the false positive and false-negative rates of algorithms using four types of images, including mugshots, application photographs from individuals applying for immigration benefits, visa photographs, and images taken of travelers entering the United States. NIST’s report reveals that: &lt;/p&gt;
&lt;p&gt;a) The most accurate identification algorithms have “undetectable” differences between demographic groups
b) The most accurate verification algorithms have low false positives and false negatives across most demographic groups
c) Algorithms can have different error rates for different demographics but still be highly accurate"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://itif.org/publications/2020/01/27/critics-were-wrong-nist-data-shows-best-facial-recognition-algorithms"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COR-GAN: Correlation-Capturing Convolutional Neural Networks for Generating Synthetic Healthcare Records&lt;/h2&gt;
&lt;p&gt;"In this paper, we propose a novel framework called correlation-capturing Generative Adversarial Network (corGAN), to generate synthetic healthcare records. In corGAN we utilize Convolutional Neural Networks to capture the correlations between adjacent medical features in the data representation space by combining Convolutional Generative Adversarial Networks and Convolutional Autoencoders"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/cor-gan-correlation-capturing-convolutional-neural-networks-for-generating-synthetic-healthcare-records"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;New surveillance AI can tell schools where students are and where they’ve been)&lt;/h2&gt;
&lt;p&gt;"Not all AI being used by schools is facial recognition. That doesn’t mean the tech doesn’t come with privacy risks. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.vox.com/recode/2020/1/25/21080749/surveillance-school-artificial-intelligence-facial-recognition"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Connected cots, talking teddies, and the rise of the algorithmic child&lt;/h2&gt;
&lt;p&gt;"Digital technologies are now a ubiquitous part of our daily lives. And questions remain as to how these technologies are reshaping how we experience the world around us, and how the world around us is being reshaped. One area this is being played out is in the family – in changing the experience of not only childhood, but what constitutes good parenting."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://keyahconsulting.com/connected-cots-talking-teddies-and-the-rise-of-the-algorithmic-child/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 21 - Jan 28</title><link href="/ai-ethics-news-roundup-jan21-jan28-2020.html" rel="alternate"></link><published>2020-01-28T09:20:00+01:00</published><updated>2020-01-28T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-28:/ai-ethics-news-roundup-jan21-jan28-2020.html</id><summary type="html">&lt;p&gt;Latest AIEthics news: AI in health (@jessRmorley @Floridi), ethics-bashing (@Elibietti), value-alignment (@IasonGabriel), bias (@SimonHegelich @SciOrestis @thyjuancarlos @FabienneMarco), explainability from(@MaribelLopez) and more from @LloydDanzig, @alexcengler and @LizzieGibney!&lt;/p&gt;</summary><content type="html">&lt;h2&gt;An ethically mindful approach to AI for health care&lt;/h2&gt;
&lt;p&gt;"Health-care systems worldwide face increasing demand, a rise in chronic disease, and resource constraints. At the same time, the use of digital health technologies in all care settings has led to an expansion of data. These data, if harnessed appropriately, could enable health-care providers to target the causes of ill-health and monitor the effectiveness of preventions and interventions. For this reason, policy makers, politicians, clinical entrepreneurs, and computer and data scientists argue that a key part of health-care solutions will be artificial Intelligence (AI), particularly machine learning."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)32975-7/fulltext"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy&lt;/h2&gt;
&lt;p&gt;"The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, "ethics" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called "ethics washing" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in "ethics bashing." This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/abs/10.1145/3351095.3372860"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence, Values and Alignment&lt;/h2&gt;
&lt;p&gt;"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bias in Word Embeddings&lt;/h2&gt;
&lt;p&gt;"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372843"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Road to Artificial Intelligence: An Ethical Minefield&lt;/h2&gt;
&lt;p&gt;"There are a number of ethical dilemmas woven inextricably into the field of Artificial Intelligence, many of which are often overlooked, even within the engineering community. Even the best intentions are often not enough to guarantee solutions free from unintended or undesired results, as humans can accidentally encode biases into AI engines and malicious actors can exploit flaws in models. In the short-term, accountability and transparency on behalf of tech companies is critical, as is vigilance on behalf of consumers."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.infoq.com/articles/algorithmic-integrity-ethics"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Preparing For AI Ethics And Explainability In 2020&lt;/h2&gt;
&lt;p&gt;"People distrust artificial intelligence and in some ways this makes  sense.  With the desire to create the best performing AI models, many organizations have prioritized complexity over the concepts of explainability and trust. As the world becomes more dependent on algorithms for making a wide range of decisions, technologies and business leaders will be tasked with explaining how a model selected its outcome. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/maribellopez/2020/01/21/preparing-for-ai-ethics-and-explainability-in-2020/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The battle for ethical AI at the world’s biggest machine-learning conference&lt;/h2&gt;
&lt;p&gt;"Bias and the prospect of societal harm increasingly plague artificial-intelligence research — but it’s not clear who should be on the lookout for these problems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/d41586-020-00160-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Ethical Upside to Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;"if correctly designed, AI should clarify and amplify the ethical frameworks that U.S. military leaders already bring to war. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://warontherocks.com/2020/01/the-ethical-upside-to-artificial-intelligence/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The case for AI transparency requirements&lt;/h2&gt;
&lt;p&gt;"As AI technologies quickly and methodically climb out of the uncanny valley, customer service calls, website chatbots, and interactions on social media and in virtual reality may become progressively less evidently artificial."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/research/the-case-for-ai-transparency-requirements/amp/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 15 - Jan 21</title><link href="/ai-ethics-news-roundup-jan21-2020.html" rel="alternate"></link><published>2020-01-21T09:20:00+01:00</published><updated>2020-01-21T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-21:/ai-ethics-news-roundup-jan21-2020.html</id><summary type="html">&lt;p&gt;Our weekly news roundup for Jan 15 - Jan 21: Including AI in Finance, political campaigns, social profiling, the workplace, the problem of identifying health data, and some pieces on explainability and governance.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Our weekly news roundup for Jan 15 - Jan 21: Including AI in Finance, political campaigns, social profiling, the workplace, the problem of identifying health data, and some pieces on explainability and governance. &lt;/p&gt;
&lt;h2&gt;Everything We Do Tells Something About Our Health - Including Our Taste in Music&lt;/h2&gt;
&lt;p&gt;Much of our data exhaust is rich enough that it can be used to generate accurate inferences about us in domains unrelated to context of collection. Machine learning analysis of large sets of data about our music listening habits can reveal information about our physical and mental health. This information, if directly present, would be subject to regulatory and ethical norms. &lt;/p&gt;
&lt;p&gt;"A crucial question is: what exactly is health data? Does it comprise the data from medical treatment and investigation, or is it more than that? In his research, Hooghiemstra refers to medical investigations, also mentioning wearables and apps. But where, precisely, is the line between what is and what is not health data?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dataethics.eu/health-data-can-include-your-taste-in-music/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI – Enabled Innovation, Part 1: Regulatory Intervention &amp;amp; AI in Financial Services&lt;/h2&gt;
&lt;p&gt;The first of a two-part series on AI in Finance: &lt;/p&gt;
&lt;p&gt;"Financial services regulators are promoting principles and giving guidance through public statements to set the expectation that firms need to ensure their governance model is fit for purpose when applied to AI enabled innovation. In time, these regulators will become more proactive in asking firms to demonstrate they fully understand their data assets and to explain how that data is exploited and how the associated risk is mitigated when using AI – enabled technologies. Financial services firms should develop a coherent AI strategy now in a way that anticipates how they will answer that question when it inevitably comes."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyslegaledge.com/2020/01/ai-enabled-innovation-part-1-regulatory-intervention-ai-in-financial-services/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Towards ethical and socio-legal governance in AI&lt;/h2&gt;
&lt;p&gt;"Many high-level ethics guidelines for AI have been produced in the past few years. It is time to work towards concrete policies within the context of existing moral, legal and cultural values, say Andreas Theodorou and Virginia Dignum."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/s42256-019-0136-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial intelligence: DeepMind unlocks secrets of human brain using AI learning technique&lt;/h2&gt;
&lt;p&gt;Researchers at Google-owned DeepMind discovered that a recent development in computer science regarding reinforcement learning could be applied to how the brain’s dopamine system works.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/artificial-intelligence-deepmind-ai-human-brain-neuroscience-a9286661.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Digital Political Ethics: Aligning Principles with Practice&lt;/h2&gt;
&lt;p&gt;"This report is the fruit of a bipartisan report to identify areas of agreement among key stakeholders concerning ethical principles and best practices in the conduct of digital campaigning in the United States. Although many have raised concerns about the potential for digital technologies to weaken or undermine democracy, the voices of digital political practitioners are largely absent from this discussion." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://citapdigitalpolitics.com/?page_id=1911"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithms at Work: The New Contested Terrain of Control&lt;/h2&gt;
&lt;p&gt;"We find that algorithmic control in theworkplace operates through six main mechanisms, which we call the“6Rs”—employers can use algorithms to direct workers by restrictingand recommending, evaluate workers by recording and rating, and discipline workers by replacing and rewarding. We also discuss several key insights regarding algorithmic control."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://www.angelechristin.com/wp-content/uploads/2020/01/Algorithms-at-Work_Annals.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Black-Boxed Politics:Opacity is a Choice in AI Systems&lt;/h2&gt;
&lt;p&gt;"There are many myths and misconceptions about AI, but in cases where these systems are being used in sensitive, high-risk scenarios such as public health and criminal justice, arguably the most damaging msconception is that these systems are ‘black boxes’ about which we simply cannot know anything."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@szymielewicz/black-boxed-politics-cebc0d5a54ad"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The US just released 10 principles that it hopes will make AI safer&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Booker beware: Airbnb can scan your online life to see if you’re a suitable guest&lt;/h2&gt;
&lt;p&gt;"Details have emerged of its “trait analyser” software built to scour the web to assess users’ “trustworthiness and compatibility” as well as their “behavioural and personality traits” in a bid to forecast suitability to rent a property. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.standard.co.uk/tech/airbnb-software-scan-online-life-suitable-guest-a4325551.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Beyond Bias: Contextualizing “Ethical AI” Within the History of Exploitation and Innovation in Medical Research&lt;/h2&gt;
&lt;p&gt;This is from late December, but it just reached us and is worth a read: "It’s time for us to move beyond “bias” as the anchor point for our efforts to build ethical and fair algorithms."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.media.mit.edu/articles/beyond-bias-contextualizing-ethical-ai-within-the-history-of-exploitation-and-innovation-in-medical-research/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 1 - Jan 15</title><link href="/ai-ethics-news-roundup-jan1-jan15-2020.html" rel="alternate"></link><published>2020-01-15T09:20:00+01:00</published><updated>2020-01-15T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-15:/ai-ethics-news-roundup-jan1-jan15-2020.html</id><summary type="html">&lt;p&gt;Happy New Year! This roundup includes the Artificial Intelligence Video Interview Act, AI in diagnostic systems, Deep Fakes, How to build an ethical career, and more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Happy New Year! &lt;/p&gt;
&lt;h2&gt;The use of AI in job search processes and tools&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.vox.com/recode/2020/1/1/21043000/artificial-intelligence-job-applications-illinios-video-interivew-act"&gt;Artificial Intelligence Video Interview Act&lt;/a&gt; takes effect Jan 1 2020. Many video interview tools now incorporate some kind of AI screening to generate reports on candidates, a practice which raises several AI ethics and safety concerns. &lt;/p&gt;
&lt;p&gt;A new article at the WSJ discusses some of these: &lt;a href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;How Job Interviews Will Transform in the Next Decade&lt;/a&gt;. "Recruiters using AI and virtual-reality simulations may hire based on a candidate’s behaviour, personality traits and physiological responses—no resumes needed"&lt;/p&gt;
&lt;p&gt;And you can already purchase countermeasures, if you can afford it. &lt;a href="https://www.scmp.com/news/asia/east-asia/article/3045795/south-korean-job-applicants-are-learning-trick-ai-hiring-bots"&gt;South Korean job applicants are learning to trick AI hiring bots that use facial recognition tech&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a topic EI has been thinking deeply about for a few months, and we have a detailed blog article in the works ... stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Comparison of the GoogleHealth breast AI paper against the RSNA Editorial Board recommendations for Assessing Radiology Research on Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;Are we holding AI diagnostic tools to the right standards? A great tweet from &lt;a href="http://twitter.com/DrHughHarvey"&gt;@DrHughHarvey&lt;/a&gt;. In the replies there's another good piece from October &lt;a href="https://www.nature.com/articles/s41598-019-51503-3"&gt;Using artificial intelligence to read chest radiographs for tuberculosis detection: A multi-site evaluation of the diagnostic accuracy of three deep learning systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/DrHughHarvey/status/1213548573071204352"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Technology Can't Fix Algorithmic Injustice&lt;/h2&gt;
&lt;p&gt;We need greater democratic oversight of AI not just from developers and designers, but from all members of society.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://bostonreview.net/science-nature-politics/annette-zimmermann-elena-di-rosa-hochan-kim-technology-cant-fix-algorithmic"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data and Justice in 2019 — Who can afford big tech, and who can live without it?&lt;/h2&gt;
&lt;p&gt;"What we see is that while you may not have access to the cloud, you can still be tracked and controlled by your government’s AI. This increase in the reach of data and analytics is even more noticeable for those who don’t have a country to call home. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://globaldatajustice.org/2020-01-01-data-and-justice-2019/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Deepfakes: The Looming Threat Of 2020&lt;/h2&gt;
&lt;p&gt;Deepfakes have been lurking on the internet for years now. But in 2020 the AI technology will become a powerful weapon for misinformation, fraud, and other crimes.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.designnews.com/artificial-intelligence/deepfakes-looming-threat-2020/109800999062105"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How will we remain USEFUL HUMANS? A longer post on the future of work, jobs, education and training&lt;/h2&gt;
&lt;p&gt;"As human intelligence (HI) encounters AI, will humans really become useless? Will all this progress be heaven (working only four hours per day, four days a week, but for the same money), or will it be hell (50% unemployment, rampant inequality and global civil unrest)? Or will it be both i.e. a kind of #hellven? Let’s have a look!"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thefuturesagency.com/2020/01/03/how-will-we-remain-useful-humans-a-longer-post-on-the-future-of-work-jobs-education-and-training/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Building an Ethical Career&lt;/h2&gt;
&lt;p&gt;Not an AI ethics piece, but some interesting reflections on how to build ethical awareness into our professional development. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hbr.org/2020/01/building-an-ethical-career"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The US just released 10 principles that it hopes will make AI safer&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Human-like robots spark fear in users according to researchers&lt;/h2&gt;
&lt;p&gt;"Japanese researcher Masahiro Mori’s “uncanny valley” theory, which he developed in the 1970s, states that we react positively to robots if they have physical features familiar to us -but they disturb us if they start looking too much like us."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.techspark.co/blog/2020/01/02/human-like-robots-spark-fear-in-users-according-to-researchers/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 17-24</title><link href="/ai-ethics-news-roundup-dec17-dec24.html" rel="alternate"></link><published>2019-12-24T09:20:00+01:00</published><updated>2019-12-24T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-24:/ai-ethics-news-roundup-dec17-dec24.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 17 - Dec 24&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;A great tweet from Arvind Narayanan&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/random_walker"&gt;Arvind Narayanan&lt;/a&gt; writes:&lt;/p&gt;
&lt;p&gt;"If you think there's too much yelling about algorithmic bias, here's an analogy. By the mid 90s the privacy community knew there was a huge problem. But it took two decades of yelling and a million privacy disasters before the public and policy makers started taking it seriously."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/random_walker/status/1208050796476215296"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Persons yet Unknown: Animals, Chimeras, Artificial Intelligence and Beyond&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/grok_"&gt;Kate Darling&lt;/a&gt; and &lt;a href="https://twitter.com/PKathrani"&gt;Paresh Kathrani&lt;/a&gt; in an fascinating discussion of robotics and AI and The Animal Law Conference. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.youtube.com/watch?v=dEFI05Gtalc"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Implementing Ethics Into Artificial Intelligence: A Contribution, From A Legal Perspective, To The Development Of An Ai Governance Regime&lt;/h2&gt;
&lt;p&gt;This is a new article added to a great issue of the Duke Law and Technology Review that came out in August, a &lt;a href="https://scholarship.law.duke.edu/dltr/vol18/iss1/"&gt;Symposium for John Perry Barlow&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;"This Article advocates for the need to conduct in-depth risk-benefit-assessments with regard to the use of AI and autonomous systems. This Article points out major concerns in relation to AI and autonomous systems such as likely job losses, causation of damages, lack of transparency, increasing loss of humanity in social relationships, loss of privacy and personal autonomy, potential information biases and the error proneness, and susceptibility to manipulation of AI and autonomous systems. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://scholarship.law.duke.edu/dltr/vol18/iss1/17/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Researchers were about to solve AI’s black box problem, then the lawyers got involved&lt;/h2&gt;
&lt;p&gt;When things go wrong and AI runs amok, the lawyers will be there to tell us the most company-friendly version of what happened. Most importantly, they’ll protect companies from having to share how their AI systems work.&lt;/p&gt;
&lt;p&gt;We’re trading a technical black box for a legal one. Somehow, this seems even more unfair.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thenextweb.com/artificial-intelligence/2019/12/17/researchers-were-about-to-solve-ais-black-box-problem-then-the-lawyers-got-involved/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Explainability and Adversarial Robustness for RNNs&lt;/h2&gt;
&lt;p&gt;"Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a common notion of adversarial robustness, and show that an adversarial training procedure can significantly and successfully reduce the attack surface."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/explainability-and-adversarial-robustness-for-rnns"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;This AI researcher is trying to ward off a reproducibility crisis&lt;/h2&gt;
&lt;p&gt;Joelle Pineau doesn’t want science’s reproducibility crisis to come to artificial intelligence (AI).&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/d41586-019-03895-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics In AI: Why Values For Data Matter&lt;/h2&gt;
&lt;p&gt;An argument for better corporate governance around AI and data. Corporations should "treat data as an asset .... the same way organizations treat inventory, fleet, and manufacturing assets. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/sap/2019/12/18/ethics-in-ai/#2d7dd5285af4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Virtuous Circle of Trusted AI: Turning Ethical and Transparent AI Into a Competitive Advantage&lt;/h2&gt;
&lt;p&gt;"Most large organizations today across the United States and Europe are talking about “duty of care” and AI (i.e. the duty to take care to refrain from causing another person injury or loss). We also hear a lot about the need for clear normative frameworks in areas such as driverless cars, drones, facial recognition, and algorithmic decisionmaking guidelines in public-facing services such as banking or retail. I shall be surprised if we will have this conversation again in two years’ time and legislation hasn’t already been seriously discussed or put in place."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.capgemini.com/research/the-virtuous-circle-of-trusted-ai-turning-ethical-and-transparent-ai-into-a-competitive-advantage-luciano-floridi/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 10-17</title><link href="/ai-ethics-news-roundup-dec10-dec17.html" rel="alternate"></link><published>2019-12-17T09:20:00+01:00</published><updated>2019-12-17T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-17:/ai-ethics-news-roundup-dec10-dec17.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 10 - Dec 17&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Index 2019 Report&lt;/h2&gt;
&lt;p&gt;An independent initiative within Stanford University’s Human-Centered Artificial Intelligence Institute, the report is in its third year and is the result of a collaborative effort led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry, in collaboration with more than 35 sponsoring partners and data contributors. The purpose of the project is to ground the discussion on AI in data, serving practitioners, industry leaders, policymakers and funders, the general public and the media that informs it. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hai.stanford.edu/news/introducing-ai-index-2019-report"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;When should we decline to write code? A small case study.&lt;/h2&gt;
&lt;p&gt;We picked this up on twitter when Emily Bender &lt;a href="https://twitter.com/ethicalai_co/status/1202638293269176321"&gt;tweeted&lt;/a&gt; that there was a task in an AI competition to create an AI that would solve problems involving the  "Prediction of Intellectual Ability and Personality Traits from Text". She's since &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;posted a thoughtful followup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an important problem. Technical and regulatory solutions should be augmented by professional codes of conduct and ethics if we want to ensure the safe and fair development of AI. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Do You Trust Jeff Bezos With Your Life? Tech Giants Like Amazon Are Getting into the Health Care Business&lt;/h2&gt;
&lt;p&gt;Would you trust the Tech Giants with your health data in exchange for more personalized and on-demand healthcare? This article covers the current initiative of telehealth by Amazon and dives into a few key implications that this new commodity would carry for society at large.&lt;/p&gt;
&lt;p&gt;"What health insurance companies, as well as employers who foot the bulk of the U.S.'s health care bill, especially fear from telehealth is that it's so easy to use that people will reach out more often for care. "It creates the risk that every little ache and pain results in a claim that has to be paid out," says the University of Pennsylvania's Asch. "Making people come into the office is health care rationing by inconvenience."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.newsweek.com/amazon-health-care-jeff-bezos-telemedicine-1475154"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A tug-of-war over biased AI&lt;/h2&gt;
&lt;p&gt;"A critical split divides AI reformers. On one side are the bias-fixers, who believe the systems can be purged of prejudice with a bit more math. (Big Tech is largely in this camp.) On the other side are the bias-blockers, who argue that AI has no place at all in some high-stakes decisions."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.axios.com/ai-bias-c7bf3397-a870-4152-9395-83b6bf1e6a67.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Emotion-detecting tech should be restricted by law&lt;/h2&gt;
&lt;p&gt;"The AI Now Institute says the field is "built on markedly shaky foundations".
Despite this, systems are on sale to help vet job seekers, test criminal suspects for signs of deception, and set insurance prices. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.bbc.co.uk/news/technology-50761116"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Would you let a Robot Take Care of Your Mother?&lt;/h2&gt;
&lt;p&gt;AI usage for social care is not a new concept. However, as it becomes more and more of a reality, we are forced to shift our questions from theoretical to personal.&lt;/p&gt;
&lt;p&gt;"Some worry robot care would carry a stigma:the potential of being seen as “not worth human company,” said one participant in a study of potential users with mild cognitive impairments."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2019/12/13/opinion/robot-caregiver-aging.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Owning Intelligence&lt;/h2&gt;
&lt;p&gt;The United States Patent and Trademark Office is trying to answer a very complicated question: who owns artificial intelligence?&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cigionline.org/articles/owning-intelligence"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics for Systemic Issues: A Structural Approach&lt;/h2&gt;
&lt;p&gt;"This paper calls for a "structural" approach to assessing AI’s effects inorder to understand and prevent such systemic risks where no individual can beheld accountable for the broader negative impacts. This is particularly relevantfor AI applied to systemic issues such as climate change and food security whichrequire political solutions and global cooperation. To properly address the widerange of AI risks and ensure ’AI for social good’, agency-focused policies must becomplemented by policies informed by a structural approach."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/48_aisg_neurips2019.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 2 - Dec 9</title><link href="/ai-ethics-news-roundup-dec-9.html" rel="alternate"></link><published>2019-12-09T09:20:00+01:00</published><updated>2019-12-09T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-09:/ai-ethics-news-roundup-dec-9.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 2 - Dec 9&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;On the Legal Compatibility of Fairness Definitions&lt;/h2&gt;
&lt;p&gt;Although the article was on arxiv last week, the author publicized it this week, and it's a good one. There's poor alignment between operationalized definitions of fairness in machine learning and the legal definitions that may in fact apply to the deployment of these systems. &lt;/p&gt;
&lt;p&gt;"Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex socio-technical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts, as well as what both the legal and ML fairness communities can learn from these tensions. We focus this paper on U.S. anti-discrimination law since the ML fairness research community regularly references terms from this body of law."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/1912.00761"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;'Good' isn’t good enough&lt;/h2&gt;
&lt;p&gt;A critical reflection on the problems that arise when the pursuit of good is taken on as a technical objective too hastily, and why sustained and rigorous ethical reflection is a necessary if we want to have any confidence that such efforts will actually succeed. &lt;/p&gt;
&lt;p&gt;"Despite widespread enthusiasm among computer scientists to contribute to “socialgood,” the field’s efforts to promote good lack a rigorous foundation in politicsor social change. There is limited discourse regarding what “good” actuallyentails, and instead a reliance on vague notions of what aspects of society aregood or bad. Moreover, the field rarely considers the types of social changethat result from algorithmic interventions, instead following a “greedy algorithm”approach of pursuing &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.benzevgreen.com/wp-content/uploads/2019/11/19-ai4sg.pdf"&gt;Read More&lt;/a&gt;technology-centric incremental reform at all points."&lt;/p&gt;
&lt;h2&gt;New guidelines on the GDPR Right to Explanation&lt;/h2&gt;
&lt;p&gt;The UK’s Data Protection Authority just issued much-anticipated guidance that clarifies the complicated issue of the GDPR’s ‘right to explanation’. Here is some background on the issue and what the new &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/arthur-ai/uk-gpdr-watchdog-says-explain-your-ai-373ef76d3c"&gt;Read More&lt;/a&gt;information means.&lt;/p&gt;
&lt;h2&gt;Ethics is Objective&lt;/h2&gt;
&lt;p&gt;Here are three arguments for the idea that ethics is subjective, presented with thoughtful rebuttals. This is a theme we took up in our last bog post, where we argued that there is a very large chunk of territory in tech ethics where ethical imperatives can be uncovered and agreed upon by sincere inquiry, &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.linkedin.com/pulse/ethics-objective-reid-blackman-ph-d-/?trackingId=B5NT8Dd1BMx16FH15vHvZQ%3D%3D"&gt;Read More&lt;/a&gt;even by those who disagree on more fundamental ethical and moral questions. &lt;/p&gt;
&lt;h2&gt;Online information of vaccines: information quality is an ethical responsibility of search engines&lt;/h2&gt;
&lt;p&gt;When health-related disinformation is available online, who is responsible? There's a growing backlash against the idea of platforms as "mere tools", but perhaps we should think the same of search engines. We don't usually think that a library is responsible for dangerous information in its books, but should we 
think differently about Google?&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.academia.edu/41168605/Online_information_of_vaccines_information_quality_is_an_ethical_responsibility_of_search_engines"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?&lt;/h2&gt;
&lt;p&gt;Multiple fairness constraints have been proposed in the literature, motivated by a range of concerns about how demographic groups might be treated unfairly by machine learning classifiers. In this work we consider a different motivation; learning from biased training data. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/recovering-from-biased-data-can-fairness-constraints-improve-accuracy"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Datafication&lt;/h2&gt;
&lt;p&gt;In the course of researching and discussing AI ethics challenges, we might run across the claim while the rate and scope of our generation of data has increased, it can be understood on a continuum with the ways in which human activity have always left traces and records. This article on the concept of "datafication" argues against this, and shows several ways to understand what is distinctive about the new systems and actors that collect and use our data. &lt;/p&gt;
&lt;p&gt;"Datafication is not just the making of information, which, in one sense, human beings have been doing since the creation of symbols and writing. Rather, datafication is a contemporary phenomenon which refers to the quantification of human life through digital information, very often for economic value. This process has major social consequences. Disciplines such as political economy, critical data studies, software studies, legal theory, and—more recently— decolonial theory, have considered different aspects of those consequences to be important. Fundamental to all such approaches is the analysis of the &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://policyreview.info/concepts/datafication"&gt;Read More&lt;/a&gt;intersection of power and knowledge. "&lt;/p&gt;
&lt;h2&gt;Amazon ready to cash in on free access to NHS data&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=https://www.thetimes.co.uk/article/amazon-ready-to-cash-in-on-free-access-to-nhs-data-bbzp52n5m""&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Nov 24 - Dec 1</title><link href="/ai-ethics-news-roundup-nov-24-dec-1.html" rel="alternate"></link><published>2019-12-02T09:20:00+01:00</published><updated>2019-12-02T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-02:/ai-ethics-news-roundup-nov-24-dec-1.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Nov 24 - Dec 1&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to our first EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;The Second Wave of Algorithmic Accountability&lt;/h2&gt;
&lt;p&gt;"... the first wave of algorithmic accountability focuses on improving existing systems, a second wave of research has asked whether they should be used at all—and, if so, who gets to govern them".&lt;/p&gt;
&lt;p&gt;Frank Pasquale argues that we can distinguish the "... first wave of algorithmic accountability research and activism", which has targeted existing systems and helped illuminate urgent ethical concerns in the AI systems already online, from "...an emerging “second wave” of algorithmic accountability has begun to address more structural concerns.". &lt;/p&gt;
&lt;p&gt;"Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology". &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Machine Learning on Encrypted Data Without Decrypting It&lt;/h2&gt;
&lt;p&gt;"Recent breakthroughs in cryptography have made it practical to perform computation on data without ever decrypting it. In our example, the user would send encrypted data (e.g. images) to the cloud API, which would run the machine learning model and then return the encrypted answer. Nowhere was the user data decrypted and in particular the cloud provider does not have access to either the orignal image nor is it able to decrypt the prediction it computed."&lt;/p&gt;
&lt;p&gt;This application of homomorphic encryption systems might mitigate a number of data protection problems, even though it would still be imperative to ensure the data was collected ethically, and that the data itself is free from biases that are not understood and accounted for. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://juliacomputing.com/blog/2019/11/22/encrypted-machine-learning.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Tainted Data Can Teach Algorithms the Wrong Lessons&lt;/h2&gt;
&lt;p&gt;Security problems become ethics problems when vulnerabilities in software systems produce risks in their application that the stakeholders (both users, and the people in the social environment) are unable to understand, assess, and freely and knowingly accept. &lt;/p&gt;
&lt;p&gt;Adversarial input is a particularly powerful way to undermine machine learning systems and to cause them to behave in unexpected and unintended ways. "“Current deep-learning systems are very vulnerable to a variety of attacks, and the rush to deploy the technology in the real world is deeply concerning,” says Cristiano Giuffrida, an assistant professor at VU Amsterdam who studies computer security, and who previously discovered a major flaw with Intel chips affecting millions of computers."&lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://link.springer.com/article/10.1007%2Fs13347-019-00354-x"&gt;recent paper&lt;/a&gt; Luciano Floridi draws our attention to the extension of the practice of ethics dumping, "the export of unethical research practices to countries where there are weaker legal and ethical frameworks" into the digital realm. This is an ethical risk, but it is also a more basic risk to the quality of research. This article argues that there are also security problems with this sort of practice. "... some companies outsource the training of their AI systems, a practice known as machine learning as a service. This makes it far harder to guarantee that an algorithm has been developed securely."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wired.com/story/tainted-data-teach-algorithms-wrong-lessons/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithms, Automation, and News&lt;/h2&gt;
&lt;p&gt;A special issue of the Journal of Digital Journalism has been published on "Algorithms, Automation, and News".&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.tandfonline.com/doi/full/10.1080/21670811.2019.1685395"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Against Ethical AI: Guidelines and Self Interest&lt;/h2&gt;
&lt;p&gt;"In this paper we use the EU guidelines on ethical AI, and the responses to it, as a starting point to discuss the problems with our community's focus on such manifestos, principles, and sets of guidelines. We cover how industry and academia are at times complicit in ‘Ethics Washing’, how developing guidelines carries the risk of diluting our rights in practice, and downplaying the role of our own self interest. We conclude by discussing briefly the role of technical practice in ethics."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.halfwaytothefuture.org/programme/mcmillan-against-ethical-ai-guidelines-and-self-interest"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;5 Q’s for Anne Kao, Senior Technical Fellow at Boeing Research and Technology](https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/)&lt;/h2&gt;
&lt;p&gt;"The Center for Data Innovation spoke with Anne Kao, Senior Technical Fellow at Boeing Research and Technology. Kao discussed how she uses machine learning to analyze maintenance reports and how philosophy influences how she approaches data science."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ukraine denounces Apple for calling Crimea part of Russia in apps](https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O)&lt;/h2&gt;
&lt;p&gt;"Reuters reporters in Moscow who typed the name of the Crimean provincial capital Simferopol into Apple’s Maps and Weather apps on Wednesday saw it displayed as “Simferopol, Crimea, Russia”. Users elsewhere — including in Ukraine’s capital Kiev and in Crimea itself — see locations in Crimea displayed without specifying which country they belong to. "&lt;/p&gt;
&lt;p&gt;One might wonder if the technical specifications in the ticket for the engineering work that was involved discussed the political and ethical implications. It's perhaps difficult to imagine that something of this magnitude wasn't remarked upon, but on the other, so much engineering work in software proceeds as though it happens in at least partial isolation from the downstream social and political environment.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry></feed>