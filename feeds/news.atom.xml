<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ethical Intelligence - news</title><link href="/" rel="alternate"></link><link href="/feeds/news.atom.xml" rel="self"></link><id>/</id><updated>2020-04-28T09:20:00+02:00</updated><entry><title>Weekly AI Ethics News Roundup: Apr 15 - Apr 28</title><link href="/ai-ethics-news-roundup-apr15-apr28-2020.html" rel="alternate"></link><published>2020-04-28T09:20:00+02:00</published><updated>2020-04-28T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-04-28:/ai-ethics-news-roundup-apr15-apr28-2020.html</id><summary type="html">&lt;p&gt;@MCoeckelbergh #covid-19 and the postdigital @strwbilly Google's Medical AI @mjasay human monitoring of AI bias @SandraWachter5 Fairness and AI @frossi_t on IBM's approach to AI ethics, @mediamocracy on AI procurement, @SpencerOverton holding social media companies responsible for discrimination + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;We missed a week, so this is a double issue!&lt;/p&gt;
&lt;h2&gt;The Postdigital in Pandemic Times: a Comment on the Covid-19 Crisis and its Political Epistemologies&lt;/h2&gt;
&lt;p&gt;What does the ‘digital’ and ‘digitalization’ mean today, for education and in general? Instead of jumping to an abstract theoretical discussion in the literature, let me start with something concrete, very concrete unfortunately: the current Covid-19 pandemic that currently disrupts our lives and societies.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007/s42438-020-00119-2"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Identifying Bias in Hospital Length of Stay Algorithm&lt;/h2&gt;
&lt;p&gt;"Recognizing the need to support shorter lengths of stay, Dr. John Fahrenbach, a data scientist at the University of Chicago Medicine (UCM), developed a machine learning model that used clinical characteristics to identify patients most suitable for discharge after 48 hours."&lt;/p&gt;
&lt;p&gt;"After introducing zip codes into the model, however, a team member who reviewed the output raised concerns."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.boozallen.com/c/insight/blog/identifying-bias-in-hospital-length-of-stay-algorithm.html?dysig_tid=d275d4f5c6744ba980c01d3cad5b241a&amp;dsuserid=101300&amp;dsuserchannelid=30185&amp;dsuserchanneltype=CutAndPaste&amp;dspostid=a5e377df-426e-443a-bc75-18db6a67523c"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Checking AI bias is a job for the humans&lt;/h2&gt;
&lt;p&gt;"By pre-processing or post-processing data, or even setting datasets to expire, humans can step in to correct machine learning models"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.infoworld.com/article/3537968/checking-ai-bias-is-a-job-for-the-humans.html#tk.rss_all"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Fairness and AI&lt;/h2&gt;
&lt;p&gt;Sandra Wachter on why fairness cannot be automated.&lt;/p&gt;
&lt;p&gt;"In our paper, we analyzed the case law of the European Court of Justice and abstracted what the court considers to be “fair” based on its case law of nondiscrimination cases over recent decades.&lt;/p&gt;
&lt;p&gt;In a second step, we compared the Court’s notion of fairness to the current technical fairness metrics that have been developed by the computer science community."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/berkman-klein-center/fairness-and-ai-c5596faddd20"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Leveraging AI to Battle This Pandemic — And The Next One&lt;/h2&gt;
&lt;p&gt;"Throughout the pandemic, great emphasis has been placed on the sharing (or lack of it) of critical information across countries — in particular from China — about the spread of the disease.  By contrast, relatively little has been said about how Covid-19 could have been better managed by leveraging the advanced data technologies that have transformed businesses over the past 20 years. In this article we discuss one way that governments could  leverage those technologies in managing a future pandemic — and perhaps even the closing phases of the current one."&lt;/p&gt;
&lt;p&gt;"Implementing the technological innovations, however, will require policy changes. Existing policies covering data privacy and cybersecurity, and their respective and differing interpretations across countries, will largely prohibit the kind of personalized pandemic management approach we are advocating."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hbr.org/2020/04/leveraging-ai-to-battle-this-pandemic-and-the-next-one"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Google’s medical AI was super accurate in a lab. Real life was a different story.&lt;/h2&gt;
&lt;p&gt;"If AI is really going to make a difference to patients we need to know how it works when real humans get their hands on it, in real situations."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/?itm_source=parsely-api"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Israeli Doctors Warn Shin Bet Surveillance Actually Hindering Efforts to Combat Coronavirus&lt;/h2&gt;
&lt;p&gt;"Physicians' association claims the security service's use of technology to identify people who have come in contact with carriers of the coronavirus is not providing the right information "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.haaretz.com/israel-news/.premium-israeli-doctors-warn-shin-bet-surveillance-hindering-efforts-to-combat-coronavirus-1.8714359"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Contact-tracing apps are not a solution to the COVID-19 crisis&lt;/h2&gt;
&lt;p&gt;"We are concerned by this rising enthusiasm for automated technology as a centerpiece of infection control. Between us, we hold extensive expertise in technology, law and policy, and epidemiology. We have serious doubts that voluntary, anonymous contact tracing through smartphone apps—as Apple, Google, and faculty at a number of academic institutions all propose—can free Americans of the terrible choice between staying home or risking exposure. We worry that contact-tracing apps will serve as vehicles for abuse and disinformation, while providing a false sense of security to justify reopening local and national economies well before it is safe to do so. Our recommendations are aimed at reducing the harm of a technological intervention that seems increasingly inevitable."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/techstream/inaccurate-and-insecure-why-contact-tracing-apps-could-be-a-disaster/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Advancing AI ethics beyond compliance&lt;/h2&gt;
&lt;p&gt;"Ethical considerations must be elevated in the dialogue about AI systems across the business landscape. The level of cognitive understanding between humans and machines is inherently lower than it is between humans and other humans, yet the latter arena has been structured for centuries around ethics. Since AI relies on huge computing power, it can derive insight from massive amounts of data that would challenge human cognition. Relying only on traditional ethical approaches to decision making may be insufficient in addressing AI-powered decisions."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.ibm.com/thought-leadership/institute-business-value/report/ai-ethics"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How ‘Bias Bounties’ May Put Ethics Principles Into Practice&lt;/h2&gt;
&lt;p&gt;"The recently published paper suggests ten different approaches to turn AI ethics principles into practice. Taking a look at the recent efforts, more than 80 organisations have come up with different AI ethics principles. However, the authors of the paper firmly believe that the present set of norms and regulations is insufficient to develop a responsible AI. The team has also advised on ‘red-teaming’ to detect susceptibility, along with aligning with third-party auditing and government policies to create new regulations specific to market needs."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://analyticsindiamag.com/how-bias-bounties-may-put-ethics-principles-into-practice/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;White Paper on Data Ethics in Public Procurement&lt;/h2&gt;
&lt;p&gt;"The present white paper covers one pivotal general area of AI adoption in Europe in which a standardized general European approach is of key importance to the adoption of AI that truly respects fundamental rights and values in its governance structures, management systems, and technical, legal and social components."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dataethics.eu/publicprocurement/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;State Power to Regulate Social Media Companies to Prevent Voter Suppression&lt;/h2&gt;
&lt;p&gt;"State lawmakers should not be deterred by arguments that Section 230 of the federal Communications Act of 1934 “immunizes” social media companies from State liability. This Essay explains that Section 230 does not limit the power of States to hold social media companies legally responsible for using data collection and algorithms to target protected classes of voters with suppressive ads. By using such techniques, social media companies contribute materially to discrimination and are thus ineligible for Section 230 immunity."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://lawreview.law.ucdavis.edu/issues/53/4/feeney_symposium/53-4_overton.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Research summary: Different Intelligibility for Different Folks&lt;/h2&gt;
&lt;p&gt;"...there is a large problem in the current approach in the sense that there isn’t enough being done to meet the needs of a diverse set of stakeholders who require different kinds of intelligibility that is understandable to them and helps them meet their needs and goals. One might argue that a deeply technical explanation ought to suffice and others kinds of explanations might be derived from that but it makes them inaccessible to those who can’t parse well the technical details, often those who are the most impacted by such systems. This paper by Yishan Zhou and David Danks offers a framework to situate the different kinds of explanations such that they are able to meet the stakeholders where they are at and provide explanations that not only help them meet their needs but ultimately engender a higher level of trust from them by highlighting better both the capabilities and limitations of the systems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://montrealethics.ai/research-summary-different-intelligibility-for-different-folks/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Aligning AI to Human Values means Picking the Right Metrics&lt;/h2&gt;
&lt;p&gt;"How can we evaluate whether any particular product is ultimately producing positive outcomes for people and society? In principle we could create metrics that capture important aspects of the effect of an AI system on human lives, just as cities and countries today record a large variety of statistical indicators. These metrics would be useful to the teams building and operating the system, to researchers who want to understand what the system is doing, and as a transparency and accountability tool."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@PartnershipAI/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics at a Distance&lt;/h2&gt;
&lt;p&gt;We may feel individually powerless to contribute to social transformation. But each of us bears responsibility for helping to create a more just world.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://bostonreview.net/philosophy-religion/vafa-ghazavi-ethics-distance"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: April 7-April 15</title><link href="/ai-ethics-news-roundup-april7-april15-2020.html" rel="alternate"></link><published>2020-04-15T09:20:00+02:00</published><updated>2020-04-15T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-04-15:/ai-ethics-news-roundup-april7-april15-2020.html</id><summary type="html">&lt;p&gt;A mini-roundup on digital-contact-tracing and #COVID19, @j2bryson AI and human agency @craigss #machinelearning and sports @Floridi/@JoshCowls/@RosariaTaddeo on AI4SG @AlexCEngler on #AI and #COVID19 hype @hackylawyER on selling your data + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;How to Design AI for Social Good: Seven Essential Factors&lt;/h2&gt;
&lt;p&gt;"The idea of artificial intelligence for social good (henceforth AI4SG) is gaining traction within information societies in general and the AI community in particular. It has the potential to tackle social problems through the development of AI-based solutions. Yet, to date, there is only limited understanding of what makes AI socially good in theory, what counts as AI4SG in practice, and how to reproduce its initial successes in terms of policies. This article addresses this gap by identifying seven ethical factors that are essential for future AI4SG initiatives. The analysis is supported by 27 case examples of AI4SG projects. Some of these factors are almost entirely novel to AI, while the significance of other factors is heightened by the use of AI. From each of these factors, corresponding best practices are formulated which, subject to context and balance, may serve as preliminary guidelines to ensure that well-designed AI is more likely to serve the social good."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007%2Fs11948-020-00213-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A guide to healthy skepticism of artificial intelligence and coronavirus&lt;/h2&gt;
&lt;p&gt;"The COVID-19 outbreak has spurred considerable news coverage about the ways artificial intelligence (AI) can combat the pandemic’s spread. Unfortunately, much of it has failed to be appropriately skeptical about the claims of AI’s value. Like many tools, AI has a role to play, but its effect on the outbreak is probably small. While this may change in the future, technologies like data reporting, telemedicine, and conventional diagnostic tools are currently far more impactful than AI."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/research/a-guide-to-healthy-skepticism-of-artificial-intelligence-and-coronavirus/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethical Implications of the Use of AI to Manage the COVID-19 Outbreak&lt;/h2&gt;
&lt;p&gt;"In the process of managing this pandemic, many artificial intel-ligence (AI) -based tools have been used (or their potential has been discussed) in order to gather and analyze relevant data, develop treatments, make medical decisions, track infected populations and manage quarantines and information dissemi-nation. In this Research Brief, we outline some of the current and potential uses for AI-based tools in managing pandemics and discuss the ethical implications of these efforts."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ieai.mcts.tum.de/wp-content/uploads/2020/04/April-2020-IEAI-Research-Brief_Covid-19-FINAL.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;You Really Don’t Want to Sell Your Data&lt;/h2&gt;
&lt;p&gt;"Proposals that would let people sell their information seem empowering—but they aren’t."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://slate.com/technology/2020/04/sell-your-own-data-bad-idea.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What are people for? Employment and the real existential threat of AI&lt;/h2&gt;
&lt;p&gt;"When people choose to pretend that the machines themselves have agency, they allow the people who programmed them, designed them, sold them, bought them, determined how to use them, operated them, or were supposed to regulate and check them – all these people are let off the hook from having done their jobs appropriately, if we allow anyone to say that the machine is the one that acted"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://joanna-bryson.blogspot.com/2020/04/employment-and-real-existential-threat.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Want to get better at sports? Listen to the Machines&lt;/h2&gt;
&lt;p&gt;Companies are now "...using the pattern-recognizing power of machine learning to revolutionize coaching and make advanced analytics available to teams of all kinds."&lt;/p&gt;
&lt;p&gt;Interesting tidbit: &lt;/p&gt;
&lt;p&gt;"Seattle Sports Sciences uses Labelbox, a training data platform that allows Mr. Milton’s data science team in Seattle to work with shifts of workers in India who label data 24 hours a day. “That’s how fast you have to move to compete in modern vision A.I.,” Mr. Milton said. “It’s basically a labeling arms race"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/04/08/technology/ai-sports-athletes-machine-learning.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Prof. Joanna Bryson interviewed on the ethical challenges of digitalization&lt;/h2&gt;
&lt;p&gt;"...we turn to ethics. What is the trade-off between privacy and security? Can we have a thriving society and innovations without a freedom of speech? Does a digital revolution mean that we also need a revolution in governance? Will artificial general intelligence (AGI) substitute doctors and teachers in the future? Joanna Bryson, Professor of Ethics and Technology at the Hertie School of Governance in Berlin, talks about the ethical challenges we face in the process of digitalization – in the time of coronavirus crisis and beyond. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://anchor.fm/anna-litvinenko/episodes/Prof--Joanna-Bryson-about-ethical-challenges-of-digitalization-ecj6fg"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COVID-19 and Digital Contact Tracing&lt;/h2&gt;
&lt;p&gt;This has been a hot topic in the last few weeks, and we've collected a few particularly interesting pieces here. &lt;/p&gt;
&lt;p&gt;Since the TraceTogether App has been a major inspiration for DCT, this article helps contextualize the contact tracing efforts in South Korea. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ophrp.org/journal/view.php?doi=10.24171/j.phrp.2020.11.1.09"&gt;Contact Transmission of COVID-19 in South Korea: Novel Investigation Techniques for Tracing Contacts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here's a post from the product lead of TraceTogether, that helps give further background on their work:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.gds-gov.tech/automated-contact-tracing-is-not-a-coronavirus-panacea-57fb3ce61d98?gi=775d620650c5"&gt;Automated contact tracing is not a coronavirus panacea&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There's a nice roundup of the privacy concerns with contact tracing here:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.washingtonpost.com/news/powerpost/paloma/the-cybersecurity-202/2020/04/14/the-cybersecurity-202-privacy-experts-fear-a-boom-in-coronavirus-surveillance/5e94901988e0fa101a7615be/"&gt;The Cybersecurity 202: Privacy experts fear a boom in coronavirus surveillance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And also from Merve Hickok:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@MerveHickok/ethical-ai-big-data-in-times-of-pandemic-df10bee77fc8"&gt;Ethical AI &amp;amp; Big Data In Times Of Pandemic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are a core set of privacy concerns with DCT that we might address through technical means, and legal protections. &lt;/p&gt;
&lt;p&gt;Here's some model legislation that illustrates the sort of legal protections we might require:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://osf.io/preprints/lawarxiv/yc6xu/"&gt;The Coronavirus (Safeguards) Bill 2020: Proposed protections for digital interventions and in relation to immunity certificates&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here's a thread from one of the people behind &lt;a href="https://github.com/DP-3T/documents"&gt;Decentralized Privacy-Preserving Proximity Tracing&lt;/a&gt; that helps explain some of the best techincal tools we have available to protect privacy:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/mikarv/status/1246124667355660291"&gt;A bluetooth COVID proximity tracing system that works at scale, where the server learns nothing about individuals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cansu Canca argues that to make privacy-preserving work equitably, it must be mandatory. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@cansucanca/why-mandatory-privacy-preserving-digital-contact-tracing-is-the-ethical-measure-against-covid-19-a0d143b7c3b6"&gt;Why ‘Mandatory Privacy-Preserving Digital Contact Tracing’ is the Ethical Measure against COVID-19&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At the core of many worries about DCT is a concern that whatever temporary concessions we make to help combat COVID-19 will become permanent. &lt;/p&gt;
&lt;p&gt;Jathan Sadowski draws an analogy with 9/11, which spurred emergency measures that reduced freedom from surveillance that remain in effect today.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://reallifemag.com/the-authoritarian-trade-off/"&gt;Exchanging privacy rights for public health is a false compromise&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ryan Calo, in evidence to the US House Committee on Commerce, Science, and Transportation, summarizes some of the reasons why we should be cautious about DCT:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.commerce.senate.gov/2020/4/enlisting-big-data-in-the-fight-against-coronavirus"&gt;Enlisting Big Data in the Fight Against Coronavirus&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This short essay by Dr. Hannah C. McLane reminds us that "If we strictly adhere to ‘save the most lives’ principle, we will be treating more white people, more men, more wealthy people"&lt;/p&gt;
&lt;p&gt;&lt;a href="https://whyy.org/articles/a-disturbing-medical-consensus-is-growing-heres-what-it-could-mean-for-black-patients-with-coronavirus/"&gt;A disturbing medical consensus is growing. Here’s what it could mean for Black patients with coronavirus.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We've got a piece of our own up from &lt;a href="https://twitter.com/andrewbuzzell"&gt;@andrewbuzzell&lt;/a&gt; on using DCT to track COVID-19 &lt;a href="https://ethicalintelligence.co/covid19-dct-wcgw.html"&gt;COVID-19 Digital Contact Tracing - Launch it fast and debug it live. What could go wrong?&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: March 31 - April 7</title><link href="/ai-ethics-news-roundup-apr7-2020.html" rel="alternate"></link><published>2020-04-07T09:20:00+02:00</published><updated>2020-04-07T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-04-07:/ai-ethics-news-roundup-apr7-2020.html</id><summary type="html">&lt;p&gt;@ds_wats0n and @Floridi on interpretable #machinelearning, @christianmunthe on moral agency, @_KarenHao AI predicting life outcomes for children, @gabrielazanfir on data protection, @AFCEA on intelligent weapons, @VincentCMueller with an SEP entry on AI ethics + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;How artificial intelligence and machine learning are used in hiring and recruiting&lt;/h2&gt;
&lt;p&gt;"Job seekers interact more with advancing tech than they realize as more companies turn to automated tools in talent acquisition."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.zdnet.com/article/how-artificial-intelligence-and-machine-learning-are-used-in-hiring-and-recruiting/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI in the headlines: the portrayal of the ethical issues of artificial intelligence in the media&lt;/h2&gt;
&lt;p&gt;"This paper expands upon previous research by systematically analyzing and categorizing the media portrayal of the ethical issues of AI to better understand how media coverage of these issues may shape public debate about AI. Our results suggest that the media has a fairly realistic and practical focus in its coverage of the ethics of AI, but that the coverage is still shallow. A multifaceted approach to handling the social, ethical and policy issues of AI technology is needed, including increasing the accessibility of correct information to the public in the form of fact sheets and ethical value statements on trusted webpages (e.g., government agencies), collaboration and inclusion of ethics and AI experts in both research and public debate, and consistent government policies or regulatory frameworks for AI technology."&lt;/p&gt;
&lt;p&gt;There's also a &lt;a href="https://news.ncsu.edu/2020/04/how-we-write-about-ai-ethics/"&gt;nice article about this piece&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And &lt;a href="https://www.futurity.org/artificial-intelligence-news-coverage-ethics-2326172/"&gt;an interview&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;"A recent analysis of how journalists deal with the ethics of artificial intelligence suggests that reporters are doing a good job of grappling with a complex set of questions—but there’s room for improvement."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007/s00146-020-00965-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI can’t predict how a child’s life will turn out even with a ton of data&lt;/h2&gt;
&lt;p&gt;"Hundreds of researchers attempted to predict children’s and families’ outcomes, using 15 years of data. None were able to do so with meaningful accuracy."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615434/ai-machine-learning-social-outcome-prediction-study/?utm_medium=tr_social&amp;utm_campaign=site_visitor.unpaid.engagement&amp;utm_source=Twitter#Echobox=1586185183"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why data protection law is uniquely equipped to let us fight a pandemic with personal data&lt;/h2&gt;
&lt;p&gt;"Data protection law is different than “privacy”. We, data protection lawyers, have been complacent recently and have failed to clarify this loud and clear for the general public. Perhaps happy to finally see this field of law taking the front stage of public debate through the GDPR, we have not stopped anyone from saying that the GDPR is a privacy law." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://pdpecho.com/2020/04/06/why-data-protection-law-is-uniquely-equipped-to-let-us-fight-a-pandemic-with-personal-data/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;It's Time to Take Seriously the Machine Ethics of Autonomous and AI Cyber Systems&lt;/h2&gt;
&lt;p&gt;"Many concepts revolve around the law of armed conflict, societal law, ethical dilemmas, psychological concepts and artificially intelligent cyber systems, as well as their relationships among each other. In addition to the delineation of machine ethic guidelines, an ethical life cycle is necessary to account for changes over time in national circumstances and personal beliefs. Just recently, the Defense Innovation Board, which serves as an advisory board to the Pentagon, met and published ethical guidelines in designing and implementing artificially intelligent weapons. Artificial intelligence (AI) systems in the Defense Department must satisfy the conditions of responsibility, equitability, traceability, reliability and governability. The Defense Innovation Board approved five ethical principles."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.afcea.org/content/its-time-take-seriously-machine-ethics-autonomous-and-ai-cyber-systems"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics - From Principles to Practice&lt;/h2&gt;
&lt;p&gt;"We have prepared this report as experts in spheres ranging from computer science, philosophy, and technology impact assessment via physics and engineering to social sciences, and we work together as the AI Ethics Impact Group (AIEI Group). Our paper offers concrete guidance to decision-makers in organisations developing and using AI on how to incorporate values into algorithmic decision-making, and how to measure the fulfilment of values using criteria, observables and indicators combined with a context-dependent risk assessment. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.ai-ethics-impact.org/resource/blob/1961130/c6db9894ee73aefa489d6249f5ee2b9f/aieig---report---download-hb-data.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A Normative Approach to Artificial Moral Agency&lt;/h2&gt;
&lt;p&gt;"This paper proposes a methodological redirection of the philosophical debate on artificial moral agency (AMA) in view of increasingly pressing practical needs due to technological development. This “normative approach” suggests abandoning theoretical discussions about what conditions may hold for moral agency and to what extent these may be met by artificial entities such as AI systems and robots. Instead, the debate should focus on how and to what extent such entities should be included in human practices normally assuming moral agency and responsibility of participants"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.researchgate.net/publication/311196481_A_Normative_Approach_to_Artificial_Moral_Agency"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics of Artificial Intelligence and Robotics&lt;/h2&gt;
&lt;p&gt;A new Stanford Encyclopedia of Philosophy entry in on the way!&lt;/p&gt;
&lt;p&gt;"Artificial intelligence (AI) and robotics are digital technologies that will be of major importance for the development of humanity in the near future. They have raised fundamental questions about what we should do with these systems, what the systems themselves should do, what risks they involve and how we can control these. After the Introduction to the field (1), the main themes of this article are: (2) Ethical issues that arise with AI systems as objects, i.e. tools made and used by humans; here, the main sections are privacy and manipulation, opacity and bias, human-robot interaction, employment, and the effects of autonomy. (3) AI systems as subjects, i.e. when ethics is for the AI systems themselves in machine ethics and artificial moral agency. (4) The problem of a possible future AI superintelligence leading to a ‘singularity’. For each section within these themes, we provide a general explanation of the ethical issues, we outline existing positions and arguments, then we analyse how this plays out with current technologies and finally what policy consequences may be drawn"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://philarchive.org/archive/MLLEOA-4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The explanation game: a formal framework for interpretable machine learning&lt;/h2&gt;
&lt;p&gt;"We propose a formal framework for interpretable machine learning. Combining elements from statistical learning, causal interventionism, and decision theory, we design an idealised explanation game in which players collaborate to find the best explanation(s) for a given algorithmic prediction. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007/s11229-020-02629-9?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst&amp;utm_source=ArticleAuthorOnlineFirst&amp;utm_medium=email&amp;utm_content=AA_en_06082018&amp;ArticleAuthorOnlineFirst_20200404"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Mar 24 - Mar 30</title><link href="/ai-ethics-news-roundup-mar24-mar30-2020.html" rel="alternate"></link><published>2020-03-30T09:20:00+02:00</published><updated>2020-03-30T09:20:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-30:/ai-ethics-news-roundup-mar24-mar30-2020.html</id><summary type="html">&lt;p&gt;@SweeLengHarris-data protection, @ShannonVallor-AI and #covid19, @PartnershipAI-responsible disclosure, @SandraWachter5 on fairness, @akapczynski on informational capitalism, @MarcelloIenca-data &amp;amp; covid19&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Coronavirus Pandemic Could Elevate ESG Factors&lt;/h2&gt;
&lt;p&gt;"Environmental, social and governance investing was growing in popularity before the virus began to circulate, as investors flocked to companies that have taken steps to manage nonfinancial risks related to matters such as climate change, board diversity or human rights issues in the supply chain.&lt;/p&gt;
&lt;p&gt;But the pandemic has demonstrated on a large scale the importance of other factors that are paramount to ESG investors. Among them: disaster preparedness, continuity planning and employee treatment through benefits such as paid sick leave as companies direct employees to work from home."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wsj.com/articles/coronavirus-pandemic-could-elevate-esg-factors-11585167518"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI can help with the COVID-19 crisis - but the right human input is key&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence (AI) has the potential to help us tackle the pressing issues raised by the COVID-19 pandemic. It is not the technology itself, though, that will make the difference but rather the knowledge and creativity of the humans who use it. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.weforum.org/agenda/2020/03/covid-19-crisis-artificial-intelligence-creativity/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data Protection Impact Assessments as rule of law governance mechanisms&lt;/h2&gt;
&lt;p&gt;"This article explores how Data Protection Impact Assessments (DPIAs) could provide a mechanism for improved rule of law governance of data processing systems developed and used by government for public purposes in civil and administrative areas. Applying rule of law principles to two case studies provides a sketch of the issues and concerns that this article’s proposals for DPIAs seek to address. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cambridge.org/core/journals/data-and-policy/article/data-protection-impact-assessments-as-rule-of-law-governance-mechanisms/3968B2FBFE796AA4DB0F886D0DBC165D#.XoL9tjyutqQ.twitter"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Shannon Vallor on AI and Covid-19&lt;/h2&gt;
&lt;p&gt;"Thoughts on the growing debate over whether COVID-19 illustrates the moral necessity of using AI and other tech for more expansive and intrusive forms of public health surveillance: a thread"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://threadreaderapp.com/thread/1242194076293890048.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Adversarial Perturbations Fool Deepfake Detectors&lt;/h2&gt;
&lt;p&gt;Hope that the deep fake problem can be solved technologically might be misplaced, instead we might have an arms race which further undermines the digital epistemic environment. &lt;/p&gt;
&lt;p&gt;"This work uses adversarial perturbations to enhance deepfake images and fool common deepfake detectors.The DIP defense achieved 95 perturbed deepfakes that fooled the original detector, while retaining 98 accuracy in other cases on a 100 image subsample."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/adversarial-perturbations-fool-deepfake-detectors"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Publication Norms for Responsible AI&lt;/h2&gt;
&lt;p&gt;Some have argued that dangerous AI technology like deep fake generators should be protected from disclosure to the public. The partnership on AI is working on new standards for publication, and is looking for comments.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.partnershiponai.org/case-study/publication-norms/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI&lt;/h2&gt;
&lt;p&gt;"In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in AI and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Recommendations on privacy and dataprotection in the fight against COVID-19&lt;/h2&gt;
&lt;p&gt;"Governments, companies, NGOs, and individuals alike have a responsibility to do their part tomitigate the consequences of COVID-19 and to show solidarity and respect for each other. Inthis paper, we will provide ​privacy and data protection recommendations forgovernments​ to fight against COVID-19 in a rights-respecting manner."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.accessnow.org/cms/assets/uploads/2020/03/Access-Now-recommendations-on-Covid-and-data-protection-and-privacy.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;‘Trustworthy AI’ is a framework to help manage unique risk&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence (AI) technology continues to advance by leaps and bounds and is quickly becoming a potential disrupter and essential enabler for nearly every company in every industry. At this stage, one of the barriers to widespread AI deployment is no longer the technology itself; rather, it’s a set of challenges that ironically are far more human: ethics, governance, and human values."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615400/trustworthy-ai-is-a-framework-to-help-manage-unique-risk/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Law of Informational Capitalism&lt;/h2&gt;
&lt;p&gt;"I construct an account of the “law of informational capitalism,” with particular attention to the law that undergirds platform power. Once we come to see informational capitalism as contingent upon specific legal choices, we can begin to consider how democratically to reshape it. Though Cohen does not emphasize it, some of the most important legal developments—specifically, developments in the law of takings, commercial speech, and trade—are those that encase private power from democratic revision. Today’s informational capitalism brings a threat not merely to our individual subjectivities but to equality and our ability to self-govern. Questions of data and democracy, not just data and dignity, must be at the core of our concern."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.yalelawjournal.org/review/the-law-of-informational-capitalism"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Vulnerable robots positively shape human conversational dynamics in a human–robot team&lt;/h2&gt;
&lt;p&gt;"In this work, we explore how a social robot influences team engagement using an experimental design where a group of three humans and one robot plays a collaborative game. Our analysis shows that a robot’s social behavior influences the conversational dynamics between human members of the human–robot group, demonstrating the ability of a robot to significantly shape human–human interaction."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.pnas.org/content/117/12/6370"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Empathy Machine: Humans Communicate Better after Robots Show Their Vulnerable Side&lt;/h2&gt;
&lt;p&gt;"“While other work has focused on how to more easily integrate robots into teams, we focused instead on how robots might positively shape the way that people react to each other,” says Sarah Sebo, a graduate student at Yale University and co-author of the research, published this month in Proceedings of the National Academy of Sciences USA. To measure these changes in reactions, researchers at Yale and Cornell University assigned participants to teams of four—consisting of three people and one small humanoid robot—and had them play a collaborative game on Android tablets. In some groups, the robots were programmed to act “vulnerable.” These machines performed actions such as apologizing for making mistakes, admitting to self-doubt, telling jokes, sharing personal stories about their “life,” and talking about how they were “feeling.” In control groups, the human participants teamed up with robots that made only neutral statements or remained entirely silent."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.scientificamerican.com/article/empathy-machine-humans-communicate-better-after-robots-show-their-vulnerable-side/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Federal Court Rules ‘Big Data’ Discrimination Studies Do Not Violate Federal Anti-Hacking Law&lt;/h2&gt;
&lt;p&gt;"The ACLU challenged a provision of the CFAA that the government argues makes it a crime to violate a website’s terms of service. Those terms, which are unilaterally set by individual sites and can change at any time, often prohibit researchers and journalists from creating tester online identities or recording what content is served up to those identities. These practices were used by, for example, investigative journalists who exposed that advertisers were using Facebook’s ad-targeting algorithm to exclude users from receiving job, housing, or credit ads based on race, gender, age, or other classes protected from discrimination in federal and state civil rights laws."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.aclu.org/press-releases/federal-court-rules-big-data-discrimination-studies-do-not-violate-federal-anti"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;On the responsible use of digital data to tackle the COVID-19 pandemic&lt;/h2&gt;
&lt;p&gt;"Large-scale collection of data could help curb the COVID-19 pandemic, but it should not neglect privacy and public trust. Best practices should be identified to maintain responsible data-collection and data-processing standards at a global scale."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/s41591-020-0832-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: March 17 - March 24</title><link href="/ai-ethics-news-roundup-mar17-mar24-2020.html" rel="alternate"></link><published>2020-03-23T09:20:00+01:00</published><updated>2020-03-23T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-23:/ai-ethics-news-roundup-mar17-mar24-2020.html</id><summary type="html">&lt;p&gt;@rachelcoldicutt on the ethics of contact tracking, @StanfordHAI online conference on Covid19 &amp;amp; AI, @carissaveliz on Data, Privacy, and the Individual, @LydNicholas on responsible tech in a pandemic, @lawfare on using data to track covid19, @stevevosloo on child-friendly AI + more&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Open Letter: Contact Tracking and NHSX&lt;/h2&gt;
&lt;p&gt;"During this global emergency, technology and data-driven decisions have a vital role in saving lives by delivering essential information, building communities and managing capacity across the NHS. But they are not a magic bullet to solve unsolvable problems."&lt;/p&gt;
&lt;p&gt;"As responsible technologists, we call upon the NHSX leadership and the Secretary of State for Health and Social Care to ensure new technologies used in the suppression of Coronavirus follow ethical best practice"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@rachelcoldicutt/open-letter-contract-tracking-and-nhsx-e503325b2703"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COVID-19 and AI: A Virtual Conference&lt;/h2&gt;
&lt;p&gt;Topics to be addressed include: AI applications in diagnostics and treatment, epidemiological tracking and forecasting of the spread of the virus, information and disinformation, and the broader human impact of COVID-19 and pandemics in general on economies, culture, government, and human behavior. Through timely, insightful presentations and interactive sessions, this event will serve to unite a global community toward solutions to benefit all of humanity.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hai.stanford.edu/events/covid-19-and-ai-virtual-conference/overview?utm_source=twitter&amp;utm_medium=social&amp;utm_content=UComm_twitter_StanfordHAI_202003221000_sf119533070&amp;utm_campaign=&amp;sf119533070=1"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data, Privacy, and the Individual&lt;/h2&gt;
&lt;p&gt;"The aim of the research project Data, Privacy, and the Individual is to contribute to a better understanding of the ethics of privacy and of differential privacy. The outcomes of the project are seven research papers on privacy, a survey, and this final report, which summarises each research paper, and goes on to offer a set of reflections and recommendations to implement best practices regarding privacy."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://philpapers.org/archive/VLIPM.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithmic systems: the consent is in the detail?&lt;/h2&gt;
&lt;p&gt;Applications of algorithmically informed decisions are becoming entrenched in society, with data processing being their main process and ingredient. While these applications are progressively gaining momentum, established data protection and privacy rules have struggled to incorporate the particularities of data-intensive information societies. It is a truism to point out the resulting misalignment between algorithmic processing of personal data and the data protection regulatory frameworks that strive for meaningful control over personal data. However, the challenges to the (traditional) role and concept of consent are particularly manifest. This article examines the transformation of consent models in order to assess how the concept and the applied models of consent can be reconciled in order to correspond not only to the current regulatory landscapes but also to the exponential growth of algorithmic processing technologies.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://policyreview.info/articles/analysis/algorithmic-systems-consent-detail"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Israeli Emergency Regulations for Location Tracking of Coronavirus Carriers&lt;/h2&gt;
&lt;p&gt;Surveillance and big data are an area of tech ethics that doesn't always overlap with AI ethics, but this is a reminder of the huge amount of data that we create, and that is retained, which AI might use for unexpected purposes down the road. &lt;/p&gt;
&lt;p&gt;"The ISA Emergency Coronavirus Regulations (“Authorizing the Israel Security Agency to Assist in the National Efforts to Reduce the Spread of the Novel Coronavirus”) authorize the ISA to receive, collect and process “technological data” for the purpose of assisting the Ministry of Health in conducting epidemiological investigations to reduce and prevent the spread of the novel coronavirus. The data should be used to identify the location data and movement routes of coronavirus carriers in the 14 days preceding their diagnosis as carriers, along with the identity of individuals who came into close contact with them."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.lawfareblog.com/israeli-emergency-regulations-location-tracking-coronavirus-carriers"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Soap Box as a Black Box: Regulating transparency in social media recommender systems&lt;/h2&gt;
&lt;p&gt;"Social media recommender systems play a central role in determining what content is seen online, and what remains hidden. As a point of control for media governance, they are subject to intense controversy and, increasingly, regulation by European policymakers. A recurring theme in such efforts is transparency, but this is an ambiguous concept that can be implemented in various ways depending on the types of accountability one envisages. This paper maps and critiques the various efforts at regulating social media recommendation transparency in Europe, and the types of accountability they pursue."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://osf.io/preprints/lawarxiv/uhxcv"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How can we build child-friendly AI that empowers children in Africa?&lt;/h2&gt;
&lt;p&gt;We missed this last week!&lt;/p&gt;
&lt;p&gt;"These reflections were gathered at an AI and Children African regional consultation that took place in Cape Town, South Africa in February 2020. It is the third in a series of regional workshops designed to develop global AI policy guidance that protects child rights."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.unicef.org/globalinsight/stories/how-can-we-build-child-friendly-ai-empowers-children-africa"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;We still need responsible tech for care in a global pandemic&lt;/h2&gt;
&lt;p&gt;"We spent the last eighteen months researching and redesigning care systems with people on the front line of care – people who give care, who work in care, who receive care, who organise care and who fight for better care, and those who do several or even all of the above. &lt;/p&gt;
&lt;p&gt;We came up with three key principles that were necessary prerequisites for technology developed for care to be responsible"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.doteveryone.org.uk/2020/03/we-still-need-responsible-tech-for-care-in-a-global-pandemic/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ada Lovelace Institute JUST AI network would like to hear from you.&lt;/h2&gt;
&lt;p&gt;Our humanities-led JUST AI network would like to hear from you...&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/AdaLovelaceInst/status/1242146621242122246"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: March 10 - March 17</title><link href="/ai-ethics-news-roundup-mar10-mar17-2020.html" rel="alternate"></link><published>2020-03-10T09:20:00+01:00</published><updated>2020-03-10T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-10:/ai-ethics-news-roundup-mar10-mar17-2020.html</id><summary type="html">&lt;p&gt;@datainnovation on AI and #covid19, @Brave challenges Google via the #GDPR, @rcalo &amp;amp; @daniellecitron on automated government, @jake_bittle on AI and deception, @MichaelEOHanlon on AI and National Security, @dmonett comments on the JAGI #AI issue, @datarobot on AI bias, @RadicalAIPod on the dangers of AI and #covid19&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;How Artificial Intelligence is Aiding the Fight Against Coronavirus&lt;/h2&gt;
&lt;p&gt;"The coronavirus (COVID-19) has dominated the global news and resulted in travel restrictions, school closures, market panic, and many more disruptions as health organizations work to contain the spread of the virus. With over 125,000 confirmed cases and over 4,500 deaths worldwide and over 1,600 cases and 41 deaths in the U.S., and the World Health Organization (WHO) officially labeling the virus as a pandemic, the global health community is relying on new tools and technologies to stay ahead. Artificial intelligence (AI) has proven especially valuable every step of the way, from detecting the first coronavirus outbreak to measuring the disease’s economic impact."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/03/how-artificial-intelligence-is-aiding-the-fight-against-coronavirus/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bias In Recruitment Software To Be ‘Illegal’ in New York, Vendors Will Need Bias Audit&lt;/h2&gt;
&lt;p&gt;"A new law by New York’s City Council will in effect make algorithmic bias in recruitment software illegal. It demands that vendors of such candidate filtering tools submit to a bias audit, with Civil penalties for those who fail to conduct an ‘impartial evaluation‘. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.artificiallawyer.com/2020/03/12/bias-in-recruitment-software-to-be-illegal-in-new-york-vendors-will-need-bias-audit/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The emergence of the professional AI risk manager&lt;/h2&gt;
&lt;p&gt;"We are now beset by data breaches and data privacy scandals, and regulators around the world have responded with data regulations. GDPR is the current role model, but I expect a global group of regulators to expand the rules to cover AI more broadly and set the standard on how to manage it. The UK ICO just released a draft but detailed guide on auditing AI. The EU is developing one as well. Interestingly, their approach is very similar to that of the Basel standards: specific AI risks should be explicitly managed. This will lead to the emergence of professional AI risk managers."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://venturebeat.com/2020/03/14/the-emergence-of-the-professional-ai-risk-manager/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI is an Ideology, Not a Technology&lt;/h2&gt;
&lt;p&gt;"A clear alternative to “AI” is to focus on the people present in the system. If a program is able to distinguish cats from dogs, don’t talk about how a machine is learning to see. Instead talk about how people contributed examples in order to define the visual qualities distinguishing “cats” from “dogs” in a rigorous way for the first time. There's always a second way to conceive of any situation in which AI is purported. This matters, because the AI way of thinking can distract from the responsibility of humans."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wired.com/story/opinion-ai-is-an-ideology-not-a-technology/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Formal GDPR complaint against Google’s internal data free-for-all&lt;/h2&gt;
&lt;p&gt;Brave has filed a formal GDPR complaint against Google for infringing the GDPR “purpose limitation” principle. Enforcement would be tantamount to a functional separation of Google’s business. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://brave.com/google-internal-data-free-for-all/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Responsible AI Moves Into Focus at Microsoft's Data Science and Law Forum&lt;/h2&gt;
&lt;p&gt;"Last week, Microsoft gathered experts from academia, civil society, policy making and more to discuss one of the most important topics in tech at the moment: responsible AI (RAI)."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cmswire.com/information-management/responsible-ai-moves-into-focus-at-microsofts-data-science-and-law-forum/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;An AI Ethics Checklist for COVID-19 Healthcare Technology&lt;/h2&gt;
&lt;p&gt;"On March 13th Forbes published an article entitled &lt;a href="https://www.forbes.com/sites/bernardmarr/2020/03/13/coronavirus-how-artificial-intelligence-data-science-and-technology-is-used-to-fight-the-pandemic/#43242df45f5f"&gt;Coronavirus: How Artificial Intelligence, Data Science And Technology Is Used To Fight The Pandemic&lt;/a&gt; written by Bernard Marr. In this article Marr lists 10 ways in which AI, ML, and DS are being used to address the pandemic. I would like to use Marr’s list as a framework to propose questions (3 questions per item) that folks designing and using this tech to engage with the virus might ask themselves to better ensure ethical creation and application. You can think of it as an AI ethics checklist"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.radicalai.org/blog/an-ai-ethics-checklist-for-covid-19-healthcare-technology"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Evolution of Artificial Intelligence and Future of National Security&lt;/h2&gt;
&lt;p&gt;"...Is AI likely to be all it’s cracked up to be? We think the answer is complex and that a modest dose of cold water should be thrown on the subject. In fact, many of the AI systems being envisioned today will take decades to develop. Moreover, AI is often being confused with things it is not. Precision about the concept will be essential if we are to have intelligent discussions about how to research, develop, and regulate AI in the years ahead."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://nationalinterest.org/feature/evolution-artificial-intelligence-and-future-national-security-133032"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Lie detectors have always been suspect. AI has made the problem worse.&lt;/h2&gt;
&lt;p&gt;An in-depth investigation into artificial-intelligence-based attempts to recognize deception.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615336/ai-lie-detectors-polygraph-silent-talker-iborderctrl-converus-neuroid/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A great thread from Dagmar Monett on the JAGI AI Issue&lt;/h2&gt;
&lt;p&gt;Dagmar Monett runs through abstracts and adds some commentary to the JAGI special issue on the concept of Artificial Intelligence. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/dmonett/status/1238791188565630977"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Automated Administrative State: A Crisis of Legitimacy&lt;/h2&gt;
&lt;p&gt;We missed this last week... &lt;/p&gt;
&lt;p&gt;"In recent decades, state and federal agencies have embraced a novel mode of operation: automation. Agencies rely more and more on software and algorithms in carrying out their delegated responsibilities. The automated administrative state, however, is demonstrably riddled with concerns. Legal challenges regarding the denial of benefits and rights—from travel to disability—have revealed a pernicious pattern of bizarre and unintelligible outcomes."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3553590"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial intelligence isn’t as smart as it thinks&lt;/h2&gt;
&lt;p&gt;"...the technology is limited to narrow tasks that require human oversight and mountains of data which is often skewed in ways that lead to unexpected, or even biased, results. The holy grail — so-called general artificial intelligence that can flit between various jobs mimicking human behavior — is still more a myth than a reality, with more than half of almost 400 AI experts recently surveyed saying it will be at least 2060, if not later, for such technology to become feasible."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://www.politico.eu/article/artificial-intelligence-drawbacks-isnt-as-smart-as-it-thinks"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Exploring Gender Imbalance in AI: Numbers, Trends, and Discussions&lt;/h2&gt;
&lt;p&gt;To highlight the contributions of women in the AI industry, Synced introduces the Women in AI special project this month and invites female researchers from the field to share their recent research works and the stories behind the idea. Join our conversation by clicking here.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/syncedreview/exploring-gender-imbalance-in-ai-numbers-trends-and-discussions-33096879bd54?_branch_match_id=768431107131880217"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Do You Define Unfair Bias in AI?&lt;/h2&gt;
&lt;p&gt;"In this blog post, rather than defining a unique solution, I will list the four key questions that you need to answer in order to derive a definition of unfair bias that matches your particular needs."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://blog.datarobot.com/how-do-you-define-unfair-bias-in-ai"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Mar 3 - Mar 10</title><link href="/ai-ethics-news-roundup-mar3-mar10-2020.html" rel="alternate"></link><published>2020-03-10T09:20:00+01:00</published><updated>2020-03-10T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-10:/ai-ethics-news-roundup-mar3-mar10-2020.html</id><summary type="html">&lt;p&gt;@Floridi &amp;amp; @agstrait ethical analysis, @carolinejmolloy: NHS and tech, @misslivirose: consent and virtuality, @jacobsonjenna: cybervetting, @mckelveyf: the public good, @timleberecht: AI, nature, and ethics, @NathalieSmuha: human rights, @SandraWachter5: fairness, @Sarah_Brayne: crime prediction + more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Ethical Foresight Analysis: What it is and Why it is Needed?&lt;/h2&gt;
&lt;p&gt;A new piece from Luciano Floridi and Andrew Strait:&lt;/p&gt;
&lt;p&gt;"An increasing number of technology firms are implementing processes to identify and evaluate the ethical risks of their systems and products. A key part of these review processes is to foresee potential impacts of these technologies on different groups of users. In this article, we use the expression Ethical Foresight Analysis (EFA) to refer to a variety of analytical strategies for anticipating or predicting the ethical issues that new technological artefacts, services, and applications may raise. This article examines several existing EFA methodologies currently in use. It identifies the purposes of ethical foresight, the kinds of methods that current methodologies employ, and the strengths and weaknesses of each of these current approaches. The conclusion is that a new kind of foresight analysis on the ethics of emerging technologies is both feasible and urgently needed."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007%2Fs11023-020-09521-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Robots Are Coming: Ethics, Politics, and Society in the Age of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;The late Kenneth Taylor's previously unpublished essay on the impact of AI and Robotics just came out in the Boston Review:&lt;/p&gt;
&lt;p&gt;"If we cannot stop or reverse the robot invasion of the built human world, we must turn and face them. We must confront hard questions about what will and should become of both them and us as we welcome ever more of them into our midst. Should we seek to regulate their development and deployment? Should we accept the inevitability that we will lose much work to them? If so, perhaps we should rethink the very basis of our economy. Nor is it merely questions of money that we must face. There are also questions of meaning. What exactly will we do with ourselves if there is no longer any economic demand for human cognitive labor? How shall we find meaning and purpose in a world without work?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://bostonreview.net/science-nature-philosophy-religion/kenneth-taylor-robots-are-coming"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithmic Legal Metrics&lt;/h2&gt;
&lt;p&gt;This is a fascinating paper that in places runs very much against the grain of current thinking. &lt;/p&gt;
&lt;p&gt;"In this paper I link the sociological and legal analysis of AI, highlighting the reflexive social processes that are engaged by algorithmic metrics. This paper examines these overlooked social effects of predictive legal algorithms, and contributes to the literature a vital fundamental but missing critique of such analytics. Specifically, this paper shows how the problematic social effects of algorithmic legal metrics extend far beyond the concerns about accuracy that have thus far dominated critiques of such metrics. Second, it demonstrates that corrective governance mechanisms such as enhanced due process or transparency will be inadequate to remedy such corrosive effects, and that some such remedies, such as transparency, may actually exacerbate the worst effects of algorithmic governmentality. Third, the paper shows that the application of algorithmic metrics to legal decisions aggravates the latent tensions between equity and autonomy in liberal institutions, undermining democratic values in a manner and on a scale not previously experienced by human societies. Illuminating these effects casts new light on the inherent social costs of AI metrics, particularly the perverse effects of deploying algorithms in legal systems. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3537337"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How ‘Smart Tech’ Masks an Emerging Era of Corporate Control&lt;/h2&gt;
&lt;p&gt;"Harvesting data requires the technical ability and social authority to probe things, people, and places. Control systems are fueled by data, which allows for more granular, more effective, and more instantaneous command over those same things, people, and places. Smart tech is the offspring of both imperatives."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://onezero.medium.com/how-smart-tech-masks-an-emerging-era-of-corporate-control-779c96b05f85"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Cybervetting job applicants on social media: the new normal?&lt;/h2&gt;
&lt;p&gt;"Our research, using an online survey with 482 participants, investigates young people’s concerns with their publicly available social media data being used in the context of job hiring. Grounded in stakeholder theory, we analyze the relationship between young people’s concerns with social media screening and their gender, job seeking status, privacy concerns, and social media use. We find that young people are generally not comfortable with social media screening. A key finding of this research is that concern for privacy for public information on social media cannot be fully explained by some “traditional” variables in privacy research. The research extends stakeholder theory to identify how social media data ethics should be inextricably linked to organizational practices. The findings have theoretical implications for a rich conceptualization of stakeholders in an age of social media and practical implications for organizations engaging in cybervetting."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://link.springer.com/article/10.1007%2Fs10676-020-09526-2"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Before Adopting New Technologies, We Must Define the Common Good&lt;/h2&gt;
&lt;p&gt;"Canada (and the rest of the world) faces an opportunity: to avoid (or halt) the harmful effects of digital control and techno-social engineering, regulators must garner more public interest in the ways that technologies shape public life. Wrestling with the effects of technology, its wide reach and impact provides opportunity to foster more inclusive and engaged consultations about the environmental, health, social, cultural and democratic implications of powerful technologies such as AI and digital platforms. Here, there is excitement as much as risk in searching for the right formats to find, define and debate the common good."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cigionline.org/articles/adopting-new-technologies-we-must-define-common-good"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Artificial Intelligence Can Turn Us Into Bigots&lt;/h2&gt;
&lt;p&gt;An intriguing thought: AI can devleop it's own pseudoscience. &lt;/p&gt;
&lt;p&gt;"While the computing involved in AI is often incomprehensibly vast and fast, it always rests upon foundations of simplifying, quantifying, and generalising. And that’s okay: it’s how engineers traditionally solve problems, by breaking them down into simple parts, simplifying where necessary, and building in safety margins to account for inaccuracies."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://10daily.com.au/views/a200304obwdr/how-artificial-intelligence-can-turn-us-into-bigots-20200306"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The case for an AI that puts nature and ethics first, not humans&lt;/h2&gt;
&lt;p&gt;"...concern is growing that we are surrendering to a paradigm of “forced reductionism” (to borrow a term from former MIT Media Lab director Joi Ito), shoehorning ourselves into a purely mechanistic, utilitarian model of technology. As AI becomes more and more powerful and invasive, it may inevitably change our world to align with these very design principles. The consequence might be a world full of “monochrome societies,” as Infineon CEO Dr. Reinhard Pless puts it."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thenextweb.com/neural/2020/03/07/the-case-for-an-ai-that-puts-nature-and-ethics-first-not-humans-syndication/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;This Company Is Using Racially-Biased Algorithms to Select Jurors&lt;/h2&gt;
&lt;p&gt;"Momus Analytics' predictive scoring system is using race to grade potential jurors on vague qualities like "leadership" and "personal responsibility.""&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.vice.com/en_us/article/epgmbw/this-company-is-using-racially-biased-algorithms-to-select-jurors"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Beyond a Human Rights-based approach to AI Governance: Promise, Pitfalls, Plea&lt;/h2&gt;
&lt;p&gt;This paper discusses the establishment of a governance framework to secure the development and deployment of “good AI”, and describes the quest for a morally objective compass to steer it. Asserting that human rights can provide such compass, this paper first examines what a human rights-based approach to AI governance entails, and sets out the promise it propagates. Subsequently, it examines the pitfalls associated with human rights, particularly focusing on the criticism that these rights may be too Western, too individualistic, too narrow in scope and too abstract to form the basis of sound AI governance. After rebutting these reproaches, a plea is made to move beyond the calls for a human rights-based approach, and start taking the necessary steps to attain its realisation. It is argued that, without elucidating the applicability and enforceability of human rights in the context of AI; adopting legal rules that concretise those rights where appropriate; enhancing existing enforcement mechanisms; and securing an underlying societal infrastructure that enables human rights in the first place, any human rights-based governance framework for AI risks falling short of its purpose. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3543112"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Will Big Tech save the NHS – or eat it alive?&lt;/h2&gt;
&lt;p&gt;"Technophile politicians and tech companies are making big promises. But the broader impact of the digital transformation of our health and public services has been too little examined." &lt;/p&gt;
&lt;p&gt;"Digitalisation is a tool to ‘tailor’ services, we’re told. But in a context of austerity, the biggest concern is that ‘tailoring’ becomes a fancy word for ‘cutting’ – that it provides the cover for cuts, privatisation, co-payments, and the loss of communal space, public accountability and social connection."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.opendemocracy.net/en/ournhs/will-big-tech-save-the-nhs-or-eat-it-alive/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI&lt;/h2&gt;
&lt;p&gt;"This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automat-ed fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as “contextual equality.”"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;It’s Time To Develop A Consent Framework For Virtual Beings&lt;/h2&gt;
&lt;p&gt;The rise of virtual beings leads to new questions regarding the rights of digital representations.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://vrscout.com/news/consent-framework-for-virtual-beings/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI In Policing: Better Than ‘A Knife Through The Chest?’&lt;/h2&gt;
&lt;p&gt;"The U.K. police are rolling out AI crime fighting with little regard to the societal risks. In an impassioned speech about police use of AI, the commissioner of the Metropolitan police, Cressida Dick, criticised privacy advocates. She intoned that concern over the use of Live Face Recognition (LFR) on law abiding citizens, “feels much, much smaller than my and the public’s vital expectation to be kept safe from a knife through the chest.” "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/noelsharkey/2020/03/06/ai-in-policing-better-than-a-knife-through-the-chest/#e1361f3548e4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Technologies of Crime Prediction: The Reception of Algorithms in Policing and Criminal Courts&lt;/h2&gt;
&lt;p&gt;"The number of predictive technologies used in the U.S. criminal justice system is on the rise. Yet there is little research to date on the reception of algorithms in criminal justice institutions. We draw on ethnographic fieldwork conducted within a large urban police department and a midsized criminal court to assess the impact of predictive technologies at different stages of the criminal justice process. '&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://academic.oup.com/socpro/advance-article-abstract/doi/10.1093/socpro/spaa004/5782114"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;JAGI Special Issue “On Defining Artificial Intelligence”&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href="https://content.sciendo.com/view/journals/jagi/11/2/jagi.11.issue-2.xml"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 25 - Mar 3</title><link href="/ai-ethics-news-roundup-feb25-mar3-2020.html" rel="alternate"></link><published>2020-03-03T09:20:00+01:00</published><updated>2020-03-03T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-03-03:/ai-ethics-news-roundup-feb25-mar3-2020.html</id><summary type="html">&lt;p&gt;@TC_IntLaw discusses a GDPR decision on #facialrecognition at @AI_Regulation, @oshdzieza on AI in the workplace, @math_rachel on selling our own data, @willknight on fooling AI, and much more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;First Ever Decision of a French Court Applying GDPR to Facial Recognition&lt;/h2&gt;
&lt;p&gt;A French court canceled a decision by the South-Est Region of France to undertake a series of tests using facial recognition at the entrance of two High schools considering that this would be illegal. This is the first decision ever by a French Court applying the General Data Protection Regulation (GDPR) on Facial Recognition Technologies (FRTs).&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ai-regulation.com/first-decision-ever-of-a-french-court-applying-gdpr-to-facial-recognition/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A Framework for Responsible Limits on Facial Recognition&lt;/h2&gt;
&lt;p&gt;"The World Economic Forum’s Framework for the Responsible use of facial recognition technology seeks to address the need for a set of concrete guidelines to ensure the trustworthy and safe use of this technology. This framework enables Governments to protect citizens from various harms potentially caused by facial recognition technology while supporting beneficial applications. It also enables industry actors to demonstrate that they have implemented robust risk mitigation processes through an independent audit of their systems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.weforum.org/whitepapers/a-framework-for-responsible-limits-on-facial-recognition-use-case-flow-management"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Crowdsourcing Moral Machines&lt;/h2&gt;
&lt;p&gt;"... We believe bringing about accountable intelligent machines that embody human ethics requires an interdisciplinary approach. First, engineers build and refine intelligent machines, and tell us how they are capable of operating. Second, scholars from the humanities—philosophers, lawyers, social theorists—propose how machines ought to behave, and identify hidden moral hazards in the system. Third, behavioral scientists, armed with tools for public engagement and data collection like the MM, provide a quantitative picture of the public's trust in intelligent machines, and of their expectations of how they should behave.b Finally, regulators monitor and quantify the performance of machines in the real world, making this data available to engineers and citizens, while using their enforcement tools to adjust the incentives of engineers and corporations building the machines."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://cacm.acm.org/magazines/2020/3/243030-crowdsourcing-moral-machines/fulltext"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bad news for explainability?&lt;/h2&gt;
&lt;p&gt;"One of the strangest mysteries in AI is that you can average two models and get a result superior to either model alone."&lt;/p&gt;
&lt;p&gt;An interesting twitter discussion on a surprising way to improve ML, the exact characterization of which is not quite certain, but perhaps by mitigating overfitting. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/theshawwn/status/1234310915895353344"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How hard will the robots make us work?&lt;/h2&gt;
&lt;p&gt;"In warehouses, call centers, and other sectors, intelligent machines are managing humans, and they’re making work more stressful, grueling, and dangerous"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theverge.com/2020/2/27/21155254/automation-robots-unemployment-jobs-vs-human-google-amazon"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;As humanity’s relationship with AI grows, experts call for protective framework&lt;/h2&gt;
&lt;p&gt;"Imperial College London researchers have suggested a new regulatory framework with which governments can minimise unintended consequences of our relationship with technology. The comment piece is published in Nature Machine Intelligence."&lt;/p&gt;
&lt;p&gt;"The proposed framework, known as the Human Impact Assessment for Technology (HIAT), would be designed to predict and evaluate the impact that new digital technologies have on society and individual wellbeing. This, they argue, should focus on ethical considerations like individual privacy, wellbeing and autonomy."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.imperial.ac.uk/news/195615/as-humanitys-relationship-with-ai-grows/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Can you sell your own data and therefore consent to how it will be used downstream?&lt;/h2&gt;
&lt;p&gt;This is an important question in light of regulations like the CCPA and calls for users to be compensated for the data extracted from them. A nice thread from Rachel Thomas.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/math_rachel/status/1233109858079166470"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;In Coronavirus Fight, China Gives Citizens a Color Code, With Red Flags&lt;/h2&gt;
&lt;p&gt;"As China encourages people to return to work despite the coronavirus outbreak, it has begun a bold mass experiment in using data to regulate citizens’ lives — by requiring them to use software on their smartphones that dictates whether they should be quarantined or allowed into subways, malls and other public spaces.&lt;/p&gt;
&lt;p&gt;But a New York Times analysis of the software’s code found that the system does more than decide in real time whether someone poses a contagion risk. It also appears to share information with the police, setting a template for new forms of automated social control that could persist long after the epidemic subsides."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/03/01/business/china-coronavirus-surveillance.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How Adversarial Attacks Could Destabilize Military AI Systems&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence and robotic technologies with semi-autonomous learning, reasoning, and decision-making capabilities are increasingly being incorporated into defense, military, and security systems. Unsurprisingly, there is increasing concern about the stability and safety of these systems. In a different sector, runaway interactions between autonomous trading systems in financial markets have produced a series of stock market “flash crashes,” and as a result, those markets now have rules to prevent such interactions from having a significant impact"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/adversarial-attacks-and-ai-systems#disqus_thread"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;More on Adversarial AI and The risks of algorithmic (il)literacy on healthcare platforms&lt;/h2&gt;
&lt;p&gt;We missed this one last week, a very nice discussion of the use of machine learning in health care and some of the ethical problems this raises, in particular, the problem of expertise that we must trust, but that we cannot engage with. &lt;/p&gt;
&lt;p&gt;We also missed a related piece from Wired on &lt;a href="https://www.wired.com/story/technique-uses-ai-fool-other-ais/"&gt;how easily algorithms can be fooled&lt;/a&gt;, for example, when assessing a medical claim. &lt;/p&gt;
&lt;p&gt;Even more further reading on deception from IEEE, &lt;a href="https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/ai-deception-when-your-ai-learns-to-lie"&gt;AI Deception: When Your Artificial Intelligence Learns to Lie&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And there's even a conference coming up: &lt;a href="https://sites.google.com/view/deceptecai2020"&gt;1st International Workshop on Deceptive AI @ECAI2020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://theconversation.com/the-risks-of-algorithmic-il-literacy-on-healthcare-platforms-130914"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Can YouTube Quiet Its Conspiracy Theorists?&lt;/h2&gt;
&lt;p&gt;The extreme and radicalizing nature of Youtube's algorithm has been a topic of significant discussion, an now "A new study examines YouTube’s efforts to limit the spread of conspiracy theories on its site, from videos claiming the end times are near to those questioning climate change."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/interactive/2020/03/02/technology/youtube-conspiracy-theory.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 17 - Feb 25</title><link href="/ai-ethics-news-roundup-feb17-feb25-2020.html" rel="alternate"></link><published>2020-02-25T09:20:00+01:00</published><updated>2020-02-25T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-25:/ai-ethics-news-roundup-feb17-feb25-2020.html</id><summary type="html">&lt;p&gt;@HMRoff on deceptive AI, @dryellowbean &amp;amp; @jud1ths1mon on ethics, @vdignum, @MullerCatelijne and @RecklessCoding on the EU AI Ethics Whitepaper, @Klonick provokes on the definition of AI, new AI ethics map from @aiethicslab&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;First analysis of the EU Whitepaper on AI&lt;/h2&gt;
&lt;p&gt;"This week, Europe took a clear stance on AI; foster the uptake of AI technologies, underpinned by what it calls ‘an ecosystem of excellence’, while also ensuring their compliance with to European ethical norms, legal requirements and social values, ‘an ecosystem of trust’. While the Whitepaper on AI of the European Commission does not propose legislation yet, it announces some bold legislative measures, that will likely materialize by the end of 2020. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://allai.nl/first-analysis-of-the-eu-whitepaper-on-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Deception: When Your Artificial Intelligence Learns to Lie&lt;/h2&gt;
&lt;p&gt;"Understanding the breadth of what “AI deception” looks like, and what happens when it is not a human’s intent behind a deceptive AI, but instead the AI agent’s own learned behavior. These may seem somewhat far-off concerns, as AI is still relatively narrow in scope and can be rather stupid in some ways. To have some analogue of an “intent” to deceive would be a large step for today’s systems. However, if we are to get ahead of the curve regarding AI deception, we need to have a robust understanding of all the ways AI could deceive. We require some conceptual framework or spectrum of the kinds of deception an AI agent may learn on its own before we can start proposing technological defenses."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/ai-deception-when-your-ai-learns-to-lie"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics lab releases AI Princples Map&lt;/h2&gt;
&lt;p&gt;"We decided to create the AI Principles Map to help understand the trends, common threads, and differences among numerous sets of principles published."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://aiethicslab.com/big-picture/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Thinking About ‘Ethics’ in the Ethics of AI&lt;/h2&gt;
&lt;p&gt;"One of the fundamental questions in the ethics of AI, therefore, can be formulated as a problem of value alignment: how can we build autonomous AI that is aligned with societally held values."&lt;/p&gt;
&lt;p&gt;"Our review of the theoretical, technical, and ethical challenges to machine ethics does not intend to be exhaustive or conclusive, and these challenges could indeed be overcome in future research and development of autonomous AI. However, we think that these challenges do warrant a pause and reconsideration of the prospects of building ethical AI. In fact, we want to advance a more fundamental critique of machine ethics before exploring another path for answering the value alignment problem."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://revistaidees.cat/en/thinking-about-ethics-in-the-ethics-of-ai/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethical Dimensions of Using Artificial Intelligence in Health Care&lt;/h2&gt;
&lt;p&gt;"An artificially intelligent computer program can now diagnose skin cancer more accurately than a board-certified dermatologist.1 Better yet, the program can do it faster and more efficiently, requiring a training data set rather than a decade of expensive and labor-intensive medical education. While it might appear that it is only a matter of time before physicians are rendered obsolete by this type of technology, a closer look at the role this technology can play in the delivery of health care is warranted to appreciate its current strengths, limitations, and ethical complexities."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://journalofethics.ama-assn.org/article/ethical-dimensions-using-artificial-intelligence-health-care/2019-02"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Would you rather a human or a black box AI perform surgery on you? Would you want the AI to be illegal?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/geoffreyhinton/status/1230592238490615816"&gt;This tweet&lt;/a&gt; sparked a wide ranging discussion this week. Some objected to the framing, arguing that the presumption of superior outcomes from the AI smuggled in a lot of assumptions regarding whom, exactly, and in what cases, might expect to benefit. Others argued that the question of illegality is an orthogonal one, and ought not be considered directly alongside a claim to superior performance. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/geoffreyhinton/status/1230592238490615816"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What Artificial Intelligence Is Not&lt;/h2&gt;
&lt;p&gt;"Artificial intelligence is not one thing." - a short provocative piece from Kate Klonick on the many different thigns that we can mean when we talk about "AI".  &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://blog.lareviewofbooks.org/provocations/artificial-intelligence/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI and the bottom line: 15 examples of artificial intelligence in finance&lt;/h2&gt;
&lt;p&gt;An updated rundown of AI applications in finance. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://builtin.com/artificial-intelligence/ai-finance-banking-applications-companies"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Forensic Architecture Founder Says United States Prevented His Visit Via Algorithm&lt;/h2&gt;
&lt;p&gt;"Eyal Weizman, the director of the investigative group, said an embassy official in London told him an algorithm had identified a security threat that was related to him."&lt;/p&gt;
&lt;p&gt;“Associative algorithms, triangulating algorithms, that look at patterns that look at relations between actions and movement, between people and places,” he said. “We need to gear up to be able identify and monitor those.”&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/02/19/arts/design/forensic-architecture-founder-says-us-denied-him-visa.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 12 - Feb 18</title><link href="/ai-ethics-news-roundup-feb2-feb18-2020.html" rel="alternate"></link><published>2020-02-18T09:20:00+01:00</published><updated>2020-02-18T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-18:/ai-ethics-news-roundup-feb2-feb18-2020.html</id><summary type="html">&lt;p&gt;History of algorithms w/ @RiederB,@silvertjeand,@_mstevenson, @dorotheabaur finds a dilemma for companies ethics-washing facial recognition, @_KarenHao on OpenAI, @chengela and @hannahdev on reading emotion, @poppynoor on medtech, and @emilymbender tweeting from AAAS2020&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Emotion AI researchers say overblown claims give their work a bad name&lt;/h2&gt;
&lt;p&gt;There are no strong, peer-reviewed studie proving that analyzing body posture or facial expressions can help pick the best workers or students (in part because companies are secretive about their methods). As a result, the hype around emotion recognition, which is projected to be a $25 billion market by 2023, has created a backlash from tech ethicists and activists who fear that the technology could raise the same kinds of discrimination problems as predictive sentencing or housing algorithms for landlords deciding whom to rent to.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615232/ai-emotion-recognition-affective-computing-hirevue-regulation-ethics/?utm_source=newsletters&amp;utm_medium=email&amp;utm_campaign=the_algorithm.unpaid.engagement"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Why we should hope that corporate claims about the ethics of facial recognition are pure marketing&lt;/h2&gt;
&lt;p&gt;"If we acknowledge the non-commercial, or rather the effectively ‘far-beyond-commercial’ dimensions of facial recognition technology, we in fact severely restrict our abilities to demand accountability for the claims regarding their ‘ethical qualities’."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@dorotheabaur/why-we-should-hope-that-corporate-claims-about-the-ethics-of-facial-recognition-are-pure-marketing-d8b2fea83319"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The historical trajectories of algorithmic techniques: an interview with Bernhard Rieder&lt;/h2&gt;
&lt;p&gt;"In this interview, Michael Stevenson (MS) and Anne Helmond (AH) talk to Bernhard Rieder (BR) about his forthcoming book entitled Engines of Order: A Mechanology of Algorithmic Techniques (University of Amsterdam Press, 2020). In particular, Rieder discusses how the practice of software-making is “constantly faced with the ‘legacies’ of previous work” and how the past continues to operate into present algorithmic techniques."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.tandfonline.com/doi/full/10.1080/24701475.2020.1723345"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;We know ethics should inform AI. But which ethics?&lt;/h2&gt;
&lt;p&gt;The ethical standards for assessing AI and its associated technologies are still in their infancy. Companies need to initiate internal discussion as well as external debate with their key stakeholders about how to avoid being caught up in difficult situations.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.gigabitmagazine.com/ai/we-know-ethics-should-inform-ai-which-ethics?utm_content=117506893&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-2797602696"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Navigating AI’s expanding role in the world of HR&lt;/h2&gt;
&lt;p&gt;As the use of artificial intelligence continues to skyrocket within the HR realm, related ethics issues represent real risk if not handled correctly, and early. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hrexecutive.com/navigating-ais-expanding-role-in-the-world-of-hr/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The messy, secretive reality behind OpenAI’s bid to save the world&lt;/h2&gt;
&lt;p&gt;Karen Hao spent half a year digging into @OpenAI, the SF-based AI research lab, originally founded by @elonmusk. I started with a few simple questions: Who are they? What are their goals? How do they work? After nearly three dozen interviews, I found so much more.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;"AI systems claiming to 'read' emotions pose discrimination risks&lt;/h2&gt;
&lt;p&gt;There are no strong, peer-reviewed studie proving that analyzing body posture or facial expressions can help pick the best workers or students (in part because companies are secretive about their methods). As a result, the hype around emotion recognition, which is projected to be a $25 billion market by 2023, has created a backlash from tech ethicists and activists who fear that the technology could raise the same kinds of discrimination problems as predictive sentencing or housing algorithms for landlords deciding whom to rent to.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/feb/16/ai-systems-claiming-to-read-emotions-pose-discrimination-risks"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Can we trust AI not to further embed racial bias and prejudice?&lt;/h2&gt;
&lt;p&gt;Heralded as an easy fix for health services under pressure, data technology is marching ahead unchecked But is there a risk it could compound inequalities? Poppy Noor investigates&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.bmj.com/content/368/bmj.m363"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Great twitter thread from the Ethics and AI panel panel at American Association for the Advancement of Science 2020&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/emilymbender/status/1228749166622334976"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Feb 5 - Feb 11</title><link href="/ai-ethics-news-roundup-feb5-feb11-2020.html" rel="alternate"></link><published>2020-02-11T09:20:00+01:00</published><updated>2020-02-11T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-11:/ai-ethics-news-roundup-feb5-feb11-2020.html</id><summary type="html">&lt;p&gt;@alexsablay and @hjegou on tracking training data, @dorotheabaur on facial recognition, @lindakinstler on tech ethicists, @benzevenbergen and @drzimmermann on ethical pitfalls, SyRI from @SaschaSchendel, @datainnovation 2019 review, @PublicStandards report on AI&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;European parliament says it will not use facial recognition tech&lt;/h2&gt;
&lt;p&gt;"The European parliament has insisted it has no plans to introduce facial recognition technology after a leaked internal memo discussing its use in security provoked an outcry."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.theguardian.com/technology/2020/feb/05/european-parliament-insists-it-will-not-use-facial-recognition-tech"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The SyRI case: a landmark ruling for benefits claimants around the world&lt;/h2&gt;
&lt;p&gt;"In NJCM cs/ De Staat der Nederlanden (NJCM vs the Netherlands), also known as the SyRI case, the court considered the legality of the System Risk Indication (SyRI), a system designed by the Dutch government to process large amounts of data collected by various Dutch public authorities to identify those most likely to commit benefits fraud."&lt;/p&gt;
&lt;p&gt;There's a &lt;a href="https://twitter.com/SaschaSchendel/status/1225094322510536705"&gt;great twitter thread on SyRI too&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://privacyinternational.org/news-analysis/3363/syri-case-landmark-ruling-benefits-claimants-around-world?PageSpeed=noscript"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Using ‘radioactive data’ to detect if a data set was used for training&lt;/h2&gt;
&lt;p&gt;"We have developed a new technique to mark the images in a data set so that researchers can determine whether a particular machine learning model has been trained using those images. This can help researchers and engineers to keep track of which data set was used to train a model so they can better understand how various data sets affect the performance of different neural networks."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/2002.00937"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Opposing facial recognition — why focusing on accuracy misses the point&lt;/h2&gt;
&lt;p&gt;"These are important and very worrying findings that emphasize the overall problem with algorithmic discrimination. But what does it imply when we argue against facial technology based on empirical arguments such as overall low accuracy and particular bias with regards to certain ethnic groups? In the worst case, proponents of facial recognition could use this argument to their advantage and take it to underline the need to train the technology better to make sure it improves its accuracy — as apparently done by Google last year. Better training means collecting more data. And collecting more data often means intruding into people’s privacy. We might open the floodgates for even larger scale data collection."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@dorotheabaur/opposing-facial-recognition-why-focusing-on-accuracy-misses-the-point-9b96ea3f864b"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;An Algorithm That Grants Freedom, or Takes It Away&lt;/h2&gt;
&lt;p&gt;"Across the United States and Europe, software is making probation decisions and predicting whether teens will commit crime. Opponents want more human oversight."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2020/02/06/technology/predictive-algorithms-crime.html?referringSource=articleShare
"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethicists were hired to save tech’s soul. Will anyone let them?&lt;/h2&gt;
&lt;p&gt;"While some tech firms have taken concrete steps to insert ethical thinking into their processes, Catherine Miller, interim CEO of the ethical consultancy Doteveryone, says there's also been a lot of "flapping round" the subject."&lt;/p&gt;
&lt;p&gt;"Critics dismiss it as "ethics-washing," the practice of merely kowtowing in the direction of moral values in order to stave off government regulation and media criticism. The term belongs to the growing lexicon around technology ethics, or "tethics," an abbreviation that began as satire on the TV show "Silicon Valley," but has since crossed over into occasionally earnest usage. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.protocol.com/ethics-silicon-valley"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society&lt;/h2&gt;
&lt;p&gt;"As AI (and associated AI-hype) grows more pervasive in our lives, its impact on society is ever more significant, raising ethical concerns and challenges regarding issues such as privacy, safety and security, surveillance, inequality, data handling and bias, personal agency, power relations, effective modes of regulation, accountability, sanctions, and workforce displacement. Only a multidisciplinary effort can find the best ways to address these concerns, including experts from various disciplines, such as ethics, philosophy, economics, sociology, psychology, law, history, politics, interaction design, informatics, social studies of science and technology, communication and media studies, and political science, as well as those with lived experience in relation to the impacts of AI systems. In order to address these issues in a scientific context, AAAI and ACM joined forces in 2018 to start the AAAI/ACM Conference on AI, Ethics, and Society. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/proceedings/10.1145/3375627#sec1"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics: Seven Traps&lt;/h2&gt;
&lt;p&gt;"The question of how to ensure that technological innovation in machine learning and artificial intelligence leads to ethically desirable—or, more minimally, ethically defensible—impacts on society has generated much public debate in recent years. Most of these discussions have been accompanied by a strong sense of urgency: as more and more studies about algorithmic bias have shown, the risk that emerging technologies will not only reflect, but also exacerbate structural injustice in society is significant."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://freedom-to-tinker.com/2019/03/25/ai-ethics-seven-traps/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence and Public Standards: Committee publishes report&lt;/h2&gt;
&lt;p&gt;“Our message to government is that the UK’s regulatory and governance framework for AI in the public sector remains a work in progress and deficiencies are notable. The work of the Office for AI, the Alan Turing Institute, the Centre for Data Ethics and Innovation (CDEI), and the Information Commissioner’s Office (ICO) are all commendable. But on transparency and data bias in particular, there is an urgent need for practical guidance and enforceable regulation."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.gov.uk/government/news/artificial-intelligence-and-public-standards-committee-publishes-report"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Most Significant AI Policy Developments in the United States in 2019&lt;/h2&gt;
&lt;p&gt;"2019 was a monumental year for artificial intelligence (AI) policy in the United States. The federal government took several important steps that prioritized AI development and deployment and positioned the United States to strengthen its global AI leadership, beginning with President Trump’s “Executive Order on Maintaining American Leadership in Artificial Intelligence,” which set the tone for the rest of the year."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/02/the-most-significant-ai-policy-developments-in-the-united-states-in-2019/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 28 - Feb 4</title><link href="/ai-ethics-news-roundup-jan28-feb4-2020.html" rel="alternate"></link><published>2020-02-04T09:20:00+01:00</published><updated>2020-02-04T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-04:/ai-ethics-news-roundup-jan28-feb4-2020.html</id><summary type="html">&lt;h1&gt;AIEthics news for Jan 28 - Feb 4 featuring health care and drug discovery (@exscientialtd and a great twitter thread from @DorotheaBaur), privacy (@BernardMarr), facial recognition (@castrotech @itifdc @mclaughlintech), impact assessment (@Rafael_A_Calvo), comment on EU AI regs from @datainnovation and more!&lt;/h1&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Would you take a drug discovered by artificial intelligence?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/DorotheaBaur/status/1223549933506191363"&gt;@DorotheaBaur sparked an interesting reaction on Twitter&lt;/a&gt;, where in a climate of frequent AI news that raises ethical concerns, she asked if we might agree that this was a good example an applicaiton of AI that was not ethically problematic. &lt;/p&gt;
&lt;p&gt;"The British startup Exscientia claims it has developed the first medication created using artificial intelligence that will be clinically tested on humans. The medication, which is meant to treat obsessive-compulsive disorder, took less than a year from conception to trial-ready capsule. Human trials are set to begin in March, but would you take a drug designed using artificially intelligent software?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.techjuice.pk/exscientia-an-oxford-based-biotech-company-is-going-to-test-the-ai-designed-medicine-on-humans/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence Is Not Ready For The Intricacies Of Radiology&lt;/h2&gt;
&lt;p&gt;"...while much of the theoretical basis for AI in the practice of radiology is extremely exciting, the reality is that the field has not yet fully embraced it. The most significant issue is that the technology simply isn’t ready, as many of the existing systems have not yet been matured to compute and manage larger data sets or work in more general practice and patient settings, and thus, are not able to perform as promised. Other issues exist on the ethical aspects of AI. Given the sheer volume of data required to both train and perfect these systems, as well as the immense data collection that these systems will engage in once fully mainstream, key stakeholders are raising fair concerns and the call for strict ethical standards to be put into place, simultaneous to the technological development of these systems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/saibala/2020/02/03/artificial-intelligence-is-not-ready-for-the-intricacies-of-radiology/#312ac17867eb"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Advancing impact assessment for intelligent systems&lt;/h2&gt;
&lt;p&gt;"We discuss how the EIA provides a partial blueprint for what we call a Human Impact Assessment for Technology (HIAT) and how more recent algorithmic and data protection impact assessment initiatives can contribute. We also discuss how ethical frameworks for such a human impact assessment could draw on recently established AI ethics principles. We argue that this approach will help build trust in an industry facing increasing criticism and scrutiny."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.repository.cam.ac.uk/handle/1810/300852"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;What Is A Data Passport: Building Trust, Data Privacy And Security In The Cloud&lt;/h2&gt;
&lt;p&gt;"Data passports allow you to extend the encryption technology that used to be only available on a physical mainframe to cloud computing. Each piece of data in the cloud has a passport assigned to it, and with the passport, you can verify if the data is misused, if the passport is still valid, etc. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/bernardmarr/2020/01/31/what-is-a-data-passport-building-trust-data-privacy-and-security-in-the-cloud/#76c416f75843"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How the EU Should Revise its AI White Paper Before it is Published&lt;/h2&gt;
&lt;p&gt;"The European Commission is planning to release a white paper to support the development and uptake of artificial intelligence (AI). Early drafts of this white paper suggest that the Commission may call for additional AI regulations that would make it more expensive and more difficult for European businesses to use AI systems in many areas of the economy. Given the EU’s desire to be a leader in AI, and to use AI to bolster its global competitiveness, the Commission should avoid heavy-handed rules that would slow adoption of this emerging technology."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2020/02/how-the-eu-should-revise-its-ai-white-paper-before-it-is-published/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Critics Were Wrong: NIST Data Shows the Best Facial Recognition Algorithms Are Neither Racist Nor Sexist&lt;/h2&gt;
&lt;p&gt;"NIST assessed the false positive and false-negative rates of algorithms using four types of images, including mugshots, application photographs from individuals applying for immigration benefits, visa photographs, and images taken of travelers entering the United States. NIST’s report reveals that: &lt;/p&gt;
&lt;p&gt;a) The most accurate identification algorithms have “undetectable” differences between demographic groups
b) The most accurate verification algorithms have low false positives and false negatives across most demographic groups
c) Algorithms can have different error rates for different demographics but still be highly accurate"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://itif.org/publications/2020/01/27/critics-were-wrong-nist-data-shows-best-facial-recognition-algorithms"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;COR-GAN: Correlation-Capturing Convolutional Neural Networks for Generating Synthetic Healthcare Records&lt;/h2&gt;
&lt;p&gt;"In this paper, we propose a novel framework called correlation-capturing Generative Adversarial Network (corGAN), to generate synthetic healthcare records. In corGAN we utilize Convolutional Neural Networks to capture the correlations between adjacent medical features in the data representation space by combining Convolutional Generative Adversarial Networks and Convolutional Autoencoders"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/cor-gan-correlation-capturing-convolutional-neural-networks-for-generating-synthetic-healthcare-records"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;New surveillance AI can tell schools where students are and where they’ve been)&lt;/h2&gt;
&lt;p&gt;"Not all AI being used by schools is facial recognition. That doesn’t mean the tech doesn’t come with privacy risks. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.vox.com/recode/2020/1/25/21080749/surveillance-school-artificial-intelligence-facial-recognition"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Connected cots, talking teddies, and the rise of the algorithmic child&lt;/h2&gt;
&lt;p&gt;"Digital technologies are now a ubiquitous part of our daily lives. And questions remain as to how these technologies are reshaping how we experience the world around us, and how the world around us is being reshaped. One area this is being played out is in the family – in changing the experience of not only childhood, but what constitutes good parenting."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://keyahconsulting.com/connected-cots-talking-teddies-and-the-rise-of-the-algorithmic-child/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 21 - Jan 28</title><link href="/ai-ethics-news-roundup-jan21-jan28-2020.html" rel="alternate"></link><published>2020-01-28T09:20:00+01:00</published><updated>2020-01-28T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-28:/ai-ethics-news-roundup-jan21-jan28-2020.html</id><summary type="html">&lt;p&gt;Latest AIEthics news: AI in health (@jessRmorley @Floridi), ethics-bashing (@Elibietti), value-alignment (@IasonGabriel), bias (@SimonHegelich @SciOrestis @thyjuancarlos @FabienneMarco), explainability from(@MaribelLopez) and more from @LloydDanzig, @alexcengler and @LizzieGibney!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;An ethically mindful approach to AI for health care&lt;/h2&gt;
&lt;p&gt;"Health-care systems worldwide face increasing demand, a rise in chronic disease, and resource constraints. At the same time, the use of digital health technologies in all care settings has led to an expansion of data. These data, if harnessed appropriately, could enable health-care providers to target the causes of ill-health and monitor the effectiveness of preventions and interventions. For this reason, policy makers, politicians, clinical entrepreneurs, and computer and data scientists argue that a key part of health-care solutions will be artificial Intelligence (AI), particularly machine learning."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)32975-7/fulltext"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy&lt;/h2&gt;
&lt;p&gt;"The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, "ethics" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called "ethics washing" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in "ethics bashing." This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/abs/10.1145/3351095.3372860"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial Intelligence, Values and Alignment&lt;/h2&gt;
&lt;p&gt;"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Bias in Word Embeddings&lt;/h2&gt;
&lt;p&gt;"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372843"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Road to Artificial Intelligence: An Ethical Minefield&lt;/h2&gt;
&lt;p&gt;"There are a number of ethical dilemmas woven inextricably into the field of Artificial Intelligence, many of which are often overlooked, even within the engineering community. Even the best intentions are often not enough to guarantee solutions free from unintended or undesired results, as humans can accidentally encode biases into AI engines and malicious actors can exploit flaws in models. In the short-term, accountability and transparency on behalf of tech companies is critical, as is vigilance on behalf of consumers."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.infoq.com/articles/algorithmic-integrity-ethics"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Preparing For AI Ethics And Explainability In 2020&lt;/h2&gt;
&lt;p&gt;"People distrust artificial intelligence and in some ways this makes  sense.  With the desire to create the best performing AI models, many organizations have prioritized complexity over the concepts of explainability and trust. As the world becomes more dependent on algorithms for making a wide range of decisions, technologies and business leaders will be tasked with explaining how a model selected its outcome. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/maribellopez/2020/01/21/preparing-for-ai-ethics-and-explainability-in-2020/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The battle for ethical AI at the world’s biggest machine-learning conference&lt;/h2&gt;
&lt;p&gt;"Bias and the prospect of societal harm increasingly plague artificial-intelligence research — but it’s not clear who should be on the lookout for these problems."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/d41586-020-00160-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Ethical Upside to Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;"if correctly designed, AI should clarify and amplify the ethical frameworks that U.S. military leaders already bring to war. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://warontherocks.com/2020/01/the-ethical-upside-to-artificial-intelligence/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The case for AI transparency requirements&lt;/h2&gt;
&lt;p&gt;"As AI technologies quickly and methodically climb out of the uncanny valley, customer service calls, website chatbots, and interactions on social media and in virtual reality may become progressively less evidently artificial."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.brookings.edu/research/the-case-for-ai-transparency-requirements/amp/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 15 - Jan 21</title><link href="/ai-ethics-news-roundup-jan21-2020.html" rel="alternate"></link><published>2020-01-21T09:20:00+01:00</published><updated>2020-01-21T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-21:/ai-ethics-news-roundup-jan21-2020.html</id><summary type="html">&lt;p&gt;Our weekly news roundup for Jan 15 - Jan 21: Including AI in Finance, political campaigns, social profiling, the workplace, the problem of identifying health data, and some pieces on explainability and governance.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;Everything We Do Tells Something About Our Health - Including Our Taste in Music&lt;/h2&gt;
&lt;p&gt;Much of our data exhaust is rich enough that it can be used to generate accurate inferences about us in domains unrelated to context of collection. Machine learning analysis of large sets of data about our music listening habits can reveal information about our physical and mental health. This information, if directly present, would be subject to regulatory and ethical norms. &lt;/p&gt;
&lt;p&gt;"A crucial question is: what exactly is health data? Does it comprise the data from medical treatment and investigation, or is it more than that? In his research, Hooghiemstra refers to medical investigations, also mentioning wearables and apps. But where, precisely, is the line between what is and what is not health data?"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://dataethics.eu/health-data-can-include-your-taste-in-music/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI – Enabled Innovation, Part 1: Regulatory Intervention &amp;amp; AI in Financial Services&lt;/h2&gt;
&lt;p&gt;The first of a two-part series on AI in Finance: &lt;/p&gt;
&lt;p&gt;"Financial services regulators are promoting principles and giving guidance through public statements to set the expectation that firms need to ensure their governance model is fit for purpose when applied to AI enabled innovation. In time, these regulators will become more proactive in asking firms to demonstrate they fully understand their data assets and to explain how that data is exploited and how the associated risk is mitigated when using AI – enabled technologies. Financial services firms should develop a coherent AI strategy now in a way that anticipates how they will answer that question when it inevitably comes."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyslegaledge.com/2020/01/ai-enabled-innovation-part-1-regulatory-intervention-ai-in-financial-services/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Towards ethical and socio-legal governance in AI&lt;/h2&gt;
&lt;p&gt;"Many high-level ethics guidelines for AI have been produced in the past few years. It is time to work towards concrete policies within the context of existing moral, legal and cultural values, say Andreas Theodorou and Virginia Dignum."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/s42256-019-0136-y"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Artificial intelligence: DeepMind unlocks secrets of human brain using AI learning technique&lt;/h2&gt;
&lt;p&gt;Researchers at Google-owned DeepMind discovered that a recent development in computer science regarding reinforcement learning could be applied to how the brain’s dopamine system works.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/artificial-intelligence-deepmind-ai-human-brain-neuroscience-a9286661.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Digital Political Ethics: Aligning Principles with Practice&lt;/h2&gt;
&lt;p&gt;"This report is the fruit of a bipartisan report to identify areas of agreement among key stakeholders concerning ethical principles and best practices in the conduct of digital campaigning in the United States. Although many have raised concerns about the potential for digital technologies to weaken or undermine democracy, the voices of digital political practitioners are largely absent from this discussion." &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://citapdigitalpolitics.com/?page_id=1911"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithms at Work: The New Contested Terrain of Control&lt;/h2&gt;
&lt;p&gt;"We find that algorithmic control in theworkplace operates through six main mechanisms, which we call the“6Rs”—employers can use algorithms to direct workers by restrictingand recommending, evaluate workers by recording and rating, and discipline workers by replacing and rewarding. We also discuss several key insights regarding algorithmic control."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://www.angelechristin.com/wp-content/uploads/2020/01/Algorithms-at-Work_Annals.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Black-Boxed Politics:Opacity is a Choice in AI Systems&lt;/h2&gt;
&lt;p&gt;"There are many myths and misconceptions about AI, but in cases where these systems are being used in sensitive, high-risk scenarios such as public health and criminal justice, arguably the most damaging msconception is that these systems are ‘black boxes’ about which we simply cannot know anything."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@szymielewicz/black-boxed-politics-cebc0d5a54ad"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The US just released 10 principles that it hopes will make AI safer&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Booker beware: Airbnb can scan your online life to see if you’re a suitable guest&lt;/h2&gt;
&lt;p&gt;"Details have emerged of its “trait analyser” software built to scour the web to assess users’ “trustworthiness and compatibility” as well as their “behavioural and personality traits” in a bid to forecast suitability to rent a property. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.standard.co.uk/tech/airbnb-software-scan-online-life-suitable-guest-a4325551.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Beyond Bias: Contextualizing “Ethical AI” Within the History of Exploitation and Innovation in Medical Research&lt;/h2&gt;
&lt;p&gt;This is from late December, but it just reached us and is worth a read: "It’s time for us to move beyond “bias” as the anchor point for our efforts to build ethical and fair algorithms."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.media.mit.edu/articles/beyond-bias-contextualizing-ethical-ai-within-the-history-of-exploitation-and-innovation-in-medical-research/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 1 - Jan 15</title><link href="/ai-ethics-news-roundup-jan1-jan15-2020.html" rel="alternate"></link><published>2020-01-15T09:20:00+01:00</published><updated>2020-01-15T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-15:/ai-ethics-news-roundup-jan1-jan15-2020.html</id><summary type="html">&lt;p&gt;Happy New Year! This roundup includes the Artificial Intelligence Video Interview Act, AI in diagnostic systems, Deep Fakes, How to build an ethical career, and more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;Happy New Year! &lt;/p&gt;
&lt;h2&gt;The use of AI in job search processes and tools&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.vox.com/recode/2020/1/1/21043000/artificial-intelligence-job-applications-illinios-video-interivew-act"&gt;Artificial Intelligence Video Interview Act&lt;/a&gt; takes effect Jan 1 2020. Many video interview tools now incorporate some kind of AI screening to generate reports on candidates, a practice which raises several AI ethics and safety concerns. &lt;/p&gt;
&lt;p&gt;A new article at the WSJ discusses some of these: &lt;a href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;How Job Interviews Will Transform in the Next Decade&lt;/a&gt;. "Recruiters using AI and virtual-reality simulations may hire based on a candidate’s behaviour, personality traits and physiological responses—no resumes needed"&lt;/p&gt;
&lt;p&gt;And you can already purchase countermeasures, if you can afford it. &lt;a href="https://www.scmp.com/news/asia/east-asia/article/3045795/south-korean-job-applicants-are-learning-trick-ai-hiring-bots"&gt;South Korean job applicants are learning to trick AI hiring bots that use facial recognition tech&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a topic EI has been thinking deeply about for a few months, and we have a detailed blog article in the works ... stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Comparison of the GoogleHealth breast AI paper against the RSNA Editorial Board recommendations for Assessing Radiology Research on Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;Are we holding AI diagnostic tools to the right standards? A great tweet from &lt;a href="http://twitter.com/DrHughHarvey"&gt;@DrHughHarvey&lt;/a&gt;. In the replies there's another good piece from October &lt;a href="https://www.nature.com/articles/s41598-019-51503-3"&gt;Using artificial intelligence to read chest radiographs for tuberculosis detection: A multi-site evaluation of the diagnostic accuracy of three deep learning systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/DrHughHarvey/status/1213548573071204352"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Technology Can't Fix Algorithmic Injustice&lt;/h2&gt;
&lt;p&gt;We need greater democratic oversight of AI not just from developers and designers, but from all members of society.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="http://bostonreview.net/science-nature-politics/annette-zimmermann-elena-di-rosa-hochan-kim-technology-cant-fix-algorithmic"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data and Justice in 2019 — Who can afford big tech, and who can live without it?&lt;/h2&gt;
&lt;p&gt;"What we see is that while you may not have access to the cloud, you can still be tracked and controlled by your government’s AI. This increase in the reach of data and analytics is even more noticeable for those who don’t have a country to call home. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://globaldatajustice.org/2020-01-01-data-and-justice-2019/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Deepfakes: The Looming Threat Of 2020&lt;/h2&gt;
&lt;p&gt;Deepfakes have been lurking on the internet for years now. But in 2020 the AI technology will become a powerful weapon for misinformation, fraud, and other crimes.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.designnews.com/artificial-intelligence/deepfakes-looming-threat-2020/109800999062105"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How will we remain USEFUL HUMANS? A longer post on the future of work, jobs, education and training&lt;/h2&gt;
&lt;p&gt;"As human intelligence (HI) encounters AI, will humans really become useless? Will all this progress be heaven (working only four hours per day, four days a week, but for the same money), or will it be hell (50% unemployment, rampant inequality and global civil unrest)? Or will it be both i.e. a kind of #hellven? Let’s have a look!"&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thefuturesagency.com/2020/01/03/how-will-we-remain-useful-humans-a-longer-post-on-the-future-of-work-jobs-education-and-training/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Building an Ethical Career&lt;/h2&gt;
&lt;p&gt;Not an AI ethics piece, but some interesting reflections on how to build ethical awareness into our professional development. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hbr.org/2020/01/building-an-ethical-career"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The US just released 10 principles that it hopes will make AI safer&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Human-like robots spark fear in users according to researchers&lt;/h2&gt;
&lt;p&gt;"Japanese researcher Masahiro Mori’s “uncanny valley” theory, which he developed in the 1970s, states that we react positively to robots if they have physical features familiar to us -but they disturb us if they start looking too much like us."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.techspark.co/blog/2020/01/02/human-like-robots-spark-fear-in-users-according-to-researchers/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 17-24</title><link href="/ai-ethics-news-roundup-dec17-dec24.html" rel="alternate"></link><published>2019-12-24T09:20:00+01:00</published><updated>2019-12-24T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-24:/ai-ethics-news-roundup-dec17-dec24.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 17 - Dec 24&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;A great tweet from Arvind Narayanan&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/random_walker"&gt;Arvind Narayanan&lt;/a&gt; writes:&lt;/p&gt;
&lt;p&gt;"If you think there's too much yelling about algorithmic bias, here's an analogy. By the mid 90s the privacy community knew there was a huge problem. But it took two decades of yelling and a million privacy disasters before the public and policy makers started taking it seriously."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://twitter.com/random_walker/status/1208050796476215296"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Persons yet Unknown: Animals, Chimeras, Artificial Intelligence and Beyond&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/grok_"&gt;Kate Darling&lt;/a&gt; and &lt;a href="https://twitter.com/PKathrani"&gt;Paresh Kathrani&lt;/a&gt; in an fascinating discussion of robotics and AI and The Animal Law Conference. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.youtube.com/watch?v=dEFI05Gtalc"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Implementing Ethics Into Artificial Intelligence: A Contribution, From A Legal Perspective, To The Development Of An Ai Governance Regime&lt;/h2&gt;
&lt;p&gt;This is a new article added to a great issue of the Duke Law and Technology Review that came out in August, a &lt;a href="https://scholarship.law.duke.edu/dltr/vol18/iss1/"&gt;Symposium for John Perry Barlow&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;"This Article advocates for the need to conduct in-depth risk-benefit-assessments with regard to the use of AI and autonomous systems. This Article points out major concerns in relation to AI and autonomous systems such as likely job losses, causation of damages, lack of transparency, increasing loss of humanity in social relationships, loss of privacy and personal autonomy, potential information biases and the error proneness, and susceptibility to manipulation of AI and autonomous systems. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://scholarship.law.duke.edu/dltr/vol18/iss1/17/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Researchers were about to solve AI’s black box problem, then the lawyers got involved&lt;/h2&gt;
&lt;p&gt;When things go wrong and AI runs amok, the lawyers will be there to tell us the most company-friendly version of what happened. Most importantly, they’ll protect companies from having to share how their AI systems work.&lt;/p&gt;
&lt;p&gt;We’re trading a technical black box for a legal one. Somehow, this seems even more unfair.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://thenextweb.com/artificial-intelligence/2019/12/17/researchers-were-about-to-solve-ais-black-box-problem-then-the-lawyers-got-involved/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Explainability and Adversarial Robustness for RNNs&lt;/h2&gt;
&lt;p&gt;"Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a common notion of adversarial robustness, and show that an adversarial training procedure can significantly and successfully reduce the attack surface."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/explainability-and-adversarial-robustness-for-rnns"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;This AI researcher is trying to ward off a reproducibility crisis&lt;/h2&gt;
&lt;p&gt;Joelle Pineau doesn’t want science’s reproducibility crisis to come to artificial intelligence (AI).&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nature.com/articles/d41586-019-03895-5"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ethics In AI: Why Values For Data Matter&lt;/h2&gt;
&lt;p&gt;An argument for better corporate governance around AI and data. Corporations should "treat data as an asset .... the same way organizations treat inventory, fleet, and manufacturing assets. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.forbes.com/sites/sap/2019/12/18/ethics-in-ai/#2d7dd5285af4"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The Virtuous Circle of Trusted AI: Turning Ethical and Transparent AI Into a Competitive Advantage&lt;/h2&gt;
&lt;p&gt;"Most large organizations today across the United States and Europe are talking about “duty of care” and AI (i.e. the duty to take care to refrain from causing another person injury or loss). We also hear a lot about the need for clear normative frameworks in areas such as driverless cars, drones, facial recognition, and algorithmic decisionmaking guidelines in public-facing services such as banking or retail. I shall be surprised if we will have this conversation again in two years’ time and legislation hasn’t already been seriously discussed or put in place."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.capgemini.com/research/the-virtuous-circle-of-trusted-ai-turning-ethical-and-transparent-ai-into-a-competitive-advantage-luciano-floridi/"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 10-17</title><link href="/ai-ethics-news-roundup-dec10-dec17.html" rel="alternate"></link><published>2019-12-17T09:20:00+01:00</published><updated>2019-12-17T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-17:/ai-ethics-news-roundup-dec10-dec17.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 10 - Dec 17&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Index 2019 Report&lt;/h2&gt;
&lt;p&gt;An independent initiative within Stanford University’s Human-Centered Artificial Intelligence Institute, the report is in its third year and is the result of a collaborative effort led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry, in collaboration with more than 35 sponsoring partners and data contributors. The purpose of the project is to ground the discussion on AI in data, serving practitioners, industry leaders, policymakers and funders, the general public and the media that informs it. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://hai.stanford.edu/news/introducing-ai-index-2019-report"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;When should we decline to write code? A small case study.&lt;/h2&gt;
&lt;p&gt;We picked this up on twitter when Emily Bender &lt;a href="https://twitter.com/ethicalai_co/status/1202638293269176321"&gt;tweeted&lt;/a&gt; that there was a task in an AI competition to create an AI that would solve problems involving the  "Prediction of Intellectual Ability and Personality Traits from Text". She's since &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;posted a thoughtful followup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an important problem. Technical and regulatory solutions should be augmented by professional codes of conduct and ethics if we want to ensure the safe and fair development of AI. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Do You Trust Jeff Bezos With Your Life? Tech Giants Like Amazon Are Getting into the Health Care Business&lt;/h2&gt;
&lt;p&gt;Would you trust the Tech Giants with your health data in exchange for more personalized and on-demand healthcare? This article covers the current initiative of telehealth by Amazon and dives into a few key implications that this new commodity would carry for society at large.&lt;/p&gt;
&lt;p&gt;"What health insurance companies, as well as employers who foot the bulk of the U.S.'s health care bill, especially fear from telehealth is that it's so easy to use that people will reach out more often for care. "It creates the risk that every little ache and pain results in a claim that has to be paid out," says the University of Pennsylvania's Asch. "Making people come into the office is health care rationing by inconvenience."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.newsweek.com/amazon-health-care-jeff-bezos-telemedicine-1475154"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A tug-of-war over biased AI&lt;/h2&gt;
&lt;p&gt;"A critical split divides AI reformers. On one side are the bias-fixers, who believe the systems can be purged of prejudice with a bit more math. (Big Tech is largely in this camp.) On the other side are the bias-blockers, who argue that AI has no place at all in some high-stakes decisions."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.axios.com/ai-bias-c7bf3397-a870-4152-9395-83b6bf1e6a67.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Emotion-detecting tech should be restricted by law&lt;/h2&gt;
&lt;p&gt;"The AI Now Institute says the field is "built on markedly shaky foundations".
Despite this, systems are on sale to help vet job seekers, test criminal suspects for signs of deception, and set insurance prices. "&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.bbc.co.uk/news/technology-50761116"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Would you let a Robot Take Care of Your Mother?&lt;/h2&gt;
&lt;p&gt;AI usage for social care is not a new concept. However, as it becomes more and more of a reality, we are forced to shift our questions from theoretical to personal.&lt;/p&gt;
&lt;p&gt;"Some worry robot care would carry a stigma:the potential of being seen as “not worth human company,” said one participant in a study of potential users with mild cognitive impairments."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.nytimes.com/2019/12/13/opinion/robot-caregiver-aging.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Owning Intelligence&lt;/h2&gt;
&lt;p&gt;The United States Patent and Trademark Office is trying to answer a very complicated question: who owns artificial intelligence?&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.cigionline.org/articles/owning-intelligence"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AI Ethics for Systemic Issues: A Structural Approach&lt;/h2&gt;
&lt;p&gt;"This paper calls for a "structural" approach to assessing AI’s effects inorder to understand and prevent such systemic risks where no individual can beheld accountable for the broader negative impacts. This is particularly relevantfor AI applied to systemic issues such as climate change and food security whichrequire political solutions and global cooperation. To properly address the widerange of AI risks and ensure ’AI for social good’, agency-focused policies must becomplemented by policies informed by a structural approach."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/48_aisg_neurips2019.pdf"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 2 - Dec 9</title><link href="/ai-ethics-news-roundup-dec-9.html" rel="alternate"></link><published>2019-12-09T09:20:00+01:00</published><updated>2019-12-09T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-09:/ai-ethics-news-roundup-dec-9.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 2 - Dec 9&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href=""&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;On the Legal Compatibility of Fairness Definitions&lt;/h2&gt;
&lt;p&gt;Although the article was on arxiv last week, the author publicized it this week, and it's a good one. There's poor alignment between operationalized definitions of fairness in machine learning and the legal definitions that may in fact apply to the deployment of these systems. &lt;/p&gt;
&lt;p&gt;"Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex socio-technical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts, as well as what both the legal and ML fairness communities can learn from these tensions. We focus this paper on U.S. anti-discrimination law since the ML fairness research community regularly references terms from this body of law."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://arxiv.org/abs/1912.00761"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;'Good' isn’t good enough&lt;/h2&gt;
&lt;p&gt;A critical reflection on the problems that arise when the pursuit of good is taken on as a technical objective too hastily, and why sustained and rigorous ethical reflection is a necessary if we want to have any confidence that such efforts will actually succeed. &lt;/p&gt;
&lt;p&gt;"Despite widespread enthusiasm among computer scientists to contribute to “socialgood,” the field’s efforts to promote good lack a rigorous foundation in politicsor social change. There is limited discourse regarding what “good” actuallyentails, and instead a reliance on vague notions of what aspects of society aregood or bad. Moreover, the field rarely considers the types of social changethat result from algorithmic interventions, instead following a “greedy algorithm”approach of pursuing &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.benzevgreen.com/wp-content/uploads/2019/11/19-ai4sg.pdf"&gt;Read More&lt;/a&gt;technology-centric incremental reform at all points."&lt;/p&gt;
&lt;h2&gt;New guidelines on the GDPR Right to Explanation&lt;/h2&gt;
&lt;p&gt;The UK’s Data Protection Authority just issued much-anticipated guidance that clarifies the complicated issue of the GDPR’s ‘right to explanation’. Here is some background on the issue and what the new &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://medium.com/arthur-ai/uk-gpdr-watchdog-says-explain-your-ai-373ef76d3c"&gt;Read More&lt;/a&gt;information means.&lt;/p&gt;
&lt;h2&gt;Ethics is Objective&lt;/h2&gt;
&lt;p&gt;Here are three arguments for the idea that ethics is subjective, presented with thoughtful rebuttals. This is a theme we took up in our last bog post, where we argued that there is a very large chunk of territory in tech ethics where ethical imperatives can be uncovered and agreed upon by sincere inquiry, &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.linkedin.com/pulse/ethics-objective-reid-blackman-ph-d-/?trackingId=B5NT8Dd1BMx16FH15vHvZQ%3D%3D"&gt;Read More&lt;/a&gt;even by those who disagree on more fundamental ethical and moral questions. &lt;/p&gt;
&lt;h2&gt;Online information of vaccines: information quality is an ethical responsibility of search engines&lt;/h2&gt;
&lt;p&gt;When health-related disinformation is available online, who is responsible? There's a growing backlash against the idea of platforms as "mere tools", but perhaps we should think the same of search engines. We don't usually think that a library is responsible for dangerous information in its books, but should we 
think differently about Google?&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.academia.edu/41168605/Online_information_of_vaccines_information_quality_is_an_ethical_responsibility_of_search_engines"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?&lt;/h2&gt;
&lt;p&gt;Multiple fairness constraints have been proposed in the literature, motivated by a range of concerns about how demographic groups might be treated unfairly by machine learning classifiers. In this work we consider a different motivation; learning from biased training data. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://deepai.org/publication/recovering-from-biased-data-can-fairness-constraints-improve-accuracy"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Datafication&lt;/h2&gt;
&lt;p&gt;In the course of researching and discussing AI ethics challenges, we might run across the claim while the rate and scope of our generation of data has increased, it can be understood on a continuum with the ways in which human activity have always left traces and records. This article on the concept of "datafication" argues against this, and shows several ways to understand what is distinctive about the new systems and actors that collect and use our data. &lt;/p&gt;
&lt;p&gt;"Datafication is not just the making of information, which, in one sense, human beings have been doing since the creation of symbols and writing. Rather, datafication is a contemporary phenomenon which refers to the quantification of human life through digital information, very often for economic value. This process has major social consequences. Disciplines such as political economy, critical data studies, software studies, legal theory, and—more recently— decolonial theory, have considered different aspects of those consequences to be important. Fundamental to all such approaches is the analysis of the &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://policyreview.info/concepts/datafication"&gt;Read More&lt;/a&gt;intersection of power and knowledge. "&lt;/p&gt;
&lt;h2&gt;Amazon ready to cash in on free access to NHS data&lt;/h2&gt;
&lt;p&gt;&lt;a class="readmore" href=https://www.thetimes.co.uk/article/amazon-ready-to-cash-in-on-free-access-to-nhs-data-bbzp52n5m""&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Nov 24 - Dec 1</title><link href="/ai-ethics-news-roundup-nov-24-dec-1.html" rel="alternate"></link><published>2019-12-02T09:20:00+01:00</published><updated>2019-12-02T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-02:/ai-ethics-news-roundup-nov-24-dec-1.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Nov 24 - Dec 1&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;p&gt;Every Tuesday we publish a list of links to articles and debates that have happened over the past week in the community, allowing you to stay as up-to-date as possible on developments and facts. We will often link to arguments from all sides of the debate, even if the opinions may be controversial. We would like to mention, however, that EI does not endorse any of the information published, all links are reflections of the author's opinions and not that of Ethical Intelligence.&lt;/p&gt;
&lt;h2&gt;The Second Wave of Algorithmic Accountability&lt;/h2&gt;
&lt;p&gt;"... the first wave of algorithmic accountability focuses on improving existing systems, a second wave of research has asked whether they should be used at all—and, if so, who gets to govern them".&lt;/p&gt;
&lt;p&gt;Frank Pasquale argues that we can distinguish the "... first wave of algorithmic accountability research and activism", which has targeted existing systems and helped illuminate urgent ethical concerns in the AI systems already online, from "...an emerging “second wave” of algorithmic accountability has begun to address more structural concerns.". &lt;/p&gt;
&lt;p&gt;"Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology". &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Machine Learning on Encrypted Data Without Decrypting It&lt;/h2&gt;
&lt;p&gt;"Recent breakthroughs in cryptography have made it practical to perform computation on data without ever decrypting it. In our example, the user would send encrypted data (e.g. images) to the cloud API, which would run the machine learning model and then return the encrypted answer. Nowhere was the user data decrypted and in particular the cloud provider does not have access to either the orignal image nor is it able to decrypt the prediction it computed."&lt;/p&gt;
&lt;p&gt;This application of homomorphic encryption systems might mitigate a number of data protection problems, even though it would still be imperative to ensure the data was collected ethically, and that the data itself is free from biases that are not understood and accounted for. &lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://juliacomputing.com/blog/2019/11/22/encrypted-machine-learning.html"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Tainted Data Can Teach Algorithms the Wrong Lessons&lt;/h2&gt;
&lt;p&gt;Security problems become ethics problems when vulnerabilities in software systems produce risks in their application that the stakeholders (both users, and the people in the social environment) are unable to understand, assess, and freely and knowingly accept. &lt;/p&gt;
&lt;p&gt;Adversarial input is a particularly powerful way to undermine machine learning systems and to cause them to behave in unexpected and unintended ways. "“Current deep-learning systems are very vulnerable to a variety of attacks, and the rush to deploy the technology in the real world is deeply concerning,” says Cristiano Giuffrida, an assistant professor at VU Amsterdam who studies computer security, and who previously discovered a major flaw with Intel chips affecting millions of computers."&lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://link.springer.com/article/10.1007%2Fs13347-019-00354-x"&gt;recent paper&lt;/a&gt; Luciano Floridi draws our attention to the extension of the practice of ethics dumping, "the export of unethical research practices to countries where there are weaker legal and ethical frameworks" into the digital realm. This is an ethical risk, but it is also a more basic risk to the quality of research. This article argues that there are also security problems with this sort of practice. "... some companies outsource the training of their AI systems, a practice known as machine learning as a service. This makes it far harder to guarantee that an algorithm has been developed securely."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.wired.com/story/tainted-data-teach-algorithms-wrong-lessons/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Algorithms, Automation, and News&lt;/h2&gt;
&lt;p&gt;A special issue of the Journal of Digital Journalism has been published on "Algorithms, Automation, and News".&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.tandfonline.com/doi/full/10.1080/21670811.2019.1685395"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Against Ethical AI: Guidelines and Self Interest&lt;/h2&gt;
&lt;p&gt;"In this paper we use the EU guidelines on ethical AI, and the responses to it, as a starting point to discuss the problems with our community's focus on such manifestos, principles, and sets of guidelines. We cover how industry and academia are at times complicit in ‘Ethics Washing’, how developing guidelines carries the risk of diluting our rights in practice, and downplaying the role of our own self interest. We conclude by discussing briefly the role of technical practice in ethics."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.halfwaytothefuture.org/programme/mcmillan-against-ethical-ai-guidelines-and-self-interest"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;5 Q’s for Anne Kao, Senior Technical Fellow at Boeing Research and Technology](https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/)&lt;/h2&gt;
&lt;p&gt;"The Center for Data Innovation spoke with Anne Kao, Senior Technical Fellow at Boeing Research and Technology. Kao discussed how she uses machine learning to analyze maintenance reports and how philosophy influences how she approaches data science."&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/"&gt;Read More&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Ukraine denounces Apple for calling Crimea part of Russia in apps](https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O)&lt;/h2&gt;
&lt;p&gt;"Reuters reporters in Moscow who typed the name of the Crimean provincial capital Simferopol into Apple’s Maps and Weather apps on Wednesday saw it displayed as “Simferopol, Crimea, Russia”. Users elsewhere — including in Ukraine’s capital Kiev and in Crimea itself — see locations in Crimea displayed without specifying which country they belong to. "&lt;/p&gt;
&lt;p&gt;One might wonder if the technical specifications in the ticket for the engineering work that was involved discussed the political and ethical implications. It's perhaps difficult to imagine that something of this magnitude wasn't remarked upon, but on the other, so much engineering work in software proceeds as though it happens in at least partial isolation from the downstream social and political environment.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O"&gt;Read More&lt;/a&gt;&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry></feed>