<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ethical Intelligence - news</title><link href="/" rel="alternate"></link><link href="/feeds/news.atom.xml" rel="self"></link><id>/</id><updated>2019-12-17T09:20:00+01:00</updated><entry><title>Weekly AI Ethics News Roundup: Dec 10-17</title><link href="/ai-ethics-news-roundup-dec10-dec17.html" rel="alternate"></link><published>2019-12-17T09:20:00+01:00</published><updated>2019-12-17T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-17:/ai-ethics-news-roundup-dec10-dec17.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 10 - Dec 17&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://hai.stanford.edu/news/introducing-ai-index-2019-report"&gt;AI Index 2019 Report&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;An independent initiative within Stanford University’s Human-Centered Artificial Intelligence Institute, the report is in its third year and is the result of a collaborative effort led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry, in collaboration with more than 35 sponsoring partners and data contributors. The purpose of the project is to ground the discussion on AI in data, serving practitioners, industry leaders, policymakers and funders, the general public and the media that informs it. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;When should we decline to write code? A small case study.&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We picked this up on twitter when Emily Bender &lt;a href="https://twitter.com/ethicalai_co/status/1202638293269176321"&gt;tweeted&lt;/a&gt; that there was a task in an AI competition to create an AI that would solve problems involving the  "Prediction of Intellectual Ability and Personality Traits from Text". She's since &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;posted a thoughtful followup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an important problem. Technical and regulatory solutions should be augmented by professional codes of conduct and ethics if we want to ensure the safe and fair development of AI. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.newsweek.com/amazon-health-care-jeff-bezos-telemedicine-1475154"&gt;Do You Trust Jeff Bezos With Your Life? Tech Giants Like Amazon Are Getting into the Health Care Business&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Would you trust the Tech Giants with your health data in exchange for more personalized and on-demand healthcare? This article covers the current initiative of telehealth by Amazon and dives into a few key implications that this new commodity would carry for society at large.&lt;/p&gt;
&lt;p&gt;"What health insurance companies, as well as employers who foot the bulk of the U.S.'s health care bill, especially fear from telehealth is that it's so easy to use that people will reach out more often for care. "It creates the risk that every little ache and pain results in a claim that has to be paid out," says the University of Pennsylvania's Asch. "Making people come into the office is health care rationing by inconvenience."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.axios.com/ai-bias-c7bf3397-a870-4152-9395-83b6bf1e6a67.html"&gt;A tug-of-war over biased AI&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"A critical split divides AI reformers. On one side are the bias-fixers, who believe the systems can be purged of prejudice with a bit more math. (Big Tech is largely in this camp.) On the other side are the bias-blockers, who argue that AI has no place at all in some high-stakes decisions."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.bbc.co.uk/news/technology-50761116"&gt;Emotion-detecting tech should be restricted by law&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The AI Now Institute says the field is "built on markedly shaky foundations".
Despite this, systems are on sale to help vet job seekers, test criminal suspects for signs of deception, and set insurance prices. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.nytimes.com/2019/12/13/opinion/robot-caregiver-aging.html"&gt;Would you let a Robot Take Care of Your Mother? &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;AI usage for social care is not a new concept. However, as it becomes more and more of a reality, we are forced to shift our questions from theoretical to personal.&lt;/p&gt;
&lt;p&gt;"Some worry robot care would carry a stigma:the potential of being seen as “not worth human company,” said one participant in a study of potential users with mild cognitive impairments."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.cigionline.org/articles/owning-intelligence"&gt;Owning Intelligence&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The United States Patent and Trademark Office is trying to answer a very complicated question: who owns artificial intelligence?&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/48_aisg_neurips2019.pdf"&gt;AI Ethics for Systemic Issues: A Structural Approach&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"This paper calls for a "structural" approach to assessing AI’s effects inorder to understand and prevent such systemic risks where no individual can beheld accountable for the broader negative impacts. This is particularly relevantfor AI applied to systemic issues such as climate change and food security whichrequire political solutions and global cooperation. To properly address the widerange of AI risks and ensure ’AI for social good’, agency-focused policies must becomplemented by policies informed by a structural approach."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 2 - Dec 9</title><link href="/ai-ethics-news-roundup-dec-9.html" rel="alternate"></link><published>2019-12-09T09:20:00+01:00</published><updated>2019-12-09T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-09:/ai-ethics-news-roundup-dec-9.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 2 - Dec 9&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://arxiv.org/abs/1912.00761"&gt;On the Legal Compatibility of Fairness Definitions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Although the article was on arxiv last week, the author publicized it this week, and it's a good one. There's poor alignment between operationalized definitions of fairness in machine learning and the legal definitions that may in fact apply to the deployment of these systems. &lt;/p&gt;
&lt;p&gt;"Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex socio-technical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts, as well as what both the legal and ML fairness communities can learn from these tensions. We focus this paper on U.S. anti-discrimination law since the ML fairness research community regularly references terms from this body of law."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.benzevgreen.com/wp-content/uploads/2019/11/19-ai4sg.pdf"&gt;'Good' isn’t good enough&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A critical reflection on the problems that arise when the pursuit of good is taken on as a technical objective too hastily, and why sustained and rigorous ethical reflection is a necessary if we want to have any confidence that such efforts will actually succeed. &lt;/p&gt;
&lt;p&gt;"Despite widespread enthusiasm among computer scientists to contribute to “socialgood,” the field’s efforts to promote good lack a rigorous foundation in politicsor social change. There is limited discourse regarding what “good” actuallyentails, and instead a reliance on vague notions of what aspects of society aregood or bad. Moreover, the field rarely considers the types of social changethat result from algorithmic interventions, instead following a “greedy algorithm”approach of pursuing technology-centric incremental reform at all points."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/arthur-ai/uk-gpdr-watchdog-says-explain-your-ai-373ef76d3c"&gt;New guidelines on the GDPR Right to Explanation&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The UK’s Data Protection Authority just issued much-anticipated guidance that clarifies the complicated issue of the GDPR’s ‘right to explanation’. Here is some background on the issue and what the new information means.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.linkedin.com/pulse/ethics-objective-reid-blackman-ph-d-/?trackingId=B5NT8Dd1BMx16FH15vHvZQ%3D%3D"&gt;Ethics is Objective&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Here are three arguments for the idea that ethics is subjective, presented with thoughtful rebuttals. This is a theme we took up in our last bog post, where we argued that there is a very large chunk of territory in tech ethics where ethical imperatives can be uncovered and agreed upon by sincere inquiry, even by those who disagree on more fundamental ethical and moral questions. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.academia.edu/41168605/Online_information_of_vaccines_information_quality_is_an_ethical_responsibility_of_search_engines"&gt;Online information of vaccines: information quality is an ethical responsibility of search engines&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When health-related disinformation is available online, who is responsible? There's a growing backlash against the idea of platforms as "mere tools", but perhaps we should think the same of search engines. We don't usually think that a library is responsible for dangerous information in its books, but should we think differently about Google?&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://deepai.org/publication/recovering-from-biased-data-can-fairness-constraints-improve-accuracy"&gt;Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Multiple fairness constraints have been proposed in the literature, motivated by a range of concerns about how demographic groups might be treated unfairly by machine learning classifiers. In this work we consider a different motivation; learning from biased training data. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://policyreview.info/concepts/datafication"&gt;Datafication&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the course of researching and discussing AI ethics challenges, we might run across the claim while the rate and scope of our generation of data has increased, it can be understood on a continuum with the ways in which human activity have always left traces and records. This article on the concept of "datafication" argues against this, and shows several ways to understand what is distinctive about the new systems and actors that collect and use our data. &lt;/p&gt;
&lt;p&gt;"Datafication is not just the making of information, which, in one sense, human beings have been doing since the creation of symbols and writing. Rather, datafication is a contemporary phenomenon which refers to the quantification of human life through digital information, very often for economic value. This process has major social consequences. Disciplines such as political economy, critical data studies, software studies, legal theory, and—more recently— decolonial theory, have considered different aspects of those consequences to be important. Fundamental to all such approaches is the analysis of the intersection of power and knowledge. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.techuk.org/insights/news/item/16447-welcome-to-techuk-digital-ethics-week"&gt;This is techUK Digital Ethics Week&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="https://www.thetimes.co.uk/article/amazon-ready-to-cash-in-on-free-access-to-nhs-data-bbzp52n5m"&gt;Amazon ready to cash in on free access to NHS data&lt;/a&gt;&lt;/h2&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Nov 24 - Dec 1</title><link href="/ai-ethics-news-roundup-nov-24-dec-1.html" rel="alternate"></link><published>2019-12-02T09:20:00+01:00</published><updated>2019-12-02T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-02:/ai-ethics-news-roundup-nov-24-dec-1.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Nov 24 - Dec 1&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to our first EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;The Second Wave of Algorithmic Accountability&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"... the first wave of algorithmic accountability focuses on improving existing systems, a second wave of research has asked whether they should be used at all—and, if so, who gets to govern them".&lt;/p&gt;
&lt;p&gt;Frank Pasquale argues that we can distinguish the "... first wave of algorithmic accountability research and activism", which has targeted existing systems and helped illuminate urgent ethical concerns in the AI systems already online, from "...an emerging “second wave” of algorithmic accountability has begun to address more structural concerns.". &lt;/p&gt;
&lt;p&gt;"Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology". &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://juliacomputing.com/blog/2019/11/22/encrypted-machine-learning.html"&gt;Machine Learning on Encrypted Data Without Decrypting It&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Recent breakthroughs in cryptography have made it practical to perform computation on data without ever decrypting it. In our example, the user would send encrypted data (e.g. images) to the cloud API, which would run the machine learning model and then return the encrypted answer. Nowhere was the user data decrypted and in particular the cloud provider does not have access to either the orignal image nor is it able to decrypt the prediction it computed."&lt;/p&gt;
&lt;p&gt;This application of homomorphic encryption systems might mitigate a number of data protection problems, even though it would still be imperative to ensure the data was collected ethically, and that the data itself is free from biases that are not understood and accounted for. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.wired.com/story/tainted-data-teach-algorithms-wrong-lessons/"&gt;Tainted Data Can Teach Algorithms the Wrong Lessons&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Security problems become ethics problems when vulnerabilities in software systems produce risks in their application that the stakeholders (both users, and the people in the social environment) are unable to understand, assess, and freely and knowingly accept. &lt;/p&gt;
&lt;p&gt;Adversarial input is a particularly powerful way to undermine machine learning systems and to cause them to behave in unexpected and unintended ways. "“Current deep-learning systems are very vulnerable to a variety of attacks, and the rush to deploy the technology in the real world is deeply concerning,” says Cristiano Giuffrida, an assistant professor at VU Amsterdam who studies computer security, and who previously discovered a major flaw with Intel chips affecting millions of computers."&lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://link.springer.com/article/10.1007%2Fs13347-019-00354-x"&gt;recent paper&lt;/a&gt; Luciano Floridi draws our attention to the extension of the practice of ethics dumping, "the export of unethical research practices to countries where there are weaker legal and ethical frameworks" into the digital realm. This is an ethical risk, but it is also a more basic risk to the quality of research. This article argues that there are also security problems with this sort of practice. "... some companies outsource the training of their AI systems, a practice known as machine learning as a service. This makes it far harder to guarantee that an algorithm has been developed securely."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.tandfonline.com/doi/full/10.1080/21670811.2019.1685395"&gt;Algorithms, Automation, and News&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A special issue of the Journal of Digital Journalism has been published on "Algorithms, Automation, and News".&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.halfwaytothefuture.org/programme/mcmillan-against-ethical-ai-guidelines-and-self-interest"&gt;Against Ethical AI: Guidelines and Self Interest&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"In this paper we use the EU guidelines on ethical AI, and the responses to it, as a starting point to discuss the problems with our community's focus on such manifestos, principles, and sets of guidelines. We cover how industry and academia are at times complicit in ‘Ethics Washing’, how developing guidelines carries the risk of diluting our rights in practice, and downplaying the role of our own self interest. We conclude by discussing briefly the role of technical practice in ethics."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/"&gt;5 Q’s for Anne Kao, Senior Technical Fellow at Boeing Research and Technology&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The Center for Data Innovation spoke with Anne Kao, Senior Technical Fellow at Boeing Research and Technology. Kao discussed how she uses machine learning to analyze maintenance reports and how philosophy influences how she approaches data science."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O"&gt;Ukraine denounces Apple for calling Crimea part of Russia in apps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Reuters reporters in Moscow who typed the name of the Crimean provincial capital Simferopol into Apple’s Maps and Weather apps on Wednesday saw it displayed as “Simferopol, Crimea, Russia”. Users elsewhere — including in Ukraine’s capital Kiev and in Crimea itself — see locations in Crimea displayed without specifying which country they belong to. "&lt;/p&gt;
&lt;p&gt;One might wonder if the technical specifications in the ticket for the engineering work that was involved discussed the political and ethical implications. It's perhaps difficult to imagine that something of this magnitude wasn't remarked upon, but on the other, so much engineering work in software proceeds as though it happens in at least partial isolation from the downstream social and political environment.&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry></feed>