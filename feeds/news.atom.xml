<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ethical Intelligence - news</title><link href="/" rel="alternate"></link><link href="/feeds/news.atom.xml" rel="self"></link><id>/</id><updated>2020-02-04T09:20:00+01:00</updated><entry><title>Weekly AI Ethics News Roundup: Jan 28 - Feb 4</title><link href="/ai-ethics-news-roundup-jan28-feb4-2020.html" rel="alternate"></link><published>2020-02-04T09:20:00+01:00</published><updated>2020-02-04T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-02-04:/ai-ethics-news-roundup-jan28-feb4-2020.html</id><summary type="html">&lt;h1&gt;AIEthics news for Jan 28 - Feb 4 featuring health care and drug discovery (@exscientialtd and a great twitter thread from @DorotheaBaur), privacy (@BernardMarr), facial recognition (@castrotech @itifdc @mclaughlintech), impact assessment (@Rafael_A_Calvo), comment on EU AI regs from @datainnovation and more!&lt;/h1&gt;</summary><content type="html">&lt;h2&gt;&lt;a href="https://www.techjuice.pk/exscientia-an-oxford-based-biotech-company-is-going-to-test-the-ai-designed-medicine-on-humans/"&gt;Would you take a drug discovered by artificial intelligence?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/DorotheaBaur/status/1223549933506191363"&gt;@DorotheaBaur sparked an interesting reaction on Twitter&lt;/a&gt;, where in a climate of frequent AI news that raises ethical concerns, she asked if we might agree that this was a good example an applicaiton of AI that was not ethically problematic. &lt;/p&gt;
&lt;p&gt;"The British startup Exscientia claims it has developed the first medication created using artificial intelligence that will be clinically tested on humans. The medication, which is meant to treat obsessive-compulsive disorder, took less than a year from conception to trial-ready capsule. Human trials are set to begin in March, but would you take a drug designed using artificially intelligent software?"&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.forbes.com/sites/saibala/2020/02/03/artificial-intelligence-is-not-ready-for-the-intricacies-of-radiology/#312ac17867eb"&gt;Artificial Intelligence Is Not Ready For The Intricacies Of Radiology &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"...while much of the theoretical basis for AI in the practice of radiology is extremely exciting, the reality is that the field has not yet fully embraced it. The most significant issue is that the technology simply isn’t ready, as many of the existing systems have not yet been matured to compute and manage larger data sets or work in more general practice and patient settings, and thus, are not able to perform as promised. Other issues exist on the ethical aspects of AI. Given the sheer volume of data required to both train and perfect these systems, as well as the immense data collection that these systems will engage in once fully mainstream, key stakeholders are raising fair concerns and the call for strict ethical standards to be put into place, simultaneous to the technological development of these systems."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.repository.cam.ac.uk/handle/1810/300852"&gt;Advancing impact assessment for intelligent systems&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"We discuss how the EIA provides a partial blueprint for what we call a Human Impact Assessment for Technology (HIAT) and how more recent algorithmic and data protection impact assessment initiatives can contribute. We also discuss how ethical frameworks for such a human impact assessment could draw on recently established AI ethics principles. We argue that this approach will help build trust in an industry facing increasing criticism and scrutiny."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.forbes.com/sites/bernardmarr/2020/01/31/what-is-a-data-passport-building-trust-data-privacy-and-security-in-the-cloud/#76c416f75843"&gt;What Is A Data Passport: Building Trust, Data Privacy And Security In The Cloud &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Data passports allow you to extend the encryption technology that used to be only available on a physical mainframe to cloud computing. Each piece of data in the cloud has a passport assigned to it, and with the passport, you can verify if the data is misused, if the passport is still valid, etc. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.datainnovation.org/2020/02/how-the-eu-should-revise-its-ai-white-paper-before-it-is-published/"&gt;How the EU Should Revise its AI White Paper Before it is Published&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The European Commission is planning to release a white paper to support the development and uptake of artificial intelligence (AI). Early drafts of this white paper suggest that the Commission may call for additional AI regulations that would make it more expensive and more difficult for European businesses to use AI systems in many areas of the economy. Given the EU’s desire to be a leader in AI, and to use AI to bolster its global competitiveness, the Commission should avoid heavy-handed rules that would slow adoption of this emerging technology."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://itif.org/publications/2020/01/27/critics-were-wrong-nist-data-shows-best-facial-recognition-algorithms"&gt;The Critics Were Wrong: NIST Data Shows the Best Facial Recognition Algorithms Are Neither Racist Nor Sexist&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"NIST assessed the false positive and false-negative rates of algorithms using four types of images, including mugshots, application photographs from individuals applying for immigration benefits, visa photographs, and images taken of travelers entering the United States. NIST’s report reveals that: &lt;/p&gt;
&lt;p&gt;a) The most accurate identification algorithms have “undetectable” differences between demographic groups
b) The most accurate verification algorithms have low false positives and false negatives across most demographic groups
c) Algorithms can have different error rates for different demographics but still be highly accurate"&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://deepai.org/publication/cor-gan-correlation-capturing-convolutional-neural-networks-for-generating-synthetic-healthcare-records"&gt;COR-GAN: Correlation-Capturing Convolutional Neural Networks for Generating Synthetic Healthcare Records&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"In this paper, we propose a novel framework called correlation-capturing Generative Adversarial Network (corGAN), to generate synthetic healthcare records. In corGAN we utilize Convolutional Neural Networks to capture the correlations between adjacent medical features in the data representation space by combining Convolutional Generative Adversarial Networks and Convolutional Autoencoders"&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.vox.com/recode/2020/1/25/21080749/surveillance-school-artificial-intelligence-facial-recognition"&gt;New surveillance AI can tell schools where students are and where they’ve been&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Not all AI being used by schools is facial recognition. That doesn’t mean the tech doesn’t come with privacy risks. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://keyahconsulting.com/connected-cots-talking-teddies-and-the-rise-of-the-algorithmic-child/"&gt;Connected cots, talking teddies, and the rise of the algorithmic child&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Digital technologies are now a ubiquitous part of our daily lives. And questions remain as to how these technologies are reshaping how we experience the world around us, and how the world around us is being reshaped. One area this is being played out is in the family – in changing the experience of not only childhood, but what constitutes good parenting."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 21 - Jan 28</title><link href="/ai-ethics-news-roundup-jan21-jan28-2020.html" rel="alternate"></link><published>2020-01-28T09:20:00+01:00</published><updated>2020-01-28T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-28:/ai-ethics-news-roundup-jan21-jan28-2020.html</id><summary type="html">&lt;p&gt;Latest AIEthics news: AI in health (@jessRmorley @Floridi), ethics-bashing (@Elibietti), value-alignment (@IasonGabriel), bias (@SimonHegelich @SciOrestis @thyjuancarlos @FabienneMarco), explainability from(@MaribelLopez) and more from @LloydDanzig, @alexcengler and @LizzieGibney!&lt;/p&gt;</summary><content type="html">&lt;h2&gt;&lt;a href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)32975-7/fulltext"&gt;An ethically mindful approach to AI for health care&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Health-care systems worldwide face increasing demand, a rise in chronic disease, and resource constraints. At the same time, the use of digital health technologies in all care settings has led to an expansion of data. These data, if harnessed appropriately, could enable health-care providers to target the causes of ill-health and monitor the effectiveness of preventions and interventions. For this reason, policy makers, politicians, clinical entrepreneurs, and computer and data scientists argue that a key part of health-care solutions will be artificial Intelligence (AI), particularly machine learning."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372860"&gt;From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, "ethics" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called "ethics washing" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in "ethics bashing." This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment"&gt;Artificial Intelligence, Values and Alignment&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372843"&gt;Bias in Word Embeddings&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.infoq.com/articles/algorithmic-integrity-ethics"&gt;The Road to Artificial Intelligence: An Ethical Minefield&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"There are a number of ethical dilemmas woven inextricably into the field of Artificial Intelligence, many of which are often overlooked, even within the engineering community. Even the best intentions are often not enough to guarantee solutions free from unintended or undesired results, as humans can accidentally encode biases into AI engines and malicious actors can exploit flaws in models. In the short-term, accountability and transparency on behalf of tech companies is critical, as is vigilance on behalf of consumers."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.forbes.com/sites/maribellopez/2020/01/21/preparing-for-ai-ethics-and-explainability-in-2020/"&gt;Preparing For AI Ethics And Explainability In 2020 &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"People distrust artificial intelligence and in some ways this makes  sense.  With the desire to create the best performing AI models, many organizations have prioritized complexity over the concepts of explainability and trust. As the world becomes more dependent on algorithms for making a wide range of decisions, technologies and business leaders will be tasked with explaining how a model selected its outcome. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.nature.com/articles/d41586-020-00160-y"&gt;The battle for ethical AI at the world’s biggest machine-learning conference&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Bias and the prospect of societal harm increasingly plague artificial-intelligence research — but it’s not clear who should be on the lookout for these problems."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://warontherocks.com/2020/01/the-ethical-upside-to-artificial-intelligence/"&gt;The Ethical Upside to Artificial Intelligence&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"if correctly designed, AI should clarify and amplify the ethical frameworks that U.S. military leaders already bring to war. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.brookings.edu/research/the-case-for-ai-transparency-requirements/amp/"&gt;The case for AI transparency requirements&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"As AI technologies quickly and methodically climb out of the uncanny valley, customer service calls, website chatbots, and interactions on social media and in virtual reality may become progressively less evidently artificial."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 15 - Jan 21</title><link href="/ai-ethics-news-roundup-jan21-2020.html" rel="alternate"></link><published>2020-01-21T09:20:00+01:00</published><updated>2020-01-21T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-21:/ai-ethics-news-roundup-jan21-2020.html</id><summary type="html">&lt;p&gt;Our weekly news roundup for Jan 15 - Jan 21: Including AI in Finance, political campaigns, social profiling, the workplace, the problem of identifying health data, and some pieces on explainability and governance.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Our weekly news roundup for Jan 15 - Jan 21: Including AI in Finance, political campaigns, social profiling, the workplace, the problem of identifying health data, and some pieces on explainability and governance. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://dataethics.eu/health-data-can-include-your-taste-in-music/"&gt;Everything We Do Tells Something About Our Health - Including Our Taste in Music&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Much of our data exhaust is rich enough that it can be used to generate accurate inferences about us in domains unrelated to context of collection. Machine learning analysis of large sets of data about our music listening habits can reveal information about our physical and mental health. This information, if directly present, would be subject to regulatory and ethical norms. &lt;/p&gt;
&lt;p&gt;"A crucial question is: what exactly is health data? Does it comprise the data from medical treatment and investigation, or is it more than that? In his research, Hooghiemstra refers to medical investigations, also mentioning wearables and apps. But where, precisely, is the line between what is and what is not health data?"&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.technologyslegaledge.com/2020/01/ai-enabled-innovation-part-1-regulatory-intervention-ai-in-financial-services/"&gt;AI – Enabled Innovation, Part 1: Regulatory Intervention &amp;amp; AI in Financial Services&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The first of a two-part series on AI in Finance: &lt;/p&gt;
&lt;p&gt;"Financial services regulators are promoting principles and giving guidance through public statements to set the expectation that firms need to ensure their governance model is fit for purpose when applied to AI enabled innovation. In time, these regulators will become more proactive in asking firms to demonstrate they fully understand their data assets and to explain how that data is exploited and how the associated risk is mitigated when using AI – enabled technologies. Financial services firms should develop a coherent AI strategy now in a way that anticipates how they will answer that question when it inevitably comes."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.nature.com/articles/s42256-019-0136-y"&gt;Towards ethical and socio-legal governance in AI&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Many high-level ethics guidelines for AI have been produced in the past few years. It is time to work towards concrete policies within the context of existing moral, legal and cultural values, say Andreas Theodorou and Virginia Dignum."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/artificial-intelligence-deepmind-ai-human-brain-neuroscience-a9286661.html"&gt;Artificial intelligence: DeepMind unlocks secrets of human brain using AI learning technique&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Researchers at Google-owned DeepMind discovered that a recent development in computer science regarding reinforcement learning could be applied to how the brain’s dopamine system works.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://citapdigitalpolitics.com/?page_id=1911"&gt;Digital Political Ethics: Aligning Principles with Practice&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"This report is the fruit of a bipartisan report to identify areas of agreement among key stakeholders concerning ethical principles and best practices in the conduct of digital campaigning in the United States. Although many have raised concerns about the potential for digital technologies to weaken or undermine democracy, the voices of digital political practitioners are largely absent from this discussion." &lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.angelechristin.com/wp-content/uploads/2020/01/Algorithms-at-Work_Annals.pdf"&gt;Algorithms at Work: The New Contested Terrain of Control&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"We find that algorithmic control in theworkplace operates through six main mechanisms, which we call the“6Rs”—employers can use algorithms to direct workers by restrictingand recommending, evaluate workers by recording and rating, and discipline workers by replacing and rewarding. We also discuss several key insights regarding algorithmic control."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/@szymielewicz/black-boxed-politics-cebc0d5a54ad"&gt;Black-Boxed Politics:Opacity is a Choice in AI Systems&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"There are many myths and misconceptions about AI, but in cases where these systems are being used in sensitive, high-risk scenarios such as public health and criminal justice, arguably the most damaging msconception is that these systems are ‘black boxes’ about which we simply cannot know anything."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;The US just released 10 principles that it hopes will make AI safer&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.standard.co.uk/tech/airbnb-software-scan-online-life-suitable-guest-a4325551.html"&gt;Booker beware: Airbnb can scan your online life to see if you’re a suitable guest&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Details have emerged of its “trait analyser” software built to scour the web to assess users’ “trustworthiness and compatibility” as well as their “behavioural and personality traits” in a bid to forecast suitability to rent a property. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.media.mit.edu/articles/beyond-bias-contextualizing-ethical-ai-within-the-history-of-exploitation-and-innovation-in-medical-research/"&gt;Beyond Bias: Contextualizing “Ethical AI” Within the History of Exploitation and Innovation in Medical Research&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is from late December, but it just reached us and is worth a read: "It’s time for us to move beyond “bias” as the anchor point for our efforts to build ethical and fair algorithms."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Jan 1 - Jan 15</title><link href="/ai-ethics-news-roundup-jan1-jan15-2020.html" rel="alternate"></link><published>2020-01-15T09:20:00+01:00</published><updated>2020-01-15T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-01-15:/ai-ethics-news-roundup-jan1-jan15-2020.html</id><summary type="html">&lt;p&gt;Happy New Year! This roundup includes the Artificial Intelligence Video Interview Act, AI in diagnostic systems, Deep Fakes, How to build an ethical career, and more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Happy New Year! &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;The use of AI in job search processes and tools&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.vox.com/recode/2020/1/1/21043000/artificial-intelligence-job-applications-illinios-video-interivew-act"&gt;Artificial Intelligence Video Interview Act&lt;/a&gt; takes effect Jan 1 2020. Many video interview tools now incorporate some kind of AI screening to generate reports on candidates, a practice which raises several AI ethics and safety concerns. &lt;/p&gt;
&lt;p&gt;A new article at the WSJ discusses some of these: &lt;a href="https://www.wsj.com/articles/how-job-interviews-will-transform-in-the-next-decade-11578409136"&gt;How Job Interviews Will Transform in the Next Decade&lt;/a&gt;. "Recruiters using AI and virtual-reality simulations may hire based on a candidate’s behaviour, personality traits and physiological responses—no resumes needed"&lt;/p&gt;
&lt;p&gt;And you can already purchase countermeasures, if you can afford it. &lt;a href="https://www.scmp.com/news/asia/east-asia/article/3045795/south-korean-job-applicants-are-learning-trick-ai-hiring-bots"&gt;South Korean job applicants are learning to trick AI hiring bots that use facial recognition tech&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a topic EI has been thinking deeply about for a few months, and we have a detailed blog article in the works ... stay tuned!&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://twitter.com/DrHughHarvey/status/1213548573071204352"&gt;Comparison of the GoogleHealth breast AI paper against the RSNA Editorial Board recommendations for Assessing Radiology Research on Artificial Intelligence&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Are we holding AI diagnostic tools to the right standards? A great tweet from &lt;a href="http://twitter.com/DrHughHarvey"&gt;@DrHughHarvey&lt;/a&gt;. In the replies there's another good piece from October &lt;a href="https://www.nature.com/articles/s41598-019-51503-3"&gt;Using artificial intelligence to read chest radiographs for tuberculosis detection: A multi-site evaluation of the diagnostic accuracy of three deep learning systems&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://bostonreview.net/science-nature-politics/annette-zimmermann-elena-di-rosa-hochan-kim-technology-cant-fix-algorithmic"&gt;Technology Can't Fix Algorithmic Injustice&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We need greater democratic oversight of AI not just from developers and designers, but from all members of society.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://globaldatajustice.org/2020-01-01-data-and-justice-2019/"&gt;Data and Justice in 2019 — Who can afford big tech, and who can live without it?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"What we see is that while you may not have access to the cloud, you can still be tracked and controlled by your government’s AI. This increase in the reach of data and analytics is even more noticeable for those who don’t have a country to call home. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.designnews.com/artificial-intelligence/deepfakes-looming-threat-2020/109800999062105"&gt;Deepfakes: The Looming Threat Of 2020&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Deepfakes have been lurking on the internet for years now. But in 2020 the AI technology will become a powerful weapon for misinformation, fraud, and other crimes.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://thefuturesagency.com/2020/01/03/how-will-we-remain-useful-humans-a-longer-post-on-the-future-of-work-jobs-education-and-training/"&gt;How will we remain USEFUL HUMANS? A longer post on the future of work, jobs, education and training&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"As human intelligence (HI) encounters AI, will humans really become useless? Will all this progress be heaven (working only four hours per day, four days a week, but for the same money), or will it be hell (50% unemployment, rampant inequality and global civil unrest)? Or will it be both i.e. a kind of #hellven? Let’s have a look!"&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://hbr.org/2020/01/building-an-ethical-career"&gt;Building an Ethical Career&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Not an AI ethics piece, but some interesting reflections on how to build ethical awareness into our professional development. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.technologyreview.com/s/615015/ai-regulatory-principles-us-white-house-american-ai-initiatve/"&gt;The US just released 10 principles that it hopes will make AI safer&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The White House has released 10 principles for government agencies to adhere to when proposing new AI regulations for the private sector."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.techspark.co/blog/2020/01/02/human-like-robots-spark-fear-in-users-according-to-researchers/"&gt;Human-like robots spark fear in users according to researchers&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Japanese researcher Masahiro Mori’s “uncanny valley” theory, which he developed in the 1970s, states that we react positively to robots if they have physical features familiar to us -but they disturb us if they start looking too much like us."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 17-24</title><link href="/ai-ethics-news-roundup-dec17-dec24.html" rel="alternate"></link><published>2019-12-24T09:20:00+01:00</published><updated>2019-12-24T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-24:/ai-ethics-news-roundup-dec17-dec24.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 17 - Dec 24&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://twitter.com/random_walker/status/1208050796476215296"&gt;A great tweet from Arvind Narayanan&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/random_walker"&gt;Arvind Narayanan&lt;/a&gt; writes:&lt;/p&gt;
&lt;p&gt;"If you think there's too much yelling about algorithmic bias, here's an analogy. By the mid 90s the privacy community knew there was a huge problem. But it took two decades of yelling and a million privacy disasters before the public and policy makers started taking it seriously."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.youtube.com/watch?v=dEFI05Gtalc"&gt;Persons yet Unknown: Animals, Chimeras, Artificial Intelligence and Beyond&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/grok_"&gt;Kate Darling&lt;/a&gt; and &lt;a href="https://twitter.com/PKathrani"&gt;Paresh Kathrani&lt;/a&gt; in an fascinating discussing of robotics and AI and The Animal Law Conference. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://scholarship.law.duke.edu/dltr/vol18/iss1/17/"&gt;Implementing Ethics Into Artificial Intelligence: A Contribution, From A Legal Perspective, To The Development Of An Ai Governance Regime&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a new article added to a great issue of the Duke Law and Technology Review that came out in August, a &lt;a href="https://scholarship.law.duke.edu/dltr/vol18/iss1/"&gt;Symposium for John Perry Barlow&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;"This Article advocates for the need to conduct in-depth risk-benefit-assessments with regard to the use of AI and autonomous systems. This Article points out major concerns in relation to AI and autonomous systems such as likely job losses, causation of damages, lack of transparency, increasing loss of humanity in social relationships, loss of privacy and personal autonomy, potential information biases and the error proneness, and susceptibility to manipulation of AI and autonomous systems. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://thenextweb.com/artificial-intelligence/2019/12/17/researchers-were-about-to-solve-ais-black-box-problem-then-the-lawyers-got-involved/"&gt;Researchers were about to solve AI’s black box problem, then the lawyers got involved&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When things go wrong and AI runs amok, the lawyers will be there to tell us the most company-friendly version of what happened. Most importantly, they’ll protect companies from having to share how their AI systems work.&lt;/p&gt;
&lt;p&gt;We’re trading a technical black box for a legal one. Somehow, this seems even more unfair.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://deepai.org/publication/explainability-and-adversarial-robustness-for-rnns"&gt;Explainability and Adversarial Robustness for RNNs&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a common notion of adversarial robustness, and show that an adversarial training procedure can significantly and successfully reduce the attack surface."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.nature.com/articles/d41586-019-03895-5"&gt;This AI researcher is trying to ward off a reproducibility crisis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Joelle Pineau doesn’t want science’s reproducibility crisis to come to artificial intelligence (AI).&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.forbes.com/sites/sap/2019/12/18/ethics-in-ai/#2d7dd5285af4"&gt;Ethics In AI: Why Values For Data Matter &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;An argument for better corporate governance around AI and data. Corporations should "treat data as an asset .... the same way organizations treat inventory, fleet, and manufacturing assets. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.capgemini.com/research/the-virtuous-circle-of-trusted-ai-turning-ethical-and-transparent-ai-into-a-competitive-advantage-luciano-floridi/"&gt;The Virtuous Circle of Trusted AI: Turning Ethical and Transparent AI Into a Competitive Advantage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A nice short piece from &lt;a href="https://twitter.com/Floridi"&gt;Luciano Floridi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Most large organizations today across the United States and Europe are talking about “duty of care” and AI (i.e. the duty to take care to refrain from causing another person injury or loss). We also hear a lot about the need for clear normative frameworks in areas such as driverless cars, drones, facial recognition, and algorithmic decisionmaking guidelines in public-facing services such as banking or retail. I shall be surprised if we will have this conversation again in two years’ time and legislation hasn’t already been seriously discussed or put in place."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 10-17</title><link href="/ai-ethics-news-roundup-dec10-dec17.html" rel="alternate"></link><published>2019-12-17T09:20:00+01:00</published><updated>2019-12-17T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-17:/ai-ethics-news-roundup-dec10-dec17.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 10 - Dec 17&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://hai.stanford.edu/news/introducing-ai-index-2019-report"&gt;AI Index 2019 Report&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;An independent initiative within Stanford University’s Human-Centered Artificial Intelligence Institute, the report is in its third year and is the result of a collaborative effort led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry, in collaboration with more than 35 sponsoring partners and data contributors. The purpose of the project is to ground the discussion on AI in data, serving practitioners, industry leaders, policymakers and funders, the general public and the media that informs it. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;When should we decline to write code? A small case study.&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We picked this up on twitter when Emily Bender &lt;a href="https://twitter.com/ethicalai_co/status/1202638293269176321"&gt;tweeted&lt;/a&gt; that there was a task in an AI competition to create an AI that would solve problems involving the  "Prediction of Intellectual Ability and Personality Traits from Text". She's since &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;posted a thoughtful followup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an important problem. Technical and regulatory solutions should be augmented by professional codes of conduct and ethics if we want to ensure the safe and fair development of AI. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.newsweek.com/amazon-health-care-jeff-bezos-telemedicine-1475154"&gt;Do You Trust Jeff Bezos With Your Life? Tech Giants Like Amazon Are Getting into the Health Care Business&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Would you trust the Tech Giants with your health data in exchange for more personalized and on-demand healthcare? This article covers the current initiative of telehealth by Amazon and dives into a few key implications that this new commodity would carry for society at large.&lt;/p&gt;
&lt;p&gt;"What health insurance companies, as well as employers who foot the bulk of the U.S.'s health care bill, especially fear from telehealth is that it's so easy to use that people will reach out more often for care. "It creates the risk that every little ache and pain results in a claim that has to be paid out," says the University of Pennsylvania's Asch. "Making people come into the office is health care rationing by inconvenience."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.axios.com/ai-bias-c7bf3397-a870-4152-9395-83b6bf1e6a67.html"&gt;A tug-of-war over biased AI&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"A critical split divides AI reformers. On one side are the bias-fixers, who believe the systems can be purged of prejudice with a bit more math. (Big Tech is largely in this camp.) On the other side are the bias-blockers, who argue that AI has no place at all in some high-stakes decisions."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.bbc.co.uk/news/technology-50761116"&gt;Emotion-detecting tech should be restricted by law&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The AI Now Institute says the field is "built on markedly shaky foundations".
Despite this, systems are on sale to help vet job seekers, test criminal suspects for signs of deception, and set insurance prices. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.nytimes.com/2019/12/13/opinion/robot-caregiver-aging.html"&gt;Would you let a Robot Take Care of Your Mother? &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;AI usage for social care is not a new concept. However, as it becomes more and more of a reality, we are forced to shift our questions from theoretical to personal.&lt;/p&gt;
&lt;p&gt;"Some worry robot care would carry a stigma:the potential of being seen as “not worth human company,” said one participant in a study of potential users with mild cognitive impairments."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.cigionline.org/articles/owning-intelligence"&gt;Owning Intelligence&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The United States Patent and Trademark Office is trying to answer a very complicated question: who owns artificial intelligence?&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/48_aisg_neurips2019.pdf"&gt;AI Ethics for Systemic Issues: A Structural Approach&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"This paper calls for a "structural" approach to assessing AI’s effects inorder to understand and prevent such systemic risks where no individual can beheld accountable for the broader negative impacts. This is particularly relevantfor AI applied to systemic issues such as climate change and food security whichrequire political solutions and global cooperation. To properly address the widerange of AI risks and ensure ’AI for social good’, agency-focused policies must becomplemented by policies informed by a structural approach."&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Dec 2 - Dec 9</title><link href="/ai-ethics-news-roundup-dec-9.html" rel="alternate"></link><published>2019-12-09T09:20:00+01:00</published><updated>2019-12-09T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-09:/ai-ethics-news-roundup-dec-9.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Dec 2 - Dec 9&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to the EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://arxiv.org/abs/1912.00761"&gt;On the Legal Compatibility of Fairness Definitions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Although the article was on arxiv last week, the author publicized it this week, and it's a good one. There's poor alignment between operationalized definitions of fairness in machine learning and the legal definitions that may in fact apply to the deployment of these systems. &lt;/p&gt;
&lt;p&gt;"Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex socio-technical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts, as well as what both the legal and ML fairness communities can learn from these tensions. We focus this paper on U.S. anti-discrimination law since the ML fairness research community regularly references terms from this body of law."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.benzevgreen.com/wp-content/uploads/2019/11/19-ai4sg.pdf"&gt;'Good' isn’t good enough&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A critical reflection on the problems that arise when the pursuit of good is taken on as a technical objective too hastily, and why sustained and rigorous ethical reflection is a necessary if we want to have any confidence that such efforts will actually succeed. &lt;/p&gt;
&lt;p&gt;"Despite widespread enthusiasm among computer scientists to contribute to “socialgood,” the field’s efforts to promote good lack a rigorous foundation in politicsor social change. There is limited discourse regarding what “good” actuallyentails, and instead a reliance on vague notions of what aspects of society aregood or bad. Moreover, the field rarely considers the types of social changethat result from algorithmic interventions, instead following a “greedy algorithm”approach of pursuing technology-centric incremental reform at all points."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/arthur-ai/uk-gpdr-watchdog-says-explain-your-ai-373ef76d3c"&gt;New guidelines on the GDPR Right to Explanation&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The UK’s Data Protection Authority just issued much-anticipated guidance that clarifies the complicated issue of the GDPR’s ‘right to explanation’. Here is some background on the issue and what the new information means.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.linkedin.com/pulse/ethics-objective-reid-blackman-ph-d-/?trackingId=B5NT8Dd1BMx16FH15vHvZQ%3D%3D"&gt;Ethics is Objective&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Here are three arguments for the idea that ethics is subjective, presented with thoughtful rebuttals. This is a theme we took up in our last bog post, where we argued that there is a very large chunk of territory in tech ethics where ethical imperatives can be uncovered and agreed upon by sincere inquiry, even by those who disagree on more fundamental ethical and moral questions. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.academia.edu/41168605/Online_information_of_vaccines_information_quality_is_an_ethical_responsibility_of_search_engines"&gt;Online information of vaccines: information quality is an ethical responsibility of search engines&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When health-related disinformation is available online, who is responsible? There's a growing backlash against the idea of platforms as "mere tools", but perhaps we should think the same of search engines. We don't usually think that a library is responsible for dangerous information in its books, but should we think differently about Google?&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://deepai.org/publication/recovering-from-biased-data-can-fairness-constraints-improve-accuracy"&gt;Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Multiple fairness constraints have been proposed in the literature, motivated by a range of concerns about how demographic groups might be treated unfairly by machine learning classifiers. In this work we consider a different motivation; learning from biased training data. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://policyreview.info/concepts/datafication"&gt;Datafication&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the course of researching and discussing AI ethics challenges, we might run across the claim while the rate and scope of our generation of data has increased, it can be understood on a continuum with the ways in which human activity have always left traces and records. This article on the concept of "datafication" argues against this, and shows several ways to understand what is distinctive about the new systems and actors that collect and use our data. &lt;/p&gt;
&lt;p&gt;"Datafication is not just the making of information, which, in one sense, human beings have been doing since the creation of symbols and writing. Rather, datafication is a contemporary phenomenon which refers to the quantification of human life through digital information, very often for economic value. This process has major social consequences. Disciplines such as political economy, critical data studies, software studies, legal theory, and—more recently— decolonial theory, have considered different aspects of those consequences to be important. Fundamental to all such approaches is the analysis of the intersection of power and knowledge. "&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.techuk.org/insights/news/item/16447-welcome-to-techuk-digital-ethics-week"&gt;This is techUK Digital Ethics Week&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="https://www.thetimes.co.uk/article/amazon-ready-to-cash-in-on-free-access-to-nhs-data-bbzp52n5m"&gt;Amazon ready to cash in on free access to NHS data&lt;/a&gt;&lt;/h2&gt;</content><category term="news"></category><category term="roundup"></category></entry><entry><title>Weekly AI Ethics News Roundup: Nov 24 - Dec 1</title><link href="/ai-ethics-news-roundup-nov-24-dec-1.html" rel="alternate"></link><published>2019-12-02T09:20:00+01:00</published><updated>2019-12-02T09:20:00+01:00</updated><author><name>EI-Team</name></author><id>tag:None,2019-12-02:/ai-ethics-news-roundup-nov-24-dec-1.html</id><summary type="html">&lt;p&gt;Weekly AI Ethics News Roundup: Nov 24 - Dec 1&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome to our first EI weekly round-up; a curation of quality posts to help you cut through the noise and get right to the heart of the discussion on AI and Tech Ethics.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;The Second Wave of Algorithmic Accountability&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"... the first wave of algorithmic accountability focuses on improving existing systems, a second wave of research has asked whether they should be used at all—and, if so, who gets to govern them".&lt;/p&gt;
&lt;p&gt;Frank Pasquale argues that we can distinguish the "... first wave of algorithmic accountability research and activism", which has targeted existing systems and helped illuminate urgent ethical concerns in the AI systems already online, from "...an emerging “second wave” of algorithmic accountability has begun to address more structural concerns.". &lt;/p&gt;
&lt;p&gt;"Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology". &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://juliacomputing.com/blog/2019/11/22/encrypted-machine-learning.html"&gt;Machine Learning on Encrypted Data Without Decrypting It&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Recent breakthroughs in cryptography have made it practical to perform computation on data without ever decrypting it. In our example, the user would send encrypted data (e.g. images) to the cloud API, which would run the machine learning model and then return the encrypted answer. Nowhere was the user data decrypted and in particular the cloud provider does not have access to either the orignal image nor is it able to decrypt the prediction it computed."&lt;/p&gt;
&lt;p&gt;This application of homomorphic encryption systems might mitigate a number of data protection problems, even though it would still be imperative to ensure the data was collected ethically, and that the data itself is free from biases that are not understood and accounted for. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.wired.com/story/tainted-data-teach-algorithms-wrong-lessons/"&gt;Tainted Data Can Teach Algorithms the Wrong Lessons&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Security problems become ethics problems when vulnerabilities in software systems produce risks in their application that the stakeholders (both users, and the people in the social environment) are unable to understand, assess, and freely and knowingly accept. &lt;/p&gt;
&lt;p&gt;Adversarial input is a particularly powerful way to undermine machine learning systems and to cause them to behave in unexpected and unintended ways. "“Current deep-learning systems are very vulnerable to a variety of attacks, and the rush to deploy the technology in the real world is deeply concerning,” says Cristiano Giuffrida, an assistant professor at VU Amsterdam who studies computer security, and who previously discovered a major flaw with Intel chips affecting millions of computers."&lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://link.springer.com/article/10.1007%2Fs13347-019-00354-x"&gt;recent paper&lt;/a&gt; Luciano Floridi draws our attention to the extension of the practice of ethics dumping, "the export of unethical research practices to countries where there are weaker legal and ethical frameworks" into the digital realm. This is an ethical risk, but it is also a more basic risk to the quality of research. This article argues that there are also security problems with this sort of practice. "... some companies outsource the training of their AI systems, a practice known as machine learning as a service. This makes it far harder to guarantee that an algorithm has been developed securely."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.tandfonline.com/doi/full/10.1080/21670811.2019.1685395"&gt;Algorithms, Automation, and News&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A special issue of the Journal of Digital Journalism has been published on "Algorithms, Automation, and News".&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.halfwaytothefuture.org/programme/mcmillan-against-ethical-ai-guidelines-and-self-interest"&gt;Against Ethical AI: Guidelines and Self Interest&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"In this paper we use the EU guidelines on ethical AI, and the responses to it, as a starting point to discuss the problems with our community's focus on such manifestos, principles, and sets of guidelines. We cover how industry and academia are at times complicit in ‘Ethics Washing’, how developing guidelines carries the risk of diluting our rights in practice, and downplaying the role of our own self interest. We conclude by discussing briefly the role of technical practice in ethics."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.datainnovation.org/2019/11/5-qs-for-anne-kao-senior-technical-fellow-at-boeing-research-and-technology/"&gt;5 Q’s for Anne Kao, Senior Technical Fellow at Boeing Research and Technology&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"The Center for Data Innovation spoke with Anne Kao, Senior Technical Fellow at Boeing Research and Technology. Kao discussed how she uses machine learning to analyze maintenance reports and how philosophy influences how she approaches data science."&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.reuters.com/article/us-apple-ukraine-crimea-idUSKBN1Y124O"&gt;Ukraine denounces Apple for calling Crimea part of Russia in apps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"Reuters reporters in Moscow who typed the name of the Crimean provincial capital Simferopol into Apple’s Maps and Weather apps on Wednesday saw it displayed as “Simferopol, Crimea, Russia”. Users elsewhere — including in Ukraine’s capital Kiev and in Crimea itself — see locations in Crimea displayed without specifying which country they belong to. "&lt;/p&gt;
&lt;p&gt;One might wonder if the technical specifications in the ticket for the engineering work that was involved discussed the political and ethical implications. It's perhaps difficult to imagine that something of this magnitude wasn't remarked upon, but on the other, so much engineering work in software proceeds as though it happens in at least partial isolation from the downstream social and political environment.&lt;/p&gt;</content><category term="news"></category><category term="roundup"></category></entry></feed>