<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ethical Intelligence - articles</title><link href="/" rel="alternate"></link><link href="/feeds/articles.atom.xml" rel="self"></link><id>/</id><updated>2020-07-09T15:00:00+02:00</updated><entry><title>Affective Video Technology and the Ethics of Flawed Scientific Foundations –Part I</title><link href="/curry-affective-ai-part-1.html" rel="alternate"></link><published>2020-07-09T15:00:00+02:00</published><updated>2020-07-09T15:00:00+02:00</updated><author><name>Alba Curry</name></author><id>tag:None,2020-07-09:/curry-affective-ai-part-1.html</id><summary type="html">&lt;p&gt;Hundreds of companies are not using affective video technology during the hiring process but the ethical ramifications of this technology have not been fully fleshed out.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Over 700 companies are now using affective video technology during the hiring process and other human resources decisions based on an analysis of the interviewee’s facial “expressions.” However, the ethical ramifications of this technology have not been fully fleshed out. I offer a brief survey of the topic: (1) how the field and companies portray themselves, and (2) how the media portrays them. In part II, I will cover the major problem with this use of affective technology: these affective technologies are based on outdated emotion science from the 1960s–80s, ignoring work in both science and philosophy from the last forty years. &lt;/p&gt;
&lt;h2&gt;What is Affective Video Technology? What is Emotion AI, also known as Affective AI?&lt;/h2&gt;
&lt;p&gt;Affective video technology, or emotion detection technology, is part of &lt;a href="https://mitsloan.mit.edu/ideas-made-to-matter/emotion-ai-explained"&gt;Emotion AI&lt;/a&gt; (also called affective AI, &lt;a href="https://www.kairos.com/blog/what-is-affective-computing"&gt;affective computing&lt;/a&gt;, artificial emotional intelligence) which dates back to at least 1995 when &lt;a href="web.media.mit.edu/~picard/"&gt;Rosalind Picard&lt;/a&gt; published “&lt;a href="https://vismod.media.mit.edu/pub/tech-reports/TR-321-ABSTRACT.html"&gt;Affective Computing&lt;/a&gt;,” after having realized that the role of emotions is essential in human cognition as they play a critical role in decision-making, perception, human interaction, and in human intelligence more broadly. She realized that if the goal of AI is to replicate the way humans think, which is debatable, then emotion had to be a part of it. Therefore, Emotion AI refers to new artificial intelligence technologies whose goal is learning and recognizing human emotions, and to use that knowledge to improve everything from marketing campaigns (&lt;a href="https://www.affectiva.com/"&gt;Affectiva&lt;/a&gt;, &lt;a href="https://www.realeyesit.com/"&gt;Realeyesit&lt;/a&gt;, and &lt;a href="https://www.kairos.com"&gt;Kairos&lt;/a&gt;) to health care (&lt;a href="www.empatica.com"&gt;Empatica&lt;/a&gt;, and &lt;a href="https://emteq.net/"&gt;emteq&lt;/a&gt;), to security (&lt;a href="https://www.wesee.com/#about"&gt;WeSee&lt;/a&gt;, and &lt;a href="https://www.oxygen-forensic.com/uploads/doc_guide/JetEngine_web.pdf"&gt;OxygenForensics&lt;/a&gt;). It is a subset of AI that aims to measure, understand, simulate, and react to human emotions. Affective video technology as it stands today is a set of algorithms and video technology that collect and interpret data points from a person’s face, voice, and also sometimes gait.&lt;/p&gt;
&lt;p&gt;Affective video technology at the moment requires two techniques: computer vision, to precisely identify facial movements, and machine learning algorithms to analyze and interpret the alleged emotional content of those facial features. Typically, the second step employs a technique called supervised learning, a process by which an algorithm is trained to recognize things it has seen before. The basic idea is that if you show the algorithm thousands and thousands of images of stereotypically happy faces with the label “happy” when it sees a new picture of a happy face, it will, again, identify it as “happy.” This is based on the premise that analyzing emotions in real time is simply a mathematical problem of astronomical proportions –an equation our brains repeatedly solve in microseconds throughout the day– and therefore it is only a matter of time before AI can master it beyond human abilities in much the same way that it mastered games like chess or Go.&lt;/p&gt;
&lt;h2&gt;HireVue’s Affective Video Technology Hiring Services&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.inc.com/minda-zetlin/ai-is-now-analyzing-candidates-facial-expressions-during-video-job-interviews.html"&gt;Firms&lt;/a&gt; have begun using affective video technology to facilitate the hiring process by not only reducing the financial costs and duration of the hiring process, but also hiring the best candidates using a method allegedly based on science which removes human bias and error. HireVue is the leading company using affective video technology to aid with the hiring process and offers its clients: “predictive assessments tied to higher quality hires,” hiring faster due to their one step assess and interview, a better experience for candidates since they can do it from home at their own convenience, and they claim that their software is validated by industrial and organizational (I-O) psychology experts who “craft valid assessments with scientific rigour.”&lt;/p&gt;
&lt;p&gt;The algorithm which determines an “employability” score is by no means transparent and neither website nor candidate help offers information on the criteria for the predictive assessment. According to Loren Larsen, HireVue’s CTO, the system dissects tiny details of candidates’ responses (facial expressions, eye contact, perceived “enthusiasm”) and compiles reports companies can use for hiring or disregard. The algorithm is trained by having current workers of the company sit through the assessment. Part of the idea behind this technology is based on &lt;a href="https://www.nature.com/news/2010/100526/full/465412a.html"&gt;already existing methods of assessing threat levels&lt;/a&gt; based on a person’s facial movements, voice inflections, and body language which was based on the work of &lt;a href="https://www.paulekman.com/"&gt;Paul Ekman&lt;/a&gt;, who claims that there is a set of universal emotions which have facial expressions associated with them which act as fingerprints. Some of those may be micro-expressions which may be difficult to detect since they occur very rapidly. It was therefore seen as fertile ground for AI to improve human’s ability to read emotions on someone’s face. Larsen compares the algorithms’ ability to boost hiring outcomes with medicine’s improvement of health outcomes, since the algorithm is more objective than flawed metrics used by human recruiters. Companies are now believing in machine decisions over human feedback. &lt;/p&gt;
&lt;h2&gt;Media Coverage, and General Ethical Concerns&lt;/h2&gt;
&lt;p&gt;Media coverage has been overwhelmingly negative and yet companies and the field as a whole is still thriving (it is estimated to be a 25 billion dollar industry). As of late, the more positive or neutral coverage focuses on &lt;a href="https://www.forbes.com/sites/robinryan/2020/05/19/employers-are-using-a-new-hiring-technique-as-reopening-begins/#131ee5a830a3"&gt;the positive impact that HireVue has had during the pandemic&lt;/a&gt; since the whole hiring process can be done off-site. Another positive comment has been how affective technologies like HireVue’s &lt;a href="https://www.inc.com/minda-zetlin/ai-is-now-analyzing-candidates-facial-expressions-during-video-job-interviews.html"&gt;can help ensure diversity&lt;/a&gt;, widening the talent pool, and &lt;a href="https://www.hirevue.com/blog/hirevue-assessments-and-preventing-algorithmic-bias"&gt;helping avoid bias–both human and algorithmic&lt;/a&gt;, as advertised by HireVue &lt;a href="https://www.hirevue.com/resources/five-recruiting-strategies-to-increase-workplace-diversity"&gt;on their website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The more critical media, most notably, Drew Harwell, writing for The Washington Post, has covered two major ethical concerns regarding the use of AI during the interview process: privacy and bias: &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Privacy&lt;/strong&gt;: The Electronic Privacy Information Center, known as EPIC, &lt;a href="https://www.washingtonpost.com/context/epic-s-ftc-complaint-about-hirevue/9797b738-e36a-4b7a-8936-667cf8748907/?itid=lk_inline_manual_4"&gt;filed an official complaint&lt;/a&gt; calling the Federal Trade Commission (FCT) to investigate HireVue’s Business practices. The privacy concern has two sides: 1) since HireVue refuses to share information about their algorithms it is impossible for job candidates to know how their data is being used or to consent to such uses. This is linked to claims that affective video technology is in general dehumanizing and invasive. 2) The information collected by HireVue can be shared with other companies as the result of a merge, reorganization, purchase or acquisition of any part of their business. Therefore, potential employees do not know what they are consenting to: what information they are giving up, and in whose hands it is going to end up. HireVue and companies like Affectiva, which uses affective AI for marketing, claim consent to their use of your data. In the case of HireVue, potential employees or employers can opt not to use HireVue’s service, which realistically puts them at a disadvantage with others who do. In the case of marketing companies, they require an opt-in consent. However those have come under scrutiny since they do not offer real choices and information about where data is coming from, how it is stored, whether it can be sold and transferred or linked to other data sets. Strict rules concerning data collection and sharing that often apply in Academia are not transferred to the private sector &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt;: The worry is what counts as the perfect employee? Since algorithms learn on data, these determine what the optimal employee looks like. Therefore, datasets need to be unbiased to ensure ethical hiring practices that are appropriate for all potential interviewees, not just the subset used for training. Problematically, most companies do not release their datasets or algorithms, making it very difficult to prove bias. Studies have recently shown that facial recognition technologies reproduce biases harming minority communities. Emotion detection technology assigns more negative emotions to black men’s faces than white counterparts, geographic proxies reveal socio-economic backgrounds and datasets based on current employees re-create current society’s biases in hiring. The &lt;a href="https://www.technologyreview.com/2019/11/07/75194/hirevue-ai-automated-hiring-discrimination-ftc-epic-bias/"&gt;EPIC complaint&lt;/a&gt;, which suggests that HireVue’s promise violates the FTC’s rules against “unfair and deceptive” practices, has been received by the FTC but not yet pursued. The company, HireVue’s Larsen said, audits its performance data to look for potentially discriminatory hiring practices, known as adverse impacts, using “world-class bias testing” techniques. The company’s algorithms, he added, have been trained “using the most deep and diverse data set of facial action units available, which includes people from many countries and cultures.” However, even when algorithms are not based on an “ethnicity classifier” as in the case of Affectiva problems arise. Affectiva uses geography as a proxy for identifying where someone is from and what their characteristic facial “expressions” are but that means that they compare British smiles against British smiles, and Chinese smiles against Chinese smiles. Therefore if there is a Chinese person in the UK, it would miss the cultural nuance. Regulators will need to figure out how much responsibility companies should be expected to shoulder in avoiding the mistakes of a prejudiced society.&lt;/p&gt;
&lt;p&gt;That said, affective video technology in the hiring process ought to attract more scrutiny than affective video technology in other corporate contexts because of the potential for harming actual people. If we accept that affective video technology does not tell us what its proponents think it tells us, then any decisions based on it will be problematic: a company’s decisions about product placement or advertising may have some impact on the company itself, but decisions based on affective video technology in the hiring process can cause significant and obvious harm to the individual applicant. In Part II of this series I will discuss a third and the more problematic worry, namely that affective video technology is based on flawed, outdated science and therefore it has a long way to go before it is viable, if it every will be. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="affective AI"></category><category term="emotion AI"></category><category term="privacy"></category><category term="bias"></category></entry><entry><title>Interview with Alex Hutchison and Alessandra Fassio from the Data for Children Collaborative</title><link href="/hutchison-fassio-interview.html" rel="alternate"></link><published>2020-07-02T18:00:00+02:00</published><updated>2020-07-02T18:00:00+02:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2020-07-02:/hutchison-fassio-interview.html</id><summary type="html">&lt;p&gt;We sat down with Alex Hutchison and Alessandra Fassio to discuss both the journey of creating an Ethical Assessment protocol as well as now implementing it over the past year of its existence.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last year Ethical Intelligence had the opportunity to work with the &lt;a href="https://www.dataforchildrencollaborative.com/"&gt;Data for Children Collaborative with UNICEF&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; in creating an Ethical Assessment that would help guide the future research projects of the Collaborative. The &lt;a href="https://www.dataforchildrencollaborative.com/ethics"&gt;Ethical Assessment&lt;/a&gt; is composed of a Compass, Roadmap and Highway Code. The Compass is the guiding ethical charter that embodies the Collaborative’s mission, the Roadmap is the ethical protocol each research project must follow, and the Highway Code is the resource document that explains the reasoning behind the protocol as well as provides further information when necessary. &lt;/p&gt;
&lt;p&gt;I had the pleasure of sitting down with Alex Hutchison, Director of the Collaborative, and Alessandra Fassio, Programme Administrator and EI Expert on the original project, to discuss both the journey of creating the Ethical Assessment as well as now implementing it over the past year of its existence. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;First of all, thank you both for joining me today. I’m excited to hear about the past year of working with the Ethical Assessment on live projects! Let’s start off with a bigger question - what is the importance of having the Ethical Assessment both within the context of the Collaborative and beyond?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hutchison&lt;/em&gt;: 
Our work at the Collaborative deals with two real pressure points; data with all its complexities, assets and ripples effects, and children as they are a vulnerable group often without say of what is done to them. The combination of these two points coming together meant we needed a clear and transparent protocol that everyone could refer back to. It is important for all organisations to have some kind of ethical assessment, but it was more pressing of an issue for us to have something because of the nature of our work. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fassio&lt;/em&gt;:
We wanted to create something practical without having to rely on previous charters and protocols of other organisations. It was important to have something freestanding that would reflect the mission of the Collaborative, not just a basic solution out of the back of the box. By having the Ethical Assessment, we have something very practical that we can use and engage with our project teams on, which is essential to the work we do here. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;It’s very clear how important having this Ethical Assessment was for the Collaborative! Generally, however, ethics can be seen as a blocker in a lot of technological contexts. Did you face any hesitation when first setting out on the project?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hutchison&lt;/em&gt;: 
Not at all. I knew we needed something, and me not being from an extensive background in ethics or morality was refreshing because I could objectively push the project forward without getting tangled up in nuances. There weren’t many frameworks to use as examples, so my main objective was to get the vision we had for the Collaborative out into the world. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fassio&lt;/em&gt;:
Philosophy is notorious for getting stuck in the ivory tower, so having to create something applicable was an interesting challenge. We could have spent months on moral theories, but that would have missed the point, or at best ended in merely virtue signalling. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Ethical Assessment came out of a strong understanding of ethical theories, how has it been now adopting it in your everyday processes, especially since you work mainly with data scientists?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fassio&lt;/em&gt;:
The Assessment was designed to be a living document, which basically means that it is not just a one form and done. It’s meant to carry through all the different stages of the project lifecycle, which saves it from being a tick box and embeds ethics into the project itself. Now, it’s a valuable asset because it creates constant reflection on that fact that children must be the top stakeholder. It is an important reminder that your priorities and objectives can change as a project develops, so there is a need to continuously check back in. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hutchison&lt;/em&gt;: 
We see different reactions to the Assessment. Some people readily accept it and are happy that there is a safeguard. But we also see people saying that as long as they use a safe dataset, there aren’t problems. The fact that the Assessment illustrates that ethics is not just about the safety of the dataset, that it’s more foundational than that, is really important for our projects. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Looking now specifically at a recent project the Collaborative undertook concerning HIV and children, what role did the Ethical Assessment play?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fassio&lt;/em&gt;:
HIV is a really broad topic, so having the Assessment actually helped us refine what the research question actually was. Being able to look at the bigger ethical picture helped us take a step back and understand what we really wanted and were able to achieve. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hutchison&lt;/em&gt;:
This was a good exemplar of what happens when you have people that are subject matter expertise and come to the project with a design already in mind. HIV is the ideal situation for someone to have a strong opinion, which makes it all the more important to be able to use the Ethical Assessment to lift out of that mindset, critically examine the issues, and ask the right questions. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;It sounds like the Ethical Assessment was a strong asset in the HIV project and beyond. How have you been measuring its success overall, since ethics can initially often seem intangible?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hutchison&lt;/em&gt;: 
We accept that sometimes it may feel like just another form to fill out, but we’ve seen how it has changed programme and project design for the better and so recognise its importance and success. I have no hesitations from stopping a project all together if we reach a point in the Ethical Assessment that signals we are in the red. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fassio&lt;/em&gt;:
The fact that it gets people talking about the ethical issues in the first place is a success. It shows that we are asking the right questions, that the people working on these sensitive subjects have taken a step back and thought everything through. It helps us analyse how we may need to adjust a project’s vision along the way so that it remains productive and ethically sound. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;As you look towards the future of the Collaborative and the role of the Ethical Assessment in your projects, what lessons have you learned over the past year that you want to carry forward?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hutchison&lt;/em&gt;: 
We’ve learned the importance of being able to build trust in what the Assessment is. By being able to clearly explain who was involved in the making of it, and communicate transparently to the various stakeholders, we are able to build trust and respect in the judgements resulting from the Assessment. Currently we track comments and feedback very closely, and we hope that as time progresses to be able to continue reviewing and sharpening the process. Each project requires a different approach to the Assessment, and each new approach teaches us something further in terms of the importance of awareness and trust. We also hope to increase the messaging around the Assessment so that other organisations can recognise the importance of having one and begin developing their own. In my opinion, the important question that will lead influence how we move forward is how do we make this part of common practice? This is about educating and training and awareness. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fassio&lt;/em&gt;:
There’s no one right way of doing it. We’ve had to learn that there isn’t a consistent way of introducing the Assessment, as gauging the personalities and dynamics of the teams will dictate how the Assessment is adopted into the process. Sometimes it’s the whole team that takes responsibility, sometimes it’s the PI. The most important thing is that it is incorporated in a productive and impactful way. Everyone does philosophy in their everyday life, it’s giving people the confidence to transfer it to the professional setting that’s so valuable. People can contribute to the conversation no matter what their background is, sometimes all they need is a little hand holding and guidance. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;To round out this interview, if you could summarise in your own opinion, what is the benefit to having the Assessment?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hutchison&lt;/em&gt;:
Credibility. It gives the Collaborative and the projects we do here undeniable credibility. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fassio&lt;/em&gt;:
The fact that it allows us to be transparent. We are not afraid to stop a project if we see it is heading in an ethically questionable direction. &lt;/p&gt;
&lt;p&gt;For more information on the Data for Children Collaborative with UNICEF, visit their website &lt;a href="https://www.dataforchildrencollaborative.com/"&gt;here&lt;/a&gt;, or check out &lt;a href="https://www.dataforchildrencollaborative.com/ethics"&gt;their dedicated ethics page here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;You can also find them on Twitter @dataforchildren or &lt;a href="https://www.linkedin.com/company/dataforchildrencollaborative"&gt;Linkedin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(1) The Data for Children Collaborative with UNICEF is a joint partnership between UNICEF, the Scottish Government and the University of Edinburgh hosted by The Data Lab. The goal of the Collaborative is to address existing problems for children, a highly sensitive and vulnerable group, by using innovative data science techniques.&lt;/em&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="ethical assessment"></category><category term="interviews"></category><category term="our work"></category></entry><entry><title>Unpacking Racial Bias — An EI Workshop Report</title><link href="/racial-bias-renee-cummings.html" rel="alternate"></link><published>2020-06-29T15:00:00+02:00</published><updated>2020-06-29T15:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-06-29:/racial-bias-renee-cummings.html</id><summary type="html">&lt;p&gt;As part of the wider Building Ethical Intelligence educational series, last week EI hosted a virtual workshop on AI &amp;amp; Racial Bias with Renée Cummings.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;“Why let the new technology reinforce old biases?" - Renée Cummings, CEO of Urban AI&lt;/h2&gt;
&lt;p&gt;As part of the wider Building Ethical Intelligence educational series, last week EI hosted a virtual workshop on AI &amp;amp; Racial Bias with Renée Cummings. Designed to create a space for open conversation on the topic of systematic racism and how it has been reinforced in technology, the workshop consisted of a presentation by Renée, group discussions, and a wider conversation to reflect on potential points of action. &lt;/p&gt;
&lt;p&gt;Additionally, we were joined by the co-founders of the &lt;a href="https://www.radicalai.org/"&gt;Radical AI Podcast&lt;/a&gt; who recorded Renée’s talk for a bonus episode. You can listen to the episode on &lt;a href="https://podcasts.apple.com/us/podcast/the-radical-ai-podcast/id1505229145"&gt;iTunes&lt;/a&gt;, &lt;a href="https://open.spotify.com/episode/2qJIrocZHfLuZ0rNXOSF6d"&gt;Spotify&lt;/a&gt; and &lt;a href="https://radicalai.podbean.com/"&gt;Podbean&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Understanding Sentiment: A glance into how we perceive racial bias&lt;/h2&gt;
&lt;p&gt;At the beginning of the workshop we asked participants to complete a survey about their attitude towards racial bias in tech. In total, we received 43 responses (after removing those who opted out of being reported). 58% of participants were female, 37% male, and 2% identified as non-binary. The participant’s professional background was varied but predominantly academic and from the tech industry. Finally, the participants were from all over the world but the United Kingdom and the United States were the most common countries. As far as the ethnic background of the participants, the biggest group identified as caucasian forming 41.9% of the participants. Black participants made up around 14% of the group, followed by Asian/Pacific Islander (9%). A significant portion of the participants (23.3%) identified as “Other”. In the following sections we will group together all people of colour (POC) as we have only a small number of participants in each group although the majority of participants were not caucasian. &lt;/p&gt;
&lt;div style="text-align:center"&gt;&lt;img src="./images/Racial_Bias_Report/Figure_1.png" alt="Ethnic background of participants." width="500"/&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Breakdown of participant's ethnicities.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To begin with, we asked participants to what degree they felt impacted by racial bias in technology. Their responses are shown in Figure 2. Overall we can see that POC tend to find that racial bias in technology impacts them, although the difference with caucasian participants is relatively small. The majority of participants across the board feel that racial bias in tech has a small to moderate impact on their lives.&lt;/p&gt;
&lt;p&gt;&lt;img alt="To what extent do you feel impacted by racial bias in tech?" src="./images/Racial_Bias_Report/Figure_2.png"&gt;
&lt;em&gt;Figure 2: Participants’ responses to the question “to what degree do you feel impacted by racial bias in technology?” from 1 (Not at all) to 5 (Very) in percentage terms, according to ethnicity. Note that we group POC as each subpopulation in our sample is small.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Next we asked participants to what degree they felt they had the ability to mitigate racial bias in tech. Their answers are shown in Figure 3. The overwhelming majority of participants felt that they had little or no ability to mitigate racial bias in tech. This stands in contrast with the professional background of the participants, a lot of whom worked in technology, academia and law.&lt;/p&gt;
&lt;p&gt;&lt;img alt="To what extent do you feel impacted by racial bias in tech?" src="./images/Racial_Bias_Report/Figure_3.png"&gt;
&lt;em&gt;Figure 3: Participants’ responses to the question “to what degree do you feel you have the ability to mitigate racial bias in technology within your daily life?” in percentage terms, according to ethnicity.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally in Figure 4 we show the participants’ attitude towards their own responsibility when it comes to fighting bias in tech. From this chart we can see minimal differences across ethnic groups as most participants feel they do have a responsibility to tackle bias in tech. Given that the majority of participants said they felt they didn’t have the ability to mitigate racial bias in tech, and yet simultaneously felt the responsibility to enact some kind of change, we can start to recognise a potential blocker in the fight against racial bias. Intention is the first step towards change, but knowing what steps to take to bring about that change is just as vital. With this in mind, we focused the breakout group discussions on practical steps to identifying racial bias in tech, as well as how to mitigate once identified. &lt;/p&gt;
&lt;p&gt;&lt;img alt="To what extent do you feel impacted by racial bias in tech?" src="./images/Racial_Bias_Report/Figure_4.png"&gt;
&lt;em&gt;Figure 4: Participants’ responses to the question “to what degree do you feel you have a responsibility to fight racial bias in technology?” in percentage terms, according to ethnicity.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Identifying systematic racism in our technology&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Technology is not just a mirror of existing biases, it is a magnifying glass.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Due to the influence and reach of our current technological systems, any implicit bias that slips into a system is immediately emphasized and amplified far beyond original intention. What we need to recognise, however, is that bias is not only a dataset problem and adding more data points will not result magically in a solution. Although bias is often found and mitigated on a dataset level, it extends beyond such into areas like data labeling, the deployment cycle, and design decisions. &lt;/p&gt;
&lt;p&gt;One of the difficulties we face in fighting racial bias is the fact that machines and algorithms create a proxy that blocks the view of those being impacted by a system from being seen by the system’s creators. Although a system’s creator may have no intention of creating a racially biased system, if they are unable to see how their decisions have led to significant impact on individual people’s lives, it becomes difficult to feel accountable for negative outcomes and take into consideration impact on communities one is not a part of. &lt;/p&gt;
&lt;p&gt;We have been able to identify and work on eliminating the vividly clear instances of racial bias, but this is only scratching the surface. Now we are faced with identifying the nuances in our technology, the magnifying glass of societal systems, in which deeply rooted systemic racism hides. Although it is essential that we thoroughly examine and question our technology, from datasets to algorithms all the way to development cycles, we must first begin by examining our societies from a human-centric perspective. &lt;/p&gt;
&lt;h2&gt;How to impact technology for good&lt;/h2&gt;
&lt;p&gt;So, where do we begin? &lt;/p&gt;
&lt;p&gt;As we saw from the results of the survey, even though we may feel a strong sense of responsibility to impact a change in racial bias, that doesn’t necessarily mean we know how to or even feel that we have the ability to do so. Awareness of the issue is a vital first step, but what comes after? &lt;/p&gt;
&lt;p&gt;For those in leadership positions, a good next step involves an active search for diverse employment tools and application protocols. We need to not only bring the right voices to the table, but also to make sure those voices have a microphone to speak into. It doesn’t matter how diverse a board is if the key voices do not have any power to impact. &lt;/p&gt;
&lt;p&gt;For those who feel they have no power to impact, learn to trust in the strength of asking the right questions. Often, people from a non technical background feel they do not have the ability to question technology, as it is too far outside of their scope of knowledge. The longer we feed into this misconception, the harder it will be to overcome these issues. At the root of it all, these are human-based issues that impact people, which means if you are human you have a right and responsibility to question any technology. &lt;/p&gt;
&lt;p&gt;Finally, for everyone, look to educate yourself and those around you. Consumers need to understand where, how and when AI is used in decision making so that they can make informed decisions about using, or not using, certain technology. Awareness, both in the tech industry and the wider public, starts with proper education and understanding. &lt;/p&gt;
&lt;p&gt;Change is never comfortable. If we are to push forward, we must proceed with empathy, compassion, honesty. But most importantly, we must proceed loudly, for the time of silence is long gone.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="racial bias"></category><category term="workshops"></category></entry><entry><title>The New Normal - Life after COVID-19?</title><link href="/life-after-covid19.html" rel="alternate"></link><published>2020-06-18T12:00:00+02:00</published><updated>2020-06-18T12:00:00+02:00</updated><author><name>Oriana Medlicott</name></author><id>tag:None,2020-06-18:/life-after-covid19.html</id><summary type="html">&lt;p&gt;DCT, facial recognition software and video conferencing were already shaping our society before COVID-19 and have continued to do so during this time of crisis, but what are their ethical implications in a post-COVID-19 world?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Right now, the news and our social media timelines are flooded with an overload of information and statistics surrounding COVID-19.  Naturally, it is an extremely worrying time for everyone and we are all feeling the impact. One of the main questions that keeps arising during this uncertain period is -  what will life be like after COVID-19? Will we be able to get back to what we knew as normal or will there be a shift in society?&lt;/p&gt;
&lt;p&gt;The economic impact is set to be huge. Many businesses are already experiencing difficulties and are forced to close temporarily, whereas some have been able to quickly adapt by moving their entire workforce to working remote. One industry that has evidently benefitted is Tech. Apps such as Zoom, Uber Eats and House Party, to name a few, have seen a giant spike in users.  If the world didn’t already massively rely on technology, then it most certainly does now and this reliance will continue post COVID-19.  As we move into a potential era of expedited dependence on technology, what will this mean for ethical concerns we already had prior to COVID-19 and,  has this unprecedented time enhanced these?  &lt;/p&gt;
&lt;p&gt;This article will be analysing  three technologies, Facial Recognition, Video Conferencing and Digital Contact Tracing (DCT) which have come to the fore in this global pandemic. The expedited adoption of these three has highlighted key ethical concerns that need to be addressed to enable their beneficial use for the future. &lt;/p&gt;
&lt;h2&gt;Facial Recognition&lt;/h2&gt;
&lt;p&gt;Over the past years we have seen a huge increase worldwide in the use of facial recognition, which has driven scrutiny for ethical consequences such as bias where the lack of diversity in programming of AI is a repeated issue. Algorithms can only be as unbiased as the people who create them and there have been many examples of &lt;a href="https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/"&gt;black people&lt;/a&gt; being wrongly identified. Clearview AI, has been at the forefront of controversy regarding the ethical breaches in the use of their facial recognition technology. According to &lt;a href="https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html"&gt;Kashmir Hill&lt;/a&gt;, due to the lack of public knowledge, over 600 law enforcement agencies spread around the US have started using Clearview AI’s tool which is able to identify murder suspects, thieves, fraud and more. &lt;/p&gt;
&lt;p&gt;Clearview poses a risk due to the sensitive data that is uploaded to a server where there is no evidence of data protection, this in turn led to their entire client database being hacked. Again, similar to the data scandal regarding &lt;a href="https://www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook"&gt;Cambridge Analytica&lt;/a&gt;, it was found that Clearview AI had used over 3 billion facial images from social media sites such as Facebook, Youtube and Twitter, again, violating  core ethical principles such as consent and privacy.  As a response to much of the backlash and outrage surrounding multiple data scandals, in January, governing bodies such as the &lt;a href="https://www.bbc.co.uk/news/technology-51148501"&gt;European Union&lt;/a&gt;, announced a potential 5 year ban whilst looking to deliver tighter policies regarding the ethics of facial recognition being used. &lt;a href="https://www.nytimes.com/2019/05/14/us/facial-recognition-ban-san-francisco.html"&gt;San Francisco&lt;/a&gt; went as far as banning the use of facial recognition throughout the city due to privacy concerns.&lt;/p&gt;
&lt;p&gt;As the COVID-19 crisis became more imminent worldwide, many governments were quick to implement facial recognition to track and monitor their citizens during lockdowns. Russia enforced widespread use of &lt;a href="https://www.youtube.com/watch?v=oKX_E7adVwg&amp;amp;t=166s"&gt;facial recognition surveillance&lt;/a&gt; to execute strict lockdown policies as a response to the spread of COVID-19. &lt;a href="https://www.aljazeera.com/news/2020/03/china-ai-big-data-combat-coronavirus-outbreak-200301063901951.html"&gt;China&lt;/a&gt;, enforced facial recognition to monitor temperatures of people on public transport alerting authorities of suspected COVID infected people. There has been a clear surge in the use of this technology to help fight the spread of the virus. In a post-pandemic world, this increased use of facial recognition may be implemented at a more rapid rate in the name of safety. However, it is important to not let our concern for health and security overcome the imperative need for ethical safeguards. &lt;/p&gt;
&lt;h2&gt;Digital Contact Tracing&lt;/h2&gt;
&lt;p&gt;Arguably,Tech is a huge component holding us together during this crisis as we all quickly transitioned to the ‘new normal’. At the beginning of April, the &lt;a href="https://www.who.int/news-room/detail/03-04-2020-digital-technology-for-covid-19-response"&gt;World Health Organisation&lt;/a&gt;, announced an overwhelming need for and support from digital technology whilst tech companies raced for health technology solutions to fight the virus. Digital Contact Tracing (DCT), has been the most prominent tool that big tech giants such as Apple and Google will make available to health authorities worldwide.  There are two types of DCT: centralised and decentralised. The tech giants are looking to use the decentralized model, whereas the UK government has opted for the centralised one, which has caused evident privacy concerns. The key difference between both, is that the centralised model uploads data into one server whereas decentralised allows for more user autonomy by enabling data to be stored on a personal device. It is argued that the decentralized model has enhanced data protection making it harder for hackers to infiltrate our personal data. &lt;/p&gt;
&lt;p&gt;Rightly so, DCT has led to many debates from Ethicists regarding data safety and surely after many data scandals, we have every right to contest this. Is trust in this technology blinded by our urgent need? Consequently leading us into further privacy breaches with our data. We must ensure users have autonomy over their data enabling applications to have clear guidelines for privacy and consent, guaranteeing that people are aware of where their data is being stored and the option for it to be erased once consent is no longer given. Another concern, is that consent may just be assumed when a person is traveling abroad, and therefore leading to personal data use by different governing states and companies.  &lt;/p&gt;
&lt;h2&gt;Video Conferencing&lt;/h2&gt;
&lt;p&gt;As the threat of COVID-19 spread, where possible companies started encouraging employees to work remotely from home. Businesses had no choice but to trust and rely on technology to keep their teams connected and their operations running smoothly.  Family and friends have been able to stay connected via ‘the weekly Zoom Quiz’, new apps such as House Party allow users to play games together and the UK government even hosted their cabinet meetings via Zoom. This is part of the ‘new digital normal’  where in the name of health, human interaction is via a screen. &lt;/p&gt;
&lt;p&gt;As quickly as Zoom gained popularity during lockdown, widespread news alerted the world of hackers infiltrating the app. The UK Government was quickly advised by the National Cyber Security Centre to stop using Zoom due to fears of Chinese surveillance. &lt;a href="https://citizenlab.ca/2020/04/move-fast-roll-your-own-crypto-a-quick-look-at-the-confidentiality-of-zoom-meetings/"&gt;The Citizens Lab  of Toronto University&lt;/a&gt; published a report in early April outlining the potential security risks of using Zoom to protect data. The report findings note that, although Zoom is a Silicon Valley based company, it has 3 companies in China where 700 people are employed to develop the software. Why? In order for  Zoom to avoid US wages and increase their profit margins. However, this leaves them vulnerable to pressure from Chinese Authorities. Additionally, allowing Chinese authorities to have access to private data from users that aren’t based in China.&lt;/p&gt;
&lt;p&gt;Covid-19 certainly increased Zoom’s popularity but exposed ethical privacy breaches that were already present before this crisis unfolded. In early January, Zoom had already been determined by cybersecurity firm Check Point to have compromised security that enabled hackers to use a meeting ID to gain access. The firm recommended ways to mitigate this by enforcing passwords, but this was ignored by Zoom. This failure to act is what caused more of the recent issues we have seen and shows how this negligence compromised user safety. &lt;/p&gt;
&lt;p&gt;Will the novelty wear off with this new method of working and communication? The flaws that have been exposed need to be addressed and private corporations must take accountability and to ensure transparency is at the forefront of their business model. Privacy and security are core principles of ethics, and must be centered in the programming and development of the technology in hand, for users to feel confident. &lt;/p&gt;
&lt;h2&gt;Tech and Out New Normal of Tomorrow?&lt;/h2&gt;
&lt;p&gt;No one can truly predict what a post-pandemic world will look like, what our new working habits will be and how we will use new technologies. However, it is evident that video conferencing, digital contact tracing and facial recognition have their role to play in our current normal and our normal of tomorrow. These technologies all come with their evident ethical concerns before COVID-19 and now, this crisis has highlighted some of their clear faults and why ethical guidelines are extremely essential at mitigating such risks. &lt;/p&gt;
&lt;p&gt;It is highly likely that many governments will mandate the use of contact tracing and facial recognition with the intention of stopping the spread of the virus. This is when transparency and accountability are important, we must be aware of who is developing the technology, why and what the intentions are. We should all have one common goal to fight against the spread of COVID-19 and keep our communities safe and healthy. Provided that ethics is at the forefront of business discussions and decisions, and that ethicists are able to play the key role they have in the innovation and expedition of new technologies, then we are headed in the right direction for safe and fair technology of tomorrow.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="dct"></category><category term="covid-19"></category><category term="facial recognition"></category></entry><entry><title>Immunity Passports: The Dangers of Mixing Health Data and Economic Liberties</title><link href="/immunity-passports-covid19.html" rel="alternate"></link><published>2020-06-11T15:00:00+02:00</published><updated>2020-06-11T15:00:00+02:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2020-06-11:/immunity-passports-covid19.html</id><summary type="html">&lt;p&gt;Why Health Data and Economic Liberties Don’t Mix - An Ethical Analysis of Immunity Passports&lt;/p&gt;</summary><content type="html">&lt;p&gt;As we transition into the beginning phases of deconfinement, we are seeing countries incorporate different variations of tech-based solutions to help assist in these delicate endeavors. Digital contract tracing (DCT) applications have taken the main stage in conversations for proposed solutions, however this is not the only tech being offered as an answer to our current crisis. Although on a much smaller scale in comparison to DCT, discussions on immunity passports have begun to appear on our timelines as another potential tech solution to expedite the exit from quarantine for certain individuals. However, just as we asked with DCT, so we must ask again with immunity passports: what are the ethics behind this technology, and, with human values in mind, is this worth pursuing? &lt;/p&gt;
&lt;h2&gt;Defining Immunity Passports&lt;/h2&gt;
&lt;p&gt;An immunity passport is essentially a digital certificate attached to a person’s identity indicating whether or not an individual has previously contracted the coronavirus and is subsequently immune to a second infection. The purpose of such technology would be to begin allowing individuals who are immune, and therefore risk-free in respect to contracting COVID-19, to begin returning to ‘normal life’ activities and liberties. &lt;/p&gt;
&lt;p&gt;Currently, the application of immunity passports has been put on hold due to the fact of insufficient evidence of the necessary antibodies that would declare an individual immune. In fact, the &lt;a href="https://www.who.int/news-room/commentaries/detail/immunity-passports-in-the-context-of-covid-19"&gt;World Health Organization&lt;/a&gt; advised against governments adopting any form of immunity passport, at this point in time, due to lack of evidence and therefore risk of potential for re-infection. Although this is a significant barrier to the immediate use of immunity passports, it does not mean that it is an absolute end to the discussion. &lt;/p&gt;
&lt;p&gt;If sufficient evidence in effective antibodies does arise, then the idea of an immunity passport will very likely come back into the conversation. Furthermore, as the world now understands the importance of preparation for a pandemic, immunity passports may be viable in future cases. So, even though we are not utilizing them at this very moment in time, it does not imply that we never will, and instead affords us the time to properly consider the ethical angles to this technology prior to technical implementation.&lt;/p&gt;
&lt;h2&gt;Breaking down the tech - what are we really working with here?&lt;/h2&gt;
&lt;p&gt;In order to properly consider the ethics of this technology, let’s take a step back from any technical, medical, or le gal barrier to immunity passports that may currently exist. These are of course vital components to the success of such passports, but can often cause distractions, confusing the debate and pulling us away from our original goal of unpacking the core ethical principles at work. So, for just a moment, let’s step into a thought experiment in which the required antibodies exist and immunity passports are technically feasible. &lt;/p&gt;
&lt;p&gt;At its very core, immunity passports are simply digital markers indicating whether or not a person has previously contracted coronavirus and so now has the antibodies required to prevent a second infection. Fundamentally speaking, the act of indicating whether or not an individual is immune is ethically neutral. As it is neither ethically good nor bad to have previously contracted coronavirus, the marker is not ethically laden, it’s merely a descriptor indicative of reality.  &lt;/p&gt;
&lt;p&gt;It is important to consider, however, that such a digital marker should be classified as health data as it is essentially an indicator of an individual’s medical history. Just as a medical record of whether or not a person has had the chickenpox is kept securely as health data, so should a record of whether or not they have had coronavirus. Under normal circumstances, broadcasting information about individuals’ medical histories to the public would be immediately rejected as we have come to respect health information as something private between an individual and their doctor. However, there are times in which individuals are willing to share health data if it means they gain some sort of benefit in return. In the case of the coronavirus, information regarding prior infection results in the gain of an immunity passport.&lt;/p&gt;
&lt;p&gt;Again, the act of indicating prior infection is ethically neutral. Sharing this information, on the other hand, starts to bring this ethical neutrality into question. However, the real complications start to arise when we begin assigning economic and social value to this immunity marker. &lt;/p&gt;
&lt;h2&gt;What happens when our medical histories affect our economic abilities?&lt;/h2&gt;
&lt;p&gt;Although an immunity passport may be a simple digital marker at its core, this is not its only layer. The differentiating factor between an immunity passport and, say, a digital marker indicating whether a person has a strawberry allergy or not, is the value that we assign to the immunity marker. Normally, a digital marker indicating some detail or another about an individual’s medical history remains securely in the medical field. However, this is where an immunity passport would differ, as the digital marker for immunity is created with the purpose of allowing an individual with the marker access to certain economic and social liberties. It is this crossover that we now need to take a closer critical eye to. &lt;/p&gt;
&lt;p&gt;Our medical histories are often out of our control. So, we try to keep them as separate as possible from our economic and social standing, as conversely these are aspects of our lives we try to maintain high amounts of control over. It is not unheard of for medical conditions to crossover into influencing economic and social factors of our lives, however these crossovers are what we term as disabilities or handicaps. For example, a person born without legs is physically handicapped and may face economic and social setbacks because of this. The important thing though, is we work as a society to combat any challenge or disadvantage caused by medical handicaps to the best of our abilities. &lt;/p&gt;
&lt;p&gt;Returning back to immunity passports, we see that such a marker would be a crossover from individuals’ medical histories into the economic and social aspects of their lives. However, in this case we would not term this crossover as a handicap, but rather an advantage, while those without the passport would be the ones put at a disadvantage. This means that an immunity passport would essentially be creating socially-imposed economic handicaps that were not previously there. Except in this case the handicap would not grant the individual access to additional aid, rather it would instead restrict the individual further.&lt;/p&gt;
&lt;p&gt;Currently, the tech industry does not possess any overarching code of ethics, so I’m going to fall back on the principles in medical ethics here for a moment to help illustrate what is going on ethically in this situation. Medical ethics has four main principles, one of which is the principle of justice that essentially states there must be an element of fairness in all medical decisions. Although immunity passports are issued to make economic and social decisions, these decisions are based on medical data, so in this case it is fair to apply the principle of justice to this technology. &lt;/p&gt;
&lt;p&gt;As currently proposed, immunity passports would be violating the ethical principle of justice due to the fact that the possession of an immunity passport would disproportionately either benefit or burden an individual based on circumstances beyond their own control. The action of assigning access to economic and social liberties to the digital marker of immunity is what makes the originally neutral technology into an ethically laden one, and not in a good way. &lt;/p&gt;
&lt;h2&gt;Respecting Ethical Limitations Leads to Stronger Solutions&lt;/h2&gt;
&lt;p&gt;As we now see, the ethical principle of justice calls immunity passports into question. When justice is put at risk in this context, we are looking at substantial consequences such as people voluntarily infecting themselves out of desperation, discrimination in access to proper testing, and a disproportionate effect on people whose professions require them to work onsite, to name a few. This is why it is so essential to break down technology proposals to the point of ethical analysis as it helps uncover, what we hope were, unintended consequences and allows us to tackle the issues before they arise. &lt;/p&gt;
&lt;p&gt;In the case of immunity passports, we see that although the technology is ethically neutral, the proposed application of it is not. It is important to note now that this does not mean we should give up on the technology itself. Instead, this implies that we have not found the best possible application for the technology according to our ethical limitations. &lt;/p&gt;
&lt;p&gt;In fact, it is quite possible to use the concept of a digital marker to help us fight COVID-19. For example, it could be beneficial for high-risk individuals to voluntarily apply for a digital marker that grants them certain aids, such as access to grocery stores the hour after they have been disinfected, or priority for access to masks. These are of course only examples that need to be further examined to see if they are viable, but the point here is that there are other applications of this technology that are better suited both technically and ethically to our end goal of the fight against coronavirus. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="immunity passports"></category><category term="covid-19"></category></entry><entry><title>Garbage In, Cyber Truck Out</title><link href="/bogani-truck.html" rel="alternate"></link><published>2020-06-10T15:00:00+02:00</published><updated>2020-06-10T15:00:00+02:00</updated><author><name>Ronny Bogani</name></author><id>tag:None,2020-06-10:/bogani-truck.html</id><summary type="html">&lt;p&gt;A proposal for Ethical Artificial Intelligence Sustainability Impact Statements (E.A.I.S.I.S.) in Autonomous Vehicle Manufacturing&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/EASIS%20-%20Bogani%202020.pdf"&gt;Read the Full Article&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Aiming to address the increasing demand for data presented by Artificial
Intelligence (AI). This essay proposes adopting an Ethical Artificial Intelligence
Sustainability Impact Statement (E.A.I.S.I.S) through the various stages of the
Artificial Intelligence Development Cycle (AIDC). The following analysis treats data as a limited resource requiring sustainable exploitation and management. Following a brief discussion about sustainability and AI, the different stages of the AIDC and accompanying factors for an ethical impact evaluation are addressed through proposed questions. These inquiries are intended to identify, disclose and address the data’s toxicity level, or potential ethical harm for use.&lt;/p&gt;
&lt;p&gt;The proposed approach to data as a public good benefits both the digital
environment (DE) and conserves natural resource demand in the real world (IRL).
Data is currently a commodity commanding prices higher than oil within a booming
data brokerage industry.1 As with other commodities, derivative markets will form
with the price for data lowering proportionally to its level of impurity, or debasement. Through using an E.A.I.S.I.S. approach, certain ethical issues attached to data can be flagged, evaluated, and provided to all users and redeployed in a transparent manner. The following discussion addresses factors to be considered in creating an E.A.I.S.I.S. in the development of an autonomous vehicle (AV). As a novel approach to AIDC, the issues raised are intended to serve as a framework for later discussion, research, development, and application.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/EASIS%20-%20Bogani%202020.pdf"&gt;Read the Full Article&lt;/a&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="autonomous vehicles"></category><category term="sustainability"></category></entry><entry><title>Building Ethical Intelligence, the Podcast</title><link href="/webinar-june20.html" rel="alternate"></link><published>2020-06-05T15:00:00+02:00</published><updated>2020-06-05T15:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-06-05:/webinar-june20.html</id><summary type="html">&lt;p&gt;The first installment in our new series of webinars, &lt;em&gt;Building Ethical Intelligence&lt;/em&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Join us as we unpack what's happened this month in AI Ethics, take a deep dive into Michael Klenk's work on online manipulation and its implications for legislation, as well as hear his thoughts on fairness in digital contact tracing.&lt;/p&gt;
&lt;p&gt;For further information on Michael’s research, you can visit his website &lt;a href="http://www.michael-klenk.com/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you would like to contact Michael, you can reach him by &lt;a href="mailto:M.B.O.T.Klenk@tudelft.nl"&gt;email&lt;/a&gt;, or connect via &lt;a href="https://www.linkedin.com/in/mbklenk/"&gt;LinkedIn&lt;/a&gt; and &lt;a href="https://twitter.com/michaelklenk"&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This podcast is part of our wider educational series, Building Ethical Intelligence, and is the audio recording of the live webinar event. The series is designed to keep you up-to-date on &lt;strong&gt;developments in AI Ethics&lt;/strong&gt;, deepen your &lt;strong&gt;understanding of integral concepts&lt;/strong&gt;, and build your ability to &lt;strong&gt;translate ethical intelligence into practical action&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;If you would like to participate in the next webinar, please sign up &lt;a href="https://www.eventbrite.co.uk/e/building-ethical-intelligence-live-tickets-106091727212"&gt;here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Subscribe via Spotify, Apple Podcasts or Google Play or follow us on LinkedIn and Twitter for future episodes. &lt;/p&gt;
&lt;iframe title="Building Ethical Intelligence" id="multi_iframe" src="https://www.podbean.com/media/player/multi?playlist=http%3A%2F%2Fplaylist.podbean.com%2F8661426%2Fplaylist_multi.xml&amp;vjs=1&amp;kdsowie31j4k1jlf913=63a18e2c15a698cf2318ae77ec22e652f69d1786&amp;size=430&amp;skin=11&amp;episode_list_bg=%23ffffff&amp;bg_left=%23fafafa&amp;bg_mid=%23fafafa&amp;bg_right=%23fbfafc&amp;podcast_title_color=%23474547&amp;episode_title_color=%23080708&amp;auto=0&amp;download=1&amp;show_playlist_recent_number=10&amp;pbad=1" frameborder="0" scrolling="no" width="100%" height="432" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;Hosted by Olivia Gambelin, Oriana Medlicott and Amanda Curry from Ethical Intelligence&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="podcast"></category></entry><entry><title>Running an Ethics Audit - Case Study: Digital Contact Tracing</title><link href="/dct-report.html" rel="alternate"></link><published>2020-05-29T15:00:00+02:00</published><updated>2020-05-29T15:00:00+02:00</updated><author><name>EI-Team</name></author><id>tag:None,2020-05-29:/dct-report.html</id><summary type="html">&lt;p&gt;An ethical analysis of Digital Contact Tracing based on the EU's guidelines for ethical AI.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/Digital%20Contact%20Tracing%20-%20EI.pdf"&gt;Read the Full Report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since the onset of the COVID-19 pandemic, the tech industry has been hard at work developing potential solutions to help fight the virus. One of the more prominent proposals has been for the use of Digital Contact Tracing (DCT), the technical application of manual contact tracing methods. DCT quickly became a topic of hot debate, as privacy and trust concerns were thrust into the spotlight as never before. Suddenly, ethical principles that had been discussed theoretically at length had a clear actionable case study plastering headlines as society began to understand the importance of ethics in emerging technology and the risks that unchecked innovation holds.&lt;/p&gt;
&lt;p&gt;Although there is now a clear call for the incorporation of ethical principles in DCT applications, the path to such is not certain. There is no rule book for how to implement DCT while still respecting human rights and ethics, no clear cut guide laying out each step to take. However, this does not mean that we are without a solution. As we look to roll out DCT applications to help fight the coronavirus, it is vital that these applications undergo due diligence ethics audits if we are to respect human dignity.&lt;/p&gt;
&lt;p&gt;The purpose of this paper is to illustrate, from a high level perspective, what it would look like to run a preliminary ethics audit on a DCT application, with the hope that it may provide some guiding structure to those currently working on similar applications.&lt;/p&gt;
&lt;p&gt;To demonstrate such, we will be evaluating a hypothetical Bluetooth Digital Contact Tracing application against the seven principles laid out by the European Union’s High-Level Expert Ethics Guidelines. For each of the seven principles we will highlight the essential considerations that must be made in order to embed the principle into practice in terms of the chosen technology. These considerations are not only applicable to DCT, but also exemplify the high-level process any COVID-19 tech should be undertaking.&lt;/p&gt;
&lt;p&gt;&lt;a class="readmore" href="https://ethicalintelligence.co/theme/files/Digital%20Contact%20Tracing%20-%20EI.pdf"&gt;Read the Full Report&lt;/a&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category><category term="dct"></category></entry><entry><title>Is Public Surveillance the Answer to COVID-19? Why Community Engagement is Needed to Shape Better Informed Decisions</title><link href="/community-engagement-covid19.html" rel="alternate"></link><published>2020-05-28T18:00:00+02:00</published><updated>2020-05-28T18:00:00+02:00</updated><author><name>Lily Grogan</name></author><id>tag:None,2020-05-28:/community-engagement-covid19.html</id><summary type="html">&lt;p&gt;More invasive tech solutions for a post-COVID-19 world do present public health benefits, but they are not infallible, and the trade-off is personal privacy, autonomy and dignity.&lt;/p&gt;</summary><content type="html">&lt;p&gt;COVID-19 has seen governments and tech working together to combat its spread, leading to the implementation of increasingly invasive surveillance measures. This has given rise to the illusion that one cannot have both privacy and health at once. This is a false choice.&lt;/p&gt;
&lt;p&gt;This &lt;a href="https://www.linkedin.com/pulse/why-we-need-ethics-tech-now-more-than-ever-covid-19-olivia-gambelin/"&gt;trade-off illusion&lt;/a&gt; must be challenged. But the implementation of emergency measures across the globe such as digital contact tracing, mobile phone tracking and even &lt;a href="https://www.bbc.com/news/av/technology-52619568/coronavirus-robot-dog-enforces-social-distancing-in-singapore-park"&gt;robot police dogs&lt;/a&gt; continues to raise ethical concerns. While these surveillance measures have great potential to help curb the spread of COVID-19, they are far from perfect, and continue to raise concerns over privacy, autonomy, and dignity. Companies and policy makers must turn to the stakeholders within their communities to develop tech that is locally acceptable- or risk losing public trust.&lt;/p&gt;
&lt;p&gt;While the social distance enforcing dog ‘droids roaming Bishan-Ang Mo Kio Park may seem like a dystopian fantasy to many in the West, they are not the first COVID-19 tech solution to be pioneered by Singapore. They were one of the first countries to implement digital contact tracing with their &lt;a href="https://www.tracetogether.gov.sg/common/privacystatement"&gt;TraceTogether&lt;/a&gt; app launched in March- the likes of which are now being developed across the globe.&lt;/p&gt;
&lt;p&gt;Digital contact tracing is a good focal point for the discussion surrounding public surveillance, for if the intended ends are achieved, it is easy to see how they might justify the means.&lt;/p&gt;
&lt;h2&gt;DCT makes sense, but how effective really is it?&lt;/h2&gt;
&lt;p&gt;The idea is simple. A user reports positive on the app if they become sick, and other users who have recently been in contact with them will receive a notification that they have been exposed to COVID-19. &lt;/p&gt;
&lt;p&gt;The UK’s NHS is developing its own app, while Australia recently released &lt;a href="https://www.health.gov.au/resources/apps-and-tools/covidsafe-app#about-the-app"&gt;COVIDSafe&lt;/a&gt;, it’s version of TraceTogether. Unlike China’s &lt;a href="https://www.theguardian.com/world/2020/apr/01/chinas-coronavirus-health-code-apps-raise-concerns-over-privacy"&gt;Health Code&lt;/a&gt; service, which freely shares the GPS location of its users with authorities, apps like TraceTogether and COVIDSafe do not require location sharing, instead relying on Bluetooth proximity to deduce when an infected person has been within range. Users are kept anonymous through an encrypted ID and safeguarded by the systematic destruction of data (usually after a period of 14-21 days).&lt;/p&gt;
&lt;p&gt;Perhaps more so than other public surveillance initiatives, these apps have the potential to drastically curb the spread of infection as they alleviate the laborious task of manual tracing. But how reliable are they? &lt;/p&gt;
&lt;p&gt;There is no guarantee that someone who self-reports positive on an app actually has the virus. Those who receive a notification that they have been in contact with a sick person have no idea who this person is and whether their encounter has credibly left them at risk of infection. Some will self-isolate, while other well people may feel compelled to contact emergency services, putting more pressure on systems already under massive strain. The potential for false positives may lead others to ignore such notifications altogether, making the app redundant. &lt;/p&gt;
&lt;p&gt;False negatives are just as problematic; Bluetooth limitations mean accuracy is not guaranteed. This could create a false sense of security leading to complacency. DCT users also require a smart phone with internet access, creating a distribution challenge with socioeconomic implications.&lt;/p&gt;
&lt;p&gt;Furthermore, the trustworthiness of those charged with handling these intimate datasets is up for debate, as tech giants and government bodies alike have been known to break their own privacy policies in the past (Google’s &lt;a href="https://www.theguardian.com/commentisfree/2017/jul/05/sensitive-health-information-deepmind-google"&gt;Deepmind scandal&lt;/a&gt; being just one example- and not the worst). The &lt;a href="https://www.lightbluetouchpaper.org/2020/04/12/contact-tracing-in-the-real-world/"&gt;difficulties&lt;/a&gt; associated with running decentralised tech systems alone mean that the temptation to centralise would be great, and Apple and Google’s efforts to predispose all of their devices to DCT scales the associated risks enormously. Also worth noting is the efficacy of such a system to cause widespread panic if masses of users suddenly report positive. This makes them excellent targets for malicious hackers.&lt;/p&gt;
&lt;p&gt;Governments are aware of these concerns, which is why these apps are voluntary. Yet as it stands, simply not enough people are opting in for them to be effective. DCT requires 60% of a population to participate for it to work, but as of now, only 20% of the Singaporean population have opted in. This means that unless these apps are made mandatory, manual contact tracing is likely to remain the predominant method. &lt;/p&gt;
&lt;p&gt;Dr Chia Shi-Lu, chairman of the Singapore Government Parliamentary Committee for Health, recently &lt;a href="https://www.tnp.sg/news/singapore/tracetogether-app-should-be-mandatory-all-experts"&gt;called for TraceTogether to become mandatory&lt;/a&gt; for the entire population. This negates previous assurances that the app was on a strictly opt-in basis, which may lead to further reluctance for public participation in similar initiatives across the globe.&lt;/p&gt;
&lt;h2&gt;Engaging with Communities Helps Shape Better Informed Decisions&lt;/h2&gt;
&lt;p&gt;How can developers and policy makers implement tech that people can trust? &lt;/p&gt;
&lt;p&gt;The discussion surrounding the widespread implications of COVID-19 must aim to address public concerns and engage with a rich variety of researchers and ethicists as well as stakeholders within the community. In doing so, responsible technologies and policies may be developed which encompass the values of those affected and respect personal and public boundaries.&lt;/p&gt;
&lt;p&gt;The sense of collective responsibility within communities has been demonstrated as people have been required to act in unison to combat the challenges at hand. Almost everyone has willingly sacrificing personal liberties to remain at home or placed their own selves at risk by travelling to work every day and providing humanities essential services. &lt;/p&gt;
&lt;p&gt;The public have demonstrated their ability to use their judgment and common sense in face of COVID-19 and should be trusted to do so moving forwards. Increasingly invasive policing and surveillance measures risk building resentment towards authority. This threatens the inter-reliance which binds communities and encourages this kind of positive collaboration. &lt;/p&gt;
&lt;p&gt;As expounded by global health emergency expert &lt;a href="https://www.nuffieldbioethics.org/publications/research-in-global-health-emergencies/"&gt;Katherine Wright&lt;/a&gt;, an environment of trust, which in turn encourages public compliance, can only be achieved through a regulatory response that is locally acceptable. This requires the appropriate assessment and incorporation of the values of those affected by given measures. When stakeholder’s concerns are sidelined in the development of solutions, effectiveness becomes compromised as people’s willingness to comply diminishes.&lt;/p&gt;
&lt;h2&gt;The Time to Set an Ethical Precedent is Now&lt;/h2&gt;
&lt;p&gt;As we move through uncertain terrain it is difficult to imagine what circumstances will develop in the coming months and the implications they may bring, making it essential that we set an ethical precedent now. More invasive tech solutions do present public health benefits, but they are not infallible, and the trade-off is personal privacy, autonomy and dignity. &lt;/p&gt;
&lt;p&gt;While the current crisis may seem to justify measures which infringe upon our personal and public boundaries, it also presents an opportunity to identify these core values not merely as a luxury afforded in times of normality, but as key components of the framework within which these policies are formed.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>EI Ethics Workshop on Digital Contact Tracing: Event Summary</title><link href="/dgt-event.html" rel="alternate"></link><published>2020-04-24T17:30:00+02:00</published><updated>2020-04-28T17:30:00+02:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2020-04-24:/dgt-event.html</id><summary type="html">&lt;p&gt;On 22 April 2020, Ethical Intelligence hosted a virtual ethics workshop specifically targeted at better understanding the ethical issues presented by digital contact tracing, in light of the COVID-19 crisis.&lt;/p&gt;</summary><content type="html">&lt;p&gt;On 22 April 2020, Ethical Intelligence hosted a virtual ethics workshop specifically targeted at better understanding the ethical issues presented by digital contact tracing, in light of the COVID-19 crisis. The event began with presentations on AI in healthcare and the ethics of digital contact tracing to lay the metaphorical building blocks for the group discussions that followed. This is a summary of the presentations and high level concepts that took place during the workshop. &lt;/p&gt;
&lt;h2&gt;AI, Healthcare &amp;amp; Ethics - Michael McAuley.&lt;/h2&gt;
&lt;p&gt;Disease has changed human history, with numerous disease hosts and transmission routes. The societal impact can range from moderate to serious, with the current Covid-19 crisis having parallels with the first recorded cholera pandemic. It is important to determine the disease involved initially, before mutation or DNA/RNA degradation, and easier to chart its spread at the beginning of an outbreak, potentially helping to change its course and save lives, alongside public engagement and education. AI technology can help by utilizing populations to track and trace disease, as well as alert people about any necessary behavioural changes, in addition to rapid vaccine development, virtual consultations, advanced screening and diagnostic software etc.&lt;/p&gt;
&lt;p&gt;The ethical debate must consider the four main principles of medical ethics starting with Beneficence, which is to ensure what is being proposed is for the benefit of individuals, making the well-being of people the primary concern, with due diligence in the design of AI, consideration for the impact on the patient-prescriber bond and, vitally, the technological prowess of healthcare professionals. Non-Maleficence, which is to evaluate the merit of the proposal to ensure no harm is caused, concerns advising creators of AI systems to identify the potential human rights implications of their creations, with a timely redress of any discriminatory outputs, while bearing in mind the impact on research, as AI may struggle to take the same approach to hazard perception as human beings. Justice, which is concerned with the proposal being fair and cost-effective, requires looking for bias, which can be introduced in development through a lack of diversity in input information, and determining whether the parameters for adoption require more than one type of data e.g. simulation data, as well as performing a cost-benefit analysis. Autonomy considers if individuals are willing participants, so investigating if the technology is moral and its role in decision-making, which must be transparent, understandable and reviewable by a competent human authority, such as a doctor or pharmacist. Protecting patients and professional autonomy will require a large number of organisations to work collaboratively and consider both the legal implications of utilising AI and the negligence implications on registrants of professional bodies. &lt;/p&gt;
&lt;p&gt;We have a robust understanding of medical ethics, but there remain many risks. In business these include future liability issues, financial penalties, loss of stakeholder trust and irreparable brand damage, while individual risks comprise matters such as privacy implications, data use and consent. We need to turn these risks into opportunities with positive, human-centric solutions.&lt;/p&gt;
&lt;h2&gt;Digital Contract Tracing - Andrew Buzzell&lt;/h2&gt;
&lt;p&gt;AI Ethics and tech ethics generally often leans heavily on functional and risk analysis. The justification for the use of technology, and for actions based on outputs from AI systems, depends on part on their strength, reliability, and the kinds of failures that can occur. Digital contact tracing depends on the viability of using bluetooth signal strength on mobile devices as a non-causal predictor of COVID-19 transmission. Properties of COVID-19, such as mode of transmission, infectivity, virulence, and pathogenicity, the nature of bluetooth radio signals, and the prevalence of compatible devices and the usage of the devices will all interact and affect the real life efficacy of DCT. &lt;/p&gt;
&lt;p&gt;Further  analysis reveals that this efficacy is not smoothly distributed - there will be uneven representation among identifiable groups of individuals along socioeconomic and other axes that will directly influence the degree to which DCT apps can detect real transmission events. &lt;/p&gt;
&lt;p&gt;This in turn should inform our ethical reflection on the kinds of actions that we should take as a result of a transmission event predicted by DCT, and the kinds of coercion that could be acceptable to drive adoption. It also should colour our analysis of the tradeoffs we might make legally, economically, and socially in order to use DCT. There has been significant discussion of the privacy tradeoffs, and the interaction between the technical power of big tech powers to make DCT easier to use (for example, by relaxing limitations on running apps access to the bluetooth antenna, and facilitating the transfer of data), and the demands of governments for support implementing DCT. &lt;/p&gt;
&lt;p&gt;There will be significant generation of false positives and false negatives, and highly uneven adoptions and coverage of DCT. As a result, the kinds of followup we take, such as automatic self-quarantine or mandated testing, will not only imperfectly track actual transmission risk, it will also affect some groups of people more than others. The technical question "can DCT work to control the spread of COVID-19" is thus tightly linked to the ethical question "should we use DCT as part of our effort to control the spread of COVID-19". There are some ethical questions we might wish to consider as being immune to context-sensitive factors - we might think some kinds of privacy are so important that no distal benefit could justify compromising. But a great deal of the ethical considerations we care about are in fact sensitive to the trade-offs we want to make, so there is value in being as clear as we can be about the real viability of DCT. Then we can lean on existing frameworks in public health ethics to begin to think about what ethical and effective deployment could look like, if it is possible at all. &lt;/p&gt;
&lt;h2&gt;Group Discussions&lt;/h2&gt;
&lt;p&gt;Digital contact tracing, if deployed on national and international levels, has the potential to touch, figuratively speaking, millions of lives. If we are to fully understand the implications and impacts this technology can have on society, then we must consider it from multiple angles through a diverse set of stakeholders. &lt;/p&gt;
&lt;p&gt;Although our various discussion groups were tasked with different topics of concentration, a common theme appeared across the board: the need for trust. If DCT is to be applied on a national and even international scale, users must first be able to not only trust that it works, but also trust that there is a transparent timeframe and scope for this technology. Currently, there is a significant amount of uncertainty surrounding the effectiveness of DCT in fighting COVID-19. Without a certain level of evidence that this technology does in fact help stop the spread of the virus, potential users are reluctant to use DCT applications as they feel that in doing so they have unevenly traded a level of privacy for an unknown, or even nonexistent benefit. In addition to this, there is fear of scope creep, with potential users lacking trust in the governments and companies deploying contact tracing to not use it for alternative means post COVID-19. Overall, the majority of considerations, implications, and fears surrounding DCT can all point back to the central concern of lacking trust in the technology and those deploying it. &lt;/p&gt;
&lt;p&gt;Extending beyond this concern for lack of trust, we dug into the considerations that need to be made in terms of bias, consent and privacy, and the short term versus long term implications of DCT within the various discussion groups. Looking into the potential sources of bias within DCT, we saw concerns surrounding access appear in forms such as a lack of proper cell phone coverage in rural areas and lack of properly updated smartphones in lower income households. When reflecting on issues of privacy and consent, there was a general consensus that these two issues are often communicated as binary entities, when in reality they exist on a scale. When classified in binary terms, more complications are created, but when consent and privacy are allowed to exist on a scale, many concerns are already addressed by the increase of control of personal data. Beyond the current application of DCT, there were a plethora of concerns on what DCT could mean in the future for our digital identities. If companies or even governments, require the use of DCT, what does that mean for individual consent and privacy? Furthermore, if DCT applications are tied to our access to resources, there is an even bigger increase of risk for bias, discrimination, and “shaming”. &lt;/p&gt;
&lt;p&gt;A global framework for track and trace data capture is possible, but many factors need to be addressed prior to implementation. Issues to address include international education and human rights, as well as ensuring that individuals are informed about data and how they consent to its use, as well as the implications of its capture and storage to them. Political freedoms need to be protected, and if this technology is misused or a framework cannot be agreed, then treaties such as CTBT may need to be used to control widespread human rights abuses. We must be positive about innovation and the future and do our best to build trust between nations and educate creators and innovators, while empowering  everyone to confidently engage with this global issue.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>A Summary of The European Commission White Paper on Artificial Intelligence – a European approach to excellence and trust</title><link href="/ec-wp-2020.html" rel="alternate"></link><published>2020-04-16T07:20:00+02:00</published><updated>2020-04-16T07:20:00+02:00</updated><author><name>Volha Litvinets</name></author><id>tag:None,2020-04-16:/ec-wp-2020.html</id><summary type="html">&lt;p&gt;On 19 February 2020, the European Commission launched a Consultation on Artificial Intelligence. Citizens and stakeholders and invited to provide their feedback by 14 June 2020. EI Expert Volha Litvinets provides an overview and discussion of the paper.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In February 2020 the European Commission released &lt;a href="https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf"&gt;"A White paper on Artificial Intelligence - A European approach to excellence and trust"&lt;/a&gt; aiming to give a definition of AI, underlining it’s benefits and technological advances in different areas, including medicine, security, farming, as well as identifying it’s potential risks: opaque decision making, gender inequality, discrimination, lack of privacy. Based on a &lt;a href="https://ec.europa.eu/digital-single-market/en/artificial-intelligence"&gt;European strategy for AI&lt;/a&gt; presented in April 2018, the current white paper is a complex document analyzing strengths, weaknesses, opportunities of Europe in the global market of Artificial Intelligence. &lt;/p&gt;
&lt;p&gt;In 2018 33 zettabytes of data was produced and it’s expected to exceed 175 zettabytes in 2025. The rapid development of new technologies and the increasing role of AI provokes a global competition and needs a global approach, in which Europe has to identify its own role. The European commission emphasizes creating multidisciplinary international cooperation practices between a private and public sector and academia. Furthermore, AI governance provokes debates and should guarantee maximum multi-stakeholders and multidisciplinarity in the European, national and international levels, and partnership between academia and the private and public sectors.&lt;/p&gt;
&lt;p&gt;The Guidelines of the High-Level Expert Group identified seven key requirements: technical robustness and safety, human agency and oversight, privacy and data governance, transparency, diversity, non-discrimination and fairness, societal and environmental wellbeing, accountability. &lt;/p&gt;
&lt;p&gt;With this, EU embraces the responsibility to addressing risk in the use and development of technologies, which must be developed according to the European values: to promote peace, to offer freedom, security, and justice, sustainable development, to combat discrimination, to ensure scientific and technological progress, to respect the culture and linguistic diversity. &lt;/p&gt;
&lt;h2&gt;Leadership in AI&lt;/h2&gt;
&lt;p&gt;Europe has an advantage for users and for technology development, a strong academic sector, innovative startups, and multiple manufacturing services in the fields of healthcare, finances, agriculture.  Europe is also a leader of AI algorithmic foundations. In addition to this, a quarter of all industrial service robots are produced in Europe. Nevertheless, Europe has a weak position in developing applications for customers, as well as a lack of investment, skills, and trust in AI, which is a significant disadvantage in the use of data assets.  The EU is a global leader in low-power electronics and neuromorphic solution, but the market of AI processor is dominated by non-EU players, European Processor Initiative can change this.&lt;/p&gt;
&lt;p&gt;The objective of the EU now is to become an attractive, safe and efficient data-agile economy, the global leader in AI. With this, the EU wants to make sure the developing technologies will be beneficial for the European citizens, “&lt;em&gt;improving their lives while respecting their rights&lt;/em&gt;”.&lt;/p&gt;
&lt;h2&gt;Increasing investments in AI&lt;/h2&gt;
&lt;p&gt;Over the past three years, EU funding for AI research has increased by 70% compared to the previous period and achieved an amount of €1.5 billion. To compare and show the need of the EU to increase the funds for AI research and development. In 2016 the EU invested in AI €3.2 billion, North America - around €12.1 billion in and Asia - €6.5 billion. &lt;/p&gt;
&lt;p&gt;Europe holds a large amount of under-used public and industrial data and has a secure digital system with low-power consumption. In order to ensure global leadership, the EU supports the investment-oriented approach. Europe needs to significantly increase its investments in this sector and to do so, there is a need to invest in next-generation technologies by mobilizing private and public funding.&lt;/p&gt;
&lt;p&gt;In December 2018 the Commission presented a Coordinated Plan aimed to force the AI progress development in Europe, proposing 70 joint actions in research, funding, market uptake, talent acquisition, international and multidisciplinary cooperation. The plan is to be adopted by the end of 2020. The objective of the European Union is to attract over €20 billions of investment per year from the Digital Europe Program, Horizon Europe as well as from the European Structural and Investment Fund.&lt;/p&gt;
&lt;h2&gt;Human-centric technologies, privacy as a fundamental human right.&lt;/h2&gt;
&lt;p&gt;The technologies have to be developed in compliance with EU rules, protecting fundamental rights and consumers aimed to give citizens confidence in AI systems, “&lt;em&gt;European AI is grounded in our values and fundamental rights such as human dignity and privacy protection&lt;/em&gt;”. With this, Europe wants to ensure the trust to tech by citizens, saying that the &lt;em&gt;trustworthiness necessary components&lt;/em&gt; in the tech development, which is impossible without expandability of opaque technologies, and from another perspective, considering poor awareness of digital users.  &lt;/p&gt;
&lt;h2&gt;Protection of Human Autonomy &amp;amp; Agency&lt;/h2&gt;
&lt;p&gt;While the European citizens are afraid of algorithmic decision-making capacities, countries are struggling with the legal uncertainty. This document states, that &lt;em&gt;AI is a collection of technologies that combine data, algorithms and computing power&lt;/em&gt;. All these three components can be biased, and by following, can lead to the material and immaterial harm and another unpredictable consequence. According to the EU, a significant role to achieve sustainable development goals (SDG), and ensure the democratic process and human rights. There should be concrete actions to protect human agency and autonomy and educate conscious digital citizens.&lt;/p&gt;
&lt;h2&gt;AI Ethics &amp;amp; research fragmentation&lt;/h2&gt;
&lt;p&gt;The complex nature of many new technologies results in cases where AI can be used to protect fundamental human rights, but can also be used for malicious purposes. As was mentioned above, international cooperation on AI matters must be based on the respect of fundamental rights, including human dignity, pluralism, inclusion, nondiscrimination and protection of privacy and personal data. &lt;/p&gt;
&lt;p&gt;The big issue with AI Ethics is a research fragmentation. The current situation with a fragmented knowledge landscape is not acceptable anymore, so it is critical to creating synergies between the multiple European research centers for cooperation in research and will create testing centers. The aim of an updated Digital Education Action Plan is to reinforce tech skills. &lt;/p&gt;
&lt;p&gt;EU position aims to promote the ethical use of AI.  The ethical guidelines were developed by the High-Level Expert Group. EU was also closely involved in developing the OECD’s ethical principles for AI. The G20 subsequently endorsed these principles in June 2019. EU recognizes that important work on AI by UNESCO, the Council of Europe, OECD, WTO, and ITU.  At the UN, the EU is involved in the follow-up of the report of the High-Level Panel on Digital Cooperation supporting regulatory convergence. &lt;/p&gt;
&lt;h2&gt;Legal Framework&lt;/h2&gt;
&lt;p&gt;The purpose of this white paper is to set out policy options and legal frameworks, based on European fundamental values to become a global leader in innovation in the data economy and its applications, and to develop a benefic AI ecosystem for citizens, business and public interest on both national and international levels. The Report, which accompanies this white paper, analyses the relevant legal framework and underlines its uncertainty.  In 2019 over 350 companies have tested this assessment list and sent feedback. A key result of the feedback process is that requirements are already reflected in existing legal or regulatory regimes, those regarding transparency, traceability, and human oversight are not specifically covered under current legislation. &lt;/p&gt;
&lt;p&gt;The regulatory framework requires compliance with EU legislation, principles, and values: freedom of expression, freedom of assembly, human dignity, gender, race, ethnic origin, religion or belief, disability, age or sexual orientation nondiscrimination, protection of personal data and private life. The document says that AI needs to be considered during the whole lifecycle, but machine learning, especially deep learning, presents challenges involving explainability that problematizes compliance with some policy goals. Europe has an academic strength in quantum computing and quantum simulators, and the document encouragis increase in the availability of testing and experimentation facilities in this field.&lt;/p&gt;
&lt;p&gt;In the white paper, it is clear that the EU legislative framework will include legislation. Some specific features of AI (e.g. opacity, complexity, unpredictability, and partially autonomous behavior) can be hard to verify and make the enforcement of legislation more difficult.  As a result, in addition to the current legislation, &lt;em&gt;new legislation specific to AI is needed&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The EU commission underlines the importance of improving digital literacy for all citizens and raising awareness of the issues related to data privacy, transparency, the definition of AI, data governance, responsibility, and trust and dual-use of technologies. The European Commission invited citizens to send comments and possible suggestions regarding this white paper.&lt;/p&gt;
&lt;p&gt;The EC &lt;a href="https://ec.europa.eu/commission/future-europe/feedback-future-europe_en"&gt;invites public comments&lt;/a&gt; by June 14. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>COVID-19 Digital Contact Tracing - Launch it fast and debug it live. What could go wrong?</title><link href="/covid19-dct-wcgw.html" rel="alternate"></link><published>2020-04-14T07:20:00+02:00</published><updated>2020-04-14T07:20:00+02:00</updated><author><name>Andrew Buzzell</name></author><id>tag:None,2020-04-14:/covid19-dct-wcgw.html</id><summary type="html">&lt;p&gt;COVID-19 Digital Contact Tracing - Launch it fast and debug it live. What could go wrong?&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;tldr; We can patch many of the issues with DCT that pose direct ethical problems, but not the deeper misalignment between the technology and the goals we are using it to pursue.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;Recently, there has been a huge amount of enthusiasm for adopting digital contact tracing (DCT), like the TraceTogether app used in Singapore, as part of a test and trace strategy to help relax physical distancing regulations and reopen economies that have been shut down to suppress the spread of COVID-19. Discussion in the AI and technology ethics community has largely divided into what can be described as compatibilist and incompatibilist positions on the possibility of implementing DCT ethically. I think the incompatibilists are right, but that there are better ways of stating the position. The primary incompatibility isn't between DCT and specific ethical challenges familiar to AI ethics, such as protecting privacy. Rather, it's a deeper tension between the limits of the technological tool and the requirements DCT would have to meet to function usefully as part of a test and trace strategy. &lt;/p&gt;
&lt;p&gt;DCT is being developed rapidly, and we will be highly motivated to make it work - billions of dollars will change hands, much of it from public coffers to private corporations, and lives really do hang in the balance. Moreover, as we learn more about the populations most likely to experience the more severe health and economic outcomes from COVID-19, it is apparent that costs and benefits will not be shared equally. &lt;/p&gt;
&lt;p&gt;The AI and technology ethics community has played a prominent role in the development and discussion of the DCT approach, and numerous proposals have been carefully developed in collaborative efforts between technologists and ethicists. Increasingly sophisticated measures to implement contact tracing anonymously such as &lt;a href="https://github.com/DP-3T/documents/blob/master/DP3T%20White%20Paper.pdf"&gt;Decentralized Privacy-Preserving Proximity Tracing&lt;/a&gt; have earned the approval of some ethicists, such as Cansu Canca, the Executive Director of AI Ethics Labs, who recently argued that there should be &lt;a href="https://medium.com/@cansucanca/why-mandatory-privacy-preserving-digital-contact-tracing-is-the-ethical-measure-against-covid-19-a0d143b7c3b6"&gt;mandatory adoption of these systems&lt;/a&gt;, in an approach she calls MPP-DCT (Mandatory Privacy Preserving Digital Contact Tracing). &lt;/p&gt;
&lt;h2&gt;Can DCT be deployed ethically?&lt;/h2&gt;
&lt;p&gt;Ethicists have been very effective bringing concerns about surveillance and privacy into the mainstream of the work on digital contact tracing, and views amongst AI and tech ethicists have largely divided into two camps. There's an incompatibilist outlook, which argues that there is no way to implement digital contact tracing that is compatible with privacy rights, and a compatibilist view, that by using technical and regulatory tools we can in fact deploy DCT in an ethical way. &lt;/p&gt;
&lt;p&gt;The compatibilists have received significant and much deserved airtime within organizations developing DCT apps, and in the broader policy discussion. This is a welcome acceptance of the essential role that ethical oversight must play in the technology development cycle as we slowly absorb the litany of research showing extensive problems with AI systems with hidden biases, unpredictable behaviour, and unanticipated social and personal and economic costs. &lt;/p&gt;
&lt;p&gt;It's not surprising that the incompatibilists receive comparatively less attention and exert less influence. Not only are some of the richest corporations on earth involved in the digital contact tracing project, but they are portrayed as an essential part of plans that could allow governments to relax public health initiatives that have substantially impeded the economic and personal freedom of millions of people. &lt;/p&gt;
&lt;p&gt;Discussion of the incompatibilist position on digital contact tracing seems to focus on a particular kind of "slippery slope" argument. If we allow them to track us to cope with the COVID-19 challenge, what will stop them from expanding this in other contexts? Technologists and ethicists have argued for years that the "If you have nothing to hide, you've got nothing to fear" attitude is problematic, and that privacy tradeoffs tend to end up a lot worse than the initial bargain reveals. This has fallen on deaf ears, in part because the problem with these tradeoffs is that there is a deep information asymmetry. We think we know what we are giving up, and it tends to be pretty innocuous, some photos we post, some basic personal info, some anonymized digital fingerprints. What could go wrong? But what we don't know, and sometimes can't know, is how they will ultimately become part of systems that cause harm, because those systems might not yet exist, or the conditions that make them harmful are not yet in place. &lt;/p&gt;
&lt;p&gt;Slippery slope arguments are vivid, appealing, and easy to devise. But they tend to be speculative and vague, and invite compelling rebuttals. Countering a slippery slope argument is easy - for any imagined bad landing, one might show that there are regulatory, economic, or technical protections that can be put in place, and then the dialectic tends to head towards a back and forth between potential pitfalls and possible solutions. As this goes on, the ground can shift, towards a higher level objection to the slippery slope, that we have to be realistic, we have to deal with the problems we are facing now, and that the argument is ultimately paranoid, contrarian, or unreasonable.&lt;/p&gt;
&lt;p&gt;The compatibilists have excellent answers to many of the slippery slope arguments against DCT. Proposals such as MPP-DCT show that tracking apps can be highly anonymous. The system proposed by Google and Apple also implements technical measures that protect individual privacy, and further innovations will doubtlessly improve these. Emergency public health legislation in effect in most countries with COVID-19 outbreaks have strict limitations that we should, in most jurisdictions, trust to function as intended. Crytopgraphic techniques to create ephemeral device fingerprints can prevent future misuse. &lt;/p&gt;
&lt;h2&gt;First Wave and Second Wave AI and Technology Ethics&lt;/h2&gt;
&lt;p&gt;Frank Pasquale, an expert on the law of artificial intelligence, algorithms, and machine learning,  recently argued that we can distinguish between what he calls &lt;a href="https://lpeblog.org/2019/11/25/the-second-wave-of-algorithmic-accountability/"&gt;"First Wave" and "Second Wave" algorithmic accountability&lt;/a&gt;. First Wave research and policy focussed on fixing the immediate emergencies created by our reliance on systems which don't do what we expect, or don't do it fairly. Second Wave AI ethics asks deeper questions about how we use these systems in the first place - whether a proposed technology can be aligned with the values and norms that matter within the domain in question, whether it should be used at all, rather than whether or not we are doing it fairly. We can fix a lot of First Wave problems for a system which is still fundamentally, morally and pragmatically misaligned. For instance, we might focus on addressing First Wave problems with AI video hiring systems by working to ensure that the data they are trained with is unbiased and used with consent, implementing technical measures to generate interpretable models, and ensuring compliance with labour and data protection regulations. With all of these First Wave problems fixed, we might still find that there is a fundamental mismatch between what the system can measure, and what actually matters when choosing a person for the job. It might just be a technological solution that can't align with our interests in the problem. &lt;/p&gt;
&lt;p&gt;The AI ethics debate about digital contact tracing has been overwhelmingly focused on First Wave problems that concern implementation challenges, such as protecting privacy, addressing data protection and ownership issues, and avoiding off-label reuse of the data collected. What are the Second Wave questions we should be asking?&lt;/p&gt;
&lt;p&gt;Instead of a slippery slope, where we worry about what might come after digital contact tracing, we should be worried about a trojan horse - what comes along with it that we haven't seen? If we incur significant sunk costs to rapidly launch DCT, what other investments will we have to make to fix it at sea, and what policies, technologies, and behaviours will we need to accept? &lt;/p&gt;
&lt;p&gt;It is important to note the economic and epidemiological viability of the test and trace strategy remains in question. For example, researchers at &lt;a href="http://www.centerforhealthsecurity.org/our-work/pubs_archive/pubs-pdfs/2020/a-national-plan-to-enable-comprehensive-COVID-19-case-finding-and-contact-tracing-in-the-US.pdf"&gt;Johns Hopkins estimate&lt;/a&gt; that this approach would require 3.6 billion dollars and 100,000 dedicated employees dedicated to maintain the “human-in-the-loop” capacity that is essential to making DCT signals actionable (indeed, TraceTogether was human-led, and the app itself merely augmented conventional contact tracing). Nobel-Laureate Paul Romer estimates that DCT would require almost &lt;a href="https://paulromer.net/covid-sim-part1/"&gt;22 million tests per day&lt;/a&gt; in the United States, a country with limited access to testing and highly variable access to health care resources. As of April 13, the United States has conducted less than &lt;a href="https://covidtracking.com/data/"&gt;3 million tests in total.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is uncertainty about the effectiveness of diagnostic and antibody tests, and the significant gaps in our understanding of the conditions of transmission. The basic reproduction number of COVID-19 is thought to be R3, which means that for every person that is infected, there will in turn be three more infected people. We contact a lot more than three people in the timespan where we would potentially transmit the virus. We know the virus can persist on surfaces and be transmitted through indirect contact. Unlike other infectious diseases where contact tracing has been used effectively, COVID-19 is contagious long before symptoms appear, enlarging the pool of contacts that must be searched. Digital contact tracing works by using low energy bluetooth signals from mobile phones, which can reach up to 20 feet, through walls, outside vehicles, and which are an imprecise proxy for contact that could lead to infection. Conventional contact tracing, in contrast, is a process conducted by experts with access to a broad spectrum of data about an individual's activities. &lt;/p&gt;
&lt;p&gt;We can look at the broad range of attack patterns familiar in the cybersecurity domain and find analogical threats to DCT, such as the denial of service and wardriving. A digital fingerprint that has been flagged as positive could be deliberately or accidentally  associated with many other devices. (For a taste of some of the vulnerabilities we can expect, see &lt;a href="https://eprint.iacr.org/2020/399"&gt;https://eprint.iacr.org/2020/399&lt;/a&gt;) Cybersecurity has historically been attentive to human factors as well as technical ones, and DCT is vulnerable to social engineering attacks and the tendency of users to quickly discover the dark logics of the systems they are forced to interact with. It won't be long before users learn to manipulate quirks in the system, for instance, it might be beneficial to have been infected so as to be flagged as such in the app. There is now doubt that antibody tests are reliable, and worries that the virus can re-activate after a period of dormancy. &lt;/p&gt;
&lt;p&gt;The reliance of DCT on non-causal predictors will drive iteration of the technology that will demand either the investment significant investment in human oversight, or the aggregation of the baseline proximity signals with other data corpora. This will raise substantial ethical and safety issues. &lt;/p&gt;
&lt;p&gt;Ethical analysis of technology should anticipate the trajectory that development of the system will take after deployment. Identifying bugs and failure modes that will require remediation within the sociotechnical paradigm of the system is a way to form hypothesis about the direction this development will take. &lt;/p&gt;
&lt;h2&gt;Even if DCT has flaws, isn’t it good enough to try?&lt;/h2&gt;
&lt;p&gt;If we launch DCT, these are problems that we will have to debug, and those solutions may have many of the features the incompatibilists are worried about. Will tech platforms need to tie the anonymous bluetooth data to their broader data holdings in order to narrow down contacts that are more likely to correlate with transmission? Will the apps need to have access to other health data in order to ensure that self-reported data is accurate, or make AI assisted medical inferences about users from other proxy signals they collect? Many of our contacts, such as meeting a delivery driver, or pumping gasoline, or running, typically happen without our mobile phones. Perhaps we'll find that we are obligated, morally, legally, or practically, to make sure we are emitting and collecting data at all times. &lt;/p&gt;
&lt;p&gt;As the limits of the actual diagnostic capacity are revealed, the apps might still have pragmatic value as a sort of security theatre, acting as a throttle on access to public spaces even when we know the mechanism isn't particularly reliable. At best it will act as quarantine-roulette, at worse it will exhibit biases that reproduce or amplify existing inequities. &lt;/p&gt;
&lt;p&gt;This isn't a slippery slope argument, it's an argument about the implication we are committed to when we adopt the logic of this technological perspective on the problem. When you build a piece of software, there's a blurred line between fixing bugs and adding features. If we want to automate the analysis of proxy signals to determine who is eligible for relaxed physical distancing requirements, using the technologies and platforms we have today, these are some of the implications that come along with it. &lt;/p&gt;
&lt;p&gt;Digital contact tracing might be a useful source of aggregate data, for macro-scale epidemiological modelling, but there are good reasons to doubt that we can use them as a reliable way of making decisions about individuals. This is not an uncommon problem with the use of big data and artificial intelligence, where we gain the capacity to generate knowledge that we often lack the resources to act on in an ethical or effective way. And yet, if we build the system, we acquire ethical and sometimes legal obligations to take responsible action with the results. It's not clear that we can with DCT, and the solutions available to try to remediate that are problematic. &lt;/p&gt;
&lt;p&gt;One might object that this overlooks the positive contribution even imperfect DCT can make. Even though we know that non-medical masks aren't reliable at preventing transmission, we endorse their use because they still help. The difference is of course the downstream commitment - with masks there are few, but with DCT they are substantial. Masks have a measurable impact on transmission, and the commitments that go with them depend on regulatory decisions about mandate and enforcement. It is true that in some places masks have been mandatory where as access to them is poor, and handing out masks would be a better exercise of government power than writing fines. But on the whole, there aren’t a lot of troubling implicated commitments with policies that encourage or mandate mask usage. For DCT, on the other hand, they are significant commitments which will be technically and politically costly to solve, and which pose substantial ethical and practical challenges. Even if one believes that efficacy has presumptive justificatory force, the degree of efficacy would need to be correspondingly high to support the full social and economic costs we would incur to implement even a plausibly effective DCT test and trace strategy. &lt;/p&gt;
&lt;h2&gt;The problems with DCT are deeper than the sort of privacy issues current proposals focus on solving.&lt;/h2&gt;
&lt;p&gt;AI and tech ethicists should embrace these Second Wave questions about the appropriate use of technology. Often First Wave issues are more tractable, and easier to solve within the institutional mandates technology ethicists operate with as part of collaborative efforts with diverse stakeholders, often spanning private and public interests. Second Wave perspectives are more likely to cast doubt fundamental assumptions about the viability of projects as a whole and the likelihood of success. We risk ethics-washing when we focus too closely on patching issues on the edges of systems that have deeper tensions with our goals and values. There was controversy this winter when the &lt;a href="https://medium.com/@emilymenonbender/is-there-research-that-shouldnt-be-done-is-there-research-that-shouldn-t-be-encouraged-b1bf7d321bb6"&gt;GermEval 2020 machine learning contest&lt;/a&gt; included a task to develop AI that makes inferences about the “intellectual ability” of the authors of text samples. Some ethicists warned that even if we solved ethical problems at the periphery, there are fundamental ethical issues baked into problem definition itself, that should cause us to object to building software for the problem at all. Sometimes we can fix the bugs, but deeper problems with the problem definition or the limits of our tools mean we still can’t perform the task ethically. Second Wave algorithmic accountability challenges us to pay attention to these cases. &lt;/p&gt;
&lt;p&gt;The implementation of DCT will require a significant exercise of political will and economic investment that could perhaps be deployed more effectively in other ways with more empirically demonstrable likelihood of having positive impacts, such as improving access to health care services, tackling known social determinants of poor outcomes, addressing the economic costs of the distancing, and putting into action the well-established practices and protocols that will help respond to future pandemics. Of course, it’s a poor argument to claim that we can’t fix these things and also implement DCT, but in practice, political and economic capital for public health initiatives is a precious and limited resource, and we should be cautious about how we spend it. “It won’t work” isn’t just a practical problem when the measures we’ll need to take to make it work have serious ethical challenges, and even more so when some of the bugs carry serious moral risks. &lt;/p&gt;
&lt;p&gt;We might still be optimistic that AI and big data could be part of the solutions to these problems, even if we are skeptical about the value of implementing DCT. &lt;/p&gt;
&lt;p&gt;This point bears repeating. In the United States &lt;a href="https://www.propublica.org/article/early-data-shows-african-americans-have-contracted-and-died-of-coronavirus-at-an-alarming-rate"&gt;evidence is mounting&lt;/a&gt; that the heaviest impacts of COVID-19 will be on the most vulnerable populations. &lt;a href="https://www.nytimes.com/2020/04/05/opinion/coronavirus-social-distancing.html"&gt;"What the vulnerable portion of society looks like varies from country to country, but in America, that vulnerability is highly intersected with race and poverty."&lt;/a&gt;. &lt;a href="https://thestarphoenix.com/opinion/columnists/cuthand-covid-19-situation-on-u-s-reserves-is-cautionary-tale/"&gt;"The rate of COVID-19 infection is eight times higher on the Navajo Nation and the death rate is 16 times higher than the rest of New Mexico."&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In Canada, where I live, we've had a heartbreaking series of news stories about terrible conditions in senior care homes with extreme levels of COVID-19 infection, highly limited capacity for treatment and care, and terrible outcomes. I recently had a chance to hear first hand from a RN at senior care facility how dire this situation is, in a part of our society that is in many ways hidden. &lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://whyy.org/articles/a-disturbing-medical-consensus-is-growing-heres-what-it-could-mean-for-black-patients-with-coronavirus/"&gt;thoughtful essay on managing scarce health care resources&lt;/a&gt;, Dr. Hannah C. McLane reminds us that saving the most lives involves value-laden judgements. Using tools like DCT to accelerate efforts to relax measures that are currently slowing the rate of infection will amplify existing inequities in our allocation of public goods. You might think that's a distal problem, an unfortunate reality, and one that shouldn't weigh too heavily against broadly utilitarian calculations that are inescapable in &lt;a href="https://www.ncchpp.ca/docs/2016_eth_frame_upshur_En.pdf"&gt;public health ethics&lt;/a&gt;. But we should pay close attention to the extent to which the technological paradigms with which we approach problems constrain these choices, tilting the scale so that making decisions that save the most lives with the least cost just happen to favour one kind of life, and risk another. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Deployment of AI Systems: “Assistive” Technology</title><link href="/03-04-2020-assistive-tech.html" rel="alternate"></link><published>2020-03-04T07:20:00+01:00</published><updated>2020-03-04T07:20:00+01:00</updated><author><name>Sam Meeson-Frizelle</name></author><id>tag:None,2020-03-04:/03-04-2020-assistive-tech.html</id><summary type="html">&lt;p&gt;AI &amp;amp; Disability: Challenges of developing "Assistive" technology&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Conversations about AI and Disability tend to focus on the ways in which AI is used in the deployment of technological tools that help disabled people to perform tasks they otherwise would not be able to carry out. Recently, this idea of help has seeped into common usage with the term “assistive” technology - a notion that is burdened with assumptions about disability which shed light on deep rooted social injustices that I will later explore.&lt;/p&gt;
&lt;p&gt;Deployment of so-called assistive tech is widespread and there are many cases where they allow disabled people to participate more fully in society. The National Theatre, for example, offers a pair of ‘Smart Caption Glasses’ for deaf people. These glasses are essentially a speech recognition tool that uses a natural language processing (NLP) AI model and provides live captioning for what is in the visual field so that deaf people can enjoy a performance. &lt;/p&gt;
&lt;p&gt;This is just one example but such language processing models are used for tools that are deployed more widely and at scale to help persons - whose disability affects their audio/visual skills - perform necessary everyday tasks.&lt;/p&gt;
&lt;h2&gt;Ways in which Assistive Technology Benefits Society&lt;/h2&gt;
&lt;p&gt;It’s clear that assistive technologies have a profound impact on the daily life of users. On Scope UK’s website, a disability equality charity, you see users of assistive technology talk about the instrumental role certain tools play in their lives. Raisa, whose disability means she cannot physically type with any proficiency, talks of her reliance on Apple’s voice recognition software to perform her “most important job” of “dictating and replying to emails”. For her, such assistive technology is paramount and she believes that it can “help you live the life you chose to live.”&lt;/p&gt;
&lt;p&gt;Assistive tech is not limited to physical disabilities. “&lt;a href="http://noisolation.com"&gt;No Isolation&lt;/a&gt;”, a Norwegian health-tech start-up, deploys what we might call an assistive tool to help address social isolation and loneliness. According to No Isolation’s research, two of the most vulnerable groups are young children with long-term illness and those who are over 80 years of age. No Isolation addresses this vulnerability for young children, by deploying it’s AV1 robot to assist those who cannot physically participate at school. The robot attends classes by proxy, employing NLP and machine vision AI systems that allow the child to engage with the teacher and with the other children.  &lt;/p&gt;
&lt;h2&gt;Challenging The Notion of “Assistive” Tech - The Language of Assistance&lt;/h2&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;Whilst such technologies appear to be taking a positive step to bring about equality and opportunity for disabled people, we might want to challenge the common usage of the term “assistive”. &lt;/p&gt;

&lt;p&gt;As Richard Ladner from the University of Washington points out, the term “assistive” technology is in some respects redundant. It’s hard to think of an example in which technology is not assistive in some way. Technology, generally, makes certain tasks possible or easier to do. So what is the distinction between  “assistive” tech and plain old tech?&lt;/p&gt;
&lt;p&gt;This is not immediately clear. When we think about people who have “Correctable Vision” - a condition not technically considered a disability - the glasses or contact lenses that help them to improve their vision are not commonly described as “assistive technology”. Yet surely glasses or lenses are in fact assistive. This begs the question which Ladner asks: “Why is it that people with disabilities have assistive technology while the rest of us just have technology?”&lt;/p&gt;
&lt;p&gt;Given the redundancy of word assistive, the term assistive technology seems to indicate that disabled people require lots of extra help - evoking a sense of dependence and a lack of capability.  As Ladner points out, it seems inherently paternalistic to say that disabled people receive assistance and it fundamentally challenges their identity, freedom and agency humans - ultimately it diminishes the autonomy individuals have over their actions.&lt;/p&gt;
&lt;p&gt;Moreover, the idea of assistive technology highlights a certain “quick fix” attitude to technology which Mara Mills, Associate Professor of Media, Culture and Communication at NYU, claims ignores important advances of “education, community support and social change”.&lt;/p&gt;
&lt;p&gt;These concerns raise some weighty concerns for disability and AI. Notably, the idea that the deployment of assistive tech carries a serious threat of power asymmetry between those who design and deploy AI systems and those for whom it is made.&lt;/p&gt;
&lt;p&gt;This opens the door to further scrutiny around the design of AI systems that fuel these tools - time to get the magnifying glass out.&lt;/p&gt;
&lt;h2&gt;Design and Disability&lt;/h2&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;When we think about the design stage of AI systems, specifically those that apply machine learning methods, it’s important to consider the nature of the dataset that an AI model is being trained on. &lt;/p&gt;

&lt;p&gt;In general being excluded from the training data in an AI model can cause problems. Concretely, if an AI system is, for example, trained on a data set that has no images of bald people, then bald people will be missing from the AI model. As a result, the AI system won’t recognise bald people - which would, let’s say, make it almost impossible for a machine vision AI tool to hunt down a Jason Statham or Bruce Willis. &lt;/p&gt;
&lt;p&gt;Moreover, the datasets that many AI systems use unfairly represent a number of groups - most commonly on the grounds of race, gender or disability. Unfair representation is a reflection of how people in these groups have been subject to historic marginalisation and discrimination.&lt;/p&gt;
&lt;p&gt;These historic patterns are imprinted in the AI model datasets, which in turn are used to train an AI system - resulting in a so-called “algorithmic bias”. Ultimately, this bias leads to yet more unfair outcomes for people in these groups - pouring ever more fuel on the discriminatory fire. &lt;/p&gt;
&lt;p&gt;AI Now, a NYU research body addressing the societal impacts of technology, calls this vicious circle  “discriminatory logics” - that is to say: “those who have borne discrimination in the past are most at risk of harm from biased and exlusionary AI in the present”. This ongoing pattern reveals an inherent toxicity in AI systems for disabled people that has been at the forefront of wider AI bias concerns.&lt;/p&gt;
&lt;p&gt;Moreover, it is particularly worrisome that the AI systems which use these encoded “discriminatory logics” carry a certain unchallengeable authority. There are countless cases of disabled individuals being discriminated against by AI systems in high-stakes decision making. &lt;/p&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;For example, as Kathryn Zsykowski observes with Amazon Mechanical Turk, a crowdsourcing marketplace for outsourcing virtual tasks, certain disabled clickworkers are unable to pass the CAPTCHA (a common reverse Turing Test that helps to prove an individual's humanity) or cannot complete work quickly enough so they are consequently rejected by the platform.&lt;/p&gt;

&lt;p&gt;We need to be really careful about how we assign authority to technology. In the case of decision-making AI systems, we might think their authority often hinges tenuously on the fact that they are the product of powerful companies who employ clever people to create technology that very few people actually understand. &lt;/p&gt;
&lt;p&gt;Immediately, this raises concerns of explainability and accountability around AI: we need to direct our attention to the companies that produce these tools and a) demand an explanation of the decisions that the system makes but, more importantly, b) hold them accountable when the decisions are discriminatory. &lt;/p&gt;
&lt;p&gt;The idea that misplaced authority leads to discrimination against disabled persons reveals a deeper social injustice in the AI realm. That is: there is an imbalance in power between those who design and deploy AI systems and those who are “classified, ranked and assessed by these systems”. Ultimately, as I will suggest, to address this imbalance we need to look at concerns of AI bias and disability in tandem with these deep rooted social injustices. &lt;/p&gt;
&lt;p&gt;In the wider tech ethics arena most of the discussion and many of the headlines are focussed on the axes of gender and racial bias, with comparably little literature discussing the treatment of disabled persons in the face of algorithmic bias. &lt;p style="font-size: 18px;font-weight: bold"&gt;As AI Now points out in its report: “disability has been largely omitted from the AI bias conversation”. &lt;/p&gt;&lt;/p&gt;
&lt;p&gt;By and large, this statement is justified. Perhaps more importantly though, even when disabled persons seem to be part of the conversation, they are not properly included and the problems they face are not addressed in the right way. Most commonly, disabled persons are not included in the right way because of exclusion and “unfair representation”.&lt;/p&gt;
&lt;h2&gt;Issues of Exclusion and Unfair Representation&lt;/h2&gt;
&lt;p&gt;One example in recent years where exclusion from training data had a fatal impact involved an autonomous Uber vehicle that hit and killed Elaine Gerzberg in 2018 as she pushed her bike across the road. &lt;/p&gt;
&lt;p&gt;In this case, the system’s training data clearly did not include enough images of a person pushing a bike, which lead to confusion for Uber's pedestrian recognition system. Not having enough representations in this case echoes the Jason Statham example above in the way that it is seemingly “unfair”. &lt;/p&gt;
&lt;p&gt;If able bodied pedestrians are at risk from misrecognition due to exclusion and unfair representation in a dataset, it’s vital that we ask ourselves how we can avoid this happening for disabled people in wheelchairs or mobility scooters?&lt;/p&gt;
&lt;p&gt;One solution might be to concentrate our design efforts on fairly representing disabled persons in the training data. We might suggest that a fair representation of disable persons equates to something like comprehensive representation  - which fully and correctly classifies all kinds of disability. &lt;/p&gt;
&lt;p&gt;Clearly this is easier said than done.&lt;/p&gt;
&lt;p&gt;Disability is such a fluid and vast concept that encompasses an almost immeasurable range of physical and mental health conditions, and that can come and go throughout time. This means there are so many outliers and often no two disabilities are the same. As Dr. Stephen Shore famously said: “if you’ve met one person with autism, you’ve met one person with autism”. &lt;/p&gt;
&lt;p&gt;Such fluidity is in direct conflict with the rigidity of AI systems. What this means is it is hard to concretely account for the multitude and variation of disabilities, which makes mapping disabilities onto AI model classifications seemingly impossible.&lt;/p&gt;
&lt;p&gt;Moreover, as AI Now points out, even if we could account for such fluidity, to achieve fair representation “may require increased surveillance and invasion of privacy in the process” - opening another can of ethically questionable worms. &lt;/p&gt;
&lt;h2&gt;Can We Move Towards Inclusion and Fair Representation?&lt;/h2&gt;
&lt;p&gt;One option for solving the AI bias issue is to take a technical approach: classifying people into a single variable, for example race or gender, and then testing the system by applying a number of methods to see if it works across a variety of people.&lt;/p&gt;
&lt;p&gt;This common technical approach is limited and simply won’t do for disability. Firstly, for the reasons of fluidity mentioned above. Secondly, and more importantly, if we take a social model of disability in which we understand it as a “product of disabling environments and thus an identity that can only be understood in relation to a given social and material context” we see that it’s very difficult to find technical fixes for social problems. &lt;/p&gt;
&lt;p&gt;Another way  to mitigate the issues of systemic algorithmic misrepresentation is to collect more data representing disabled people. However, simply augmenting the data collection process raises issues again of how this data is collected and also how it is classified. &lt;/p&gt;
&lt;p style="font-size: 18px;font-weight: bold"&gt;For example, there are a number of grassroot collection efforts within the disability community to gather data in the hope of better understanding and addressing their health. But in such cases where healthcare isn’t guaranteed, it is difficult, as AI Now reports, to “ensure that such data won’t be reused in ways that could cause harm” - even if that was not the original intention. &lt;/p&gt;

&lt;p&gt;Moreover, the workshop, which the AI Now report emerged from, revealed that much of the effort to increase the data representation of disabled persons is performed by “clickworkers” who “label data as being from people who are disabled based on what is effectively a hunch”. Paradoxically here, the effort to include more disabled people is stunted by the classification of categories that “effectively exclude many of those they are meant to represent”. &lt;/p&gt;
&lt;h2&gt;Going Back to the Drawing Board&lt;/h2&gt;
&lt;p&gt;Although the development of technological tools that help disabled people to participate in society is seemingly beneficial and perhaps life changing, the impact on the wider disability movement is momentary.&lt;/p&gt;
&lt;p&gt;As I looked, in this article, at AI and disability through the critical lens of philosophy - I saw the issues of “assistance” and bias that reflected deeply rooted social concerns.&lt;/p&gt;
&lt;p&gt;To properly address the issue of bias and disability, specifically addressing the concerns of algorithmic marginalisation and discrimination, we need to focus on disability in it’s social and environmental context. This is no small task and probably beyond the scope of this article. That said, there is slowly more and more work being done to take steps in the right direction.&lt;/p&gt;
&lt;p&gt;The AI Now paper cited in this article is a great resource for an overview of the challenges that face the disability community with respect to AI. It is also particularly constructive in the way it draws our attention to the vital need to include disabled persons in the design stage of tech development. &lt;p style="font-size: 18px;font-weight: bold"&gt;The resounding message from AI Now is that disabled persons should be designing “with, not for” the mantra of the disability movement: “Nothing About Us Without Us”.&lt;/p&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>The Ethics of Data Processing: Comparing the CCPA and GDPR</title><link href="/02-12-gdpr-ccpa.html" rel="alternate"></link><published>2020-02-12T07:20:00+01:00</published><updated>2020-02-12T07:20:00+01:00</updated><author><name>Annie Valentine</name></author><id>tag:None,2020-02-12:/02-12-gdpr-ccpa.html</id><summary type="html">&lt;p&gt;GDPR Lite? A comparison of the CCPA and GDPR&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;Why the California Consumer Privacy Act, why now?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://ccpa-info.com/california-consumer-privacy-act-full-text/"&gt;California’s CCPA&lt;/a&gt; came into force January 1st 2020, becoming fully enforceable penalties and all, July 1st 2020. &lt;/p&gt;
&lt;p&gt;The act has been endearingly referred to as &lt;em&gt;GDPR Lite&lt;/em&gt;, a more laid-back version of the GDPR that could be passed through California’s legislative process with more minimal protest from stakeholders. The CCPA applies against a backdrop of diverse businesses processing personal data. &lt;/p&gt;
&lt;p&gt;In anticipation of getting a piece of the new “sharing economy,” countless businesses have cropped up across California which capitalise on the opportunity to cut out the middle-men of their industry. &lt;/p&gt;
&lt;p&gt;Business models in the sharing economy bring goods and services directly to consumers, without the stress and liability of managing the aforementioned goods or services themselves. Think Airbnb, which is known as the “The world’s largest hotel which owns no property.” Or Uber, which is the largest ride-sharing service in the world, yet owns no cars and (&lt;a href="https://www.theverge.com/2019/3/12/18261755/uber-driver-classification-lawsuit-settlement-20-million"&gt;so far&lt;/a&gt;) employs no drivers. &lt;/p&gt;
&lt;p&gt;Companies participating in the sharing economy rely on data-driven methodologies to maintain healthy profit margins. Facebook can afford to provide you with a “free” service because they profit from the data you generate while engaging with their services. FB’s overall value increases as you post content. California’s Silicon Valley culture has sent a resounding message – &lt;em&gt;data is the gift that keeps on giving&lt;/em&gt;. With a sea of new start-ups relying on data to bolster fragile business models, it’s no wonder that the first US state to enact its own privacy legislation is California. The privacy risk consumers face has increased exponentially. &lt;/p&gt;
&lt;p&gt;&lt;b&gt;59%&lt;/b&gt; &lt;em&gt;of companies believe that data and data analytics are vital to their organisation&lt;/em&gt;&lt;br&gt;
&lt;b&gt;29%&lt;/b&gt; &lt;em&gt;of businesses surveyed believed the “big data” allowed them to generate new revenue from existing products or services&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://eiuperspectives.economist.com/technology-innovation/business-data-0"&gt; The Economist, The Business of Data, Intelligence Report, The Economist Intelligence Unit (EIU) 2016&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The dangers of the digital economy&lt;/h2&gt;
&lt;p&gt;While most digital consumers are wary the moment they input a credit card or phone number online, they may not be aware of the technicalities of data processing, understand nuances of the legal framework, or be familiar with technical terms like “shadow profiling.” &lt;/p&gt;
&lt;p&gt;A shadow profile is made up of data harvested from a multitude of sources, collected as you engage with websites, apps and other digital services. Data is combined by linking matching data points. Accurate estimates can then be made about who you are – your gender, health, interests, socio-economic status, and even your criminal background may be accurately determined.  &lt;/p&gt;
&lt;p&gt;We are also surrounded by a multitude of sensors day-to-day that may be collecting data. Your phone, personal assistant, discounted TV nabbed in the Black Friday sale, and even your refrigerator or vacuum cleaner may contain sensors. &lt;/p&gt;
&lt;p&gt;The consequences of loss of personal data have devastating effects, including, as &lt;a href="https://www.eclypses.com/wp-content/uploads/2019/10/CCPA_Regulations-SRIA-DOF.pdf"&gt;California’s DOJ remarks&lt;/a&gt; “financial fraud, identity theft, unnecessary costs to personal time and finances… reputational damage, emotional stress, and even potential physical harm.” The DOJ also cited that there is a pressing need for enhanced consumer privacy rights, as “neither state nor federal law have kept pace with these [technological] developments in ways that enable consumers to exert control over the collection, use, and protection of their personal information.” &lt;/p&gt;
&lt;p&gt;One of the biggest concerns for consumers is how to prevent data from being sold to and used in unpredictable ways by third, fourth, and fifth parties, as it’s sold on ad infinitum. Chances are, if you are not paying a subscription for your software-heavy product, your data is being used to fill the profit-gaps. The CCPA is intended to provide traceability for this very reason.  &lt;/p&gt;
&lt;p&gt;In a &lt;a href="https://oag.ca.gov/sites/all/files/agweb/pdfs/privacy/ccpa-isor-appendices.pdf"&gt;statement of reasons&lt;/a&gt; for the CCPA, the California legislature notes that “although California has been a leader in privacy protection, the law has not kept pace with rapid technological developments and the proliferation of personal information that fuels the internet economy. As a result, consumers are largely unable to control or even understand the collection and use of their personal information by a myriad of businesses and organizations.” &lt;/p&gt;
&lt;h2&gt;Isn’t the CCPA just another GDPR? Think again...&lt;/h2&gt;
&lt;p&gt;The intention of the CCPA echoes that of the GDPR. The GDPR states in its preamble that “[technological] developments require a strong and more coherent data protection framework in the Union...natural persons should have control of their own personal data.”&lt;/p&gt;
&lt;p&gt;How do the rights granted to consumers in the GDPR compare to those under the CCPA? 
On the face of it, the rights granted to California’s consumers mirror those enumerated in the GDPR. The CCPA also includes the right to be informed about what data is being collected, the right to be informed about how and why your personal data is being used and sold, the right to request deletion of your data, the right to request that companies do not sell personal data, and protections for consumers against discrimination for exercising these rights among other rights. &lt;a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB1121"&gt;You can read the full act here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are other similarities. The European Data Protection Supervisory (or EDPS) has remarked that access to a service must not be made conditional upon the individual being forced to ‘consent’ to being tracked and prevents discrimination. Both the GDPR and CCPA explicitly prohibit discriminatory practices against consumers who choose to exercise their privacy rights. &lt;/p&gt;
&lt;p style="font-size: 18px"&gt;&lt;b&gt;However, being compliant with the GDPR does not mean you are automatically compliant with the CCPA. &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Prior to July of this year, businesses will need to prepare on the ground by, for example, reviewing their site designs and updating their records of data processing. Consumers who expect the CCPA to protect them to the same degree as the GDPR will need to manage high expectations.&lt;/p&gt;
&lt;p&gt;The CCPA is much more narrow in scope than the GDPR, excluding certain types of data such as health information, information related to vehicles, or information related to credit worthiness. &lt;/p&gt;
&lt;p&gt;Additionally, the CCPA applies only to businesses that meet certain thresholds – the most notable being that 50% or more of a business’ revenue must be derived from selling personal information. &lt;em&gt;“Selling” data doesn’t just cover cash in exchange for data, but is wide enough to cover other processes deriving value from data.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;Unlike the GDPR, which is more free-form in its requirements as to how businesses establish opt-in consent (e.g. opt-ins can be a tick-box, or an “I agree” button), the &lt;em&gt;CCPA requires that all websites have a link or logo reading “do not sell my personal information.”&lt;/em&gt; This logo may look jarring to consumers, as it makes it obvious that websites are selling consumer data. &lt;/p&gt;
&lt;h2&gt;The culture of privacy; is data privacy the same in the USA as in Europe?&lt;/h2&gt;
&lt;p&gt;The EU and USA have markedly different privacy cultures. &lt;a href="https://www.asser.nl/asserpress/books/?rId=12915"&gt;Experts argue&lt;/a&gt; that the EU relies more heavily on written laws to set privacy standards, the legislature pushing specific risk-management rules down onto businesses who view privacy compliance as a legal jargon, an administrative burden. &lt;/p&gt;
&lt;p&gt;By contrast, and “particularly in the United States, there is a surprising deep chasm between privacy law in the books and privacy practice on the ground…the focus is on a reputation-based approach.” In the US, privacy is more closely related to corporate responsibility than litigious risk-management. Regulators push the onus of responsibility onto businesses to determine and drive the appropriate privacy protections within their organizations.    &lt;/p&gt;
&lt;p&gt;This can be further demonstrated via the “data as a commodity” argument, which is gaining steam in the USA. This is the idea that data should be viewed as an asset you own and license others to use. Prohibited in earlier versions of the act, the CCPA allows &lt;a href="https://oag.ca.gov/sites/all/files/agweb/pdfs/privacy/ccpa-isor-appendices.pdf"&gt;financial incentives or price differentials to compensate&lt;/a&gt; for profit gaps. If a consumer doesn’t want to consent to having their data used, they can just pay money to avoid compromising their data.&lt;/p&gt;
&lt;h2&gt;How American business empires were built on consumer data and loose terms of privacy&lt;/h2&gt;
&lt;p&gt;The payment model is currently being marketed as a potential legal “solution” for businesses who have built fragile business models that mostly rely on data to generate profit. A great example of a business with such a model is Google. Google offers a free service directly to the consumer – you can Google something, use services like Gmail, Google Maps and Google docs without a monetary charge. Their business model is fragile because it relies on data, and without data there is no business.&lt;/p&gt;
&lt;p&gt;Google can afford to stay on the forefront of technological development, have brick-and-mortar locations globally employing thousands of people, and provide consumers with top-tier services and ongoing support by using the data they collect from you. In 2018, &lt;a href="https://www.statista.com/statistics/266206/googles-annual-global-revenue/"&gt;85% of Google’s revenue&lt;/a&gt; came from ads. Let’s look at targeted ads as an example. &lt;/p&gt;
&lt;p&gt;Targeted ads work by collecting, analyzing, and processing consumer data. This data processing is used to generate ads that are relevant to you. Data may be disclosed or sold to third parties. If the legal privacy landscape changes significantly, prohibiting Google from using and selling personal data as it can be done today, the business model collapses.  &lt;/p&gt;
&lt;p&gt;A great example of what would happen if the tide were to turn in favor of stronger privacy regulation can be demonstrated by the recent senatorial hearings with Facebook, following the &lt;a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal"&gt;Cambridge Analytica data scandal&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;During the hearings, &lt;a href="https://www.washingtonpost.com/news/the-switch/wp/2018/04/10/transcript-of-mark-zuckerbergs-senate-hearing/"&gt;Senator Grassley asked Mark Zuckerburg&lt;/a&gt; “Are you actually considering having Facebook users pay for you not to use the information?” He replied “I think what Sheryl &lt;a href="https://www.wsj.com/articles/facebooks-mark-zuckerberg-hints-at-possibility-of-paid-service-1523399467"&gt;Facebook’s COO&lt;/a&gt; was saying was that, in order to not run ads at all, we would still need some sort of business model.” &lt;/p&gt;
&lt;p&gt;Mark Zuckerburg stated that FB’s business model would fall apart if the company was prohibited from running ads. He implied that the burden of making up the loss in profits could go to consumers, perhaps by FB’s adopting of a “data as a commodity” approach.  &lt;/p&gt;
&lt;h2&gt;What snail beer can teach us about the ethical obligations of data processing&lt;/h2&gt;
&lt;p&gt;I can imagine Grassley admired the &lt;em&gt;chutzpah&lt;/em&gt; – no other industry would have the tenacity to argue such a point. On the face of it, it seems like a reasonable business solution. But let’s put it in the context of consumer protection law in food standards, from one of the best-known Scots law cases – &lt;a href="https://www.bailii.org/uk/cases/UKHL/1932/100.html"&gt;Donaghue vs Stevenson&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Mrs. Donaghue was treated to a ginger-beer by her friend in Wellmeadow Café in Paisley, one afternoon in 1928. Much to the horror of everyone involved, a lone decomposed snail was found at the bottom of her beer, eventually resulting in Donaghue’s hospitalization from gastroenteritis and shock. The case was fundamentally about negligence, as calculable harm was caused to Mrs. Donaghue. Donaghue vs Stevenson set off a chain-reaction that eventually resulted in the modern concept of “duty of care” toward consumers.   &lt;/p&gt;
&lt;p&gt;Now let’s say the “&lt;em&gt;ode-d’escargot&lt;/em&gt;” ginger-beer represents Facebook processing data negligently, such as selling data to third parties like Cambridge Analytica without appropriate controls. Mrs. Donaghue represents the consumer, harmed by the detrimental data processing.  &lt;/p&gt;
&lt;p&gt;The equivalent would be like if the drinks manufacturer, Mr. “Facebook” Stevenson, built his entire business model on the continuous sale of snail beer, maintaining a predilection for not telling his customers about the snails. Imagine if Stevenson testified in court that “snails” were his thing, and not only was he not prepared to stop selling snail beer, but henceforth intended to charge all of his customers who didn’t want a snail in their ginger beer extra for the privilege. &lt;/p&gt;
&lt;p&gt;After all, how else was he going to pay to keep the Café doors open after &lt;em&gt;all of this negative press&lt;/em&gt;? &lt;/p&gt;
&lt;h2&gt;CCPA: A step in the right direction, but not a complete solution&lt;/h2&gt;
&lt;p&gt;Propositions for a payment model have raised a number of &lt;a href="https://www.eclypses.com/wp-content/uploads/2019/10/CCPA_Regulations-SRIA-DOF.pdf"&gt;equity concerns&lt;/a&gt; for the DOJ, as “low-income groups may be more likely to give up their personal information in exchange for services while high-income groups will pay the service fee to protect their data.” Big data is arguing for “&lt;a href="https://www.economist.com/the-world-if/2018/07/07/data-workers-of-the-world-unite"&gt;Technofeudalism&lt;/a&gt;.” To continue with Facebook as the provocative example…&lt;/p&gt;
&lt;p&gt;Facebook represents the crown: the technology creator and controller. Since Facebook determines access to, use of and service conditions for their platform, they also represent the nobility in this example. &lt;/p&gt;
&lt;p&gt;FB users upload their own content to the platform, a process which NY times bestseller &lt;a href="http://www.roughtype.com/?p=634"&gt;Nicholas Carr&lt;/a&gt; cleverly named “digital sharecropping.” Creating content increases the value of Facebook overtime– generating profits for the company that consumers will never see. Ultimately, consumers pay for access to the platform with their data (and maybe in the future, with cash or check too). The consumer ultimately represents the peasantry, and always gets the short-end of the stick. &lt;/p&gt;
&lt;p&gt;If differential pricing models become an industry standard, the CCPA could unintentionally contribute to a ‘privacy class system,’ where higher socio-economic groups are able to pay to protect their personal information and disadvantaged groups have no choice but to allow their data to be used. &lt;/p&gt;
&lt;p&gt;Still, these services are so engrained into our education system, jobs, and daily lives that any move to regulate them has to navigate the complex short and long-term consequences diligently. Some of these technologies are so pervasive that to not have access to them puts consumers at a measurable social or financial disadvantage. &lt;/p&gt;
&lt;p&gt;While the CCPA is a step toward greater transparency and moves some power back into the hands of the consumer, there is still work to be done. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Remember, terms and conditions may change.&lt;/em&gt;&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Interview with Will Griffin, VP of Ethics &amp; Diversity in AI, Hypergiant Industries</title><link href="/02-08-2020-interview-hypergiant.html" rel="alternate"></link><published>2020-02-08T07:20:00+01:00</published><updated>2020-02-08T07:20:00+01:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2020-02-08:/02-08-2020-interview-hypergiant.html</id><summary type="html">&lt;p&gt;Some thoughts about the interface between programmers and tech ethics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p style="float: right;margin-left: 20px;margin-bottom: 15px;"&gt;&lt;img alt="Alt Text" src="/images/will.jpg"&gt;&lt;/p&gt;

&lt;p&gt;Hypergiant Industries builds AI systems for enterprises and governments around the world. We were fortunate to have the chance to interview Will Griffin, Hypergiant’s VP of Ethics and Diversity, to talk about the challenges of bringing ethical awareness to the day to day operations of a major AI systems developer. &lt;/p&gt;
&lt;div style="border-bottom: 1px solid #666;margin-bottom: 30px;padding-bottom 20px;width: 50px"&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;As the VP of Ethics and Diversity in AI at Hyper Giant, you have joined a small but growing number of professionals with ‘ethics’ in their title. What led you to taking this position with a growing tech startup?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Our Co-Founder and CEO, Ben Lamm’s vision of “delivering on the future we were promised”, inspired me. Ben is an uber-successful serial entrepreneur with multiple exits to his credit. I know Hypergiant will be successful and, combined with a heavy emphasis on ethics; we have the potential to re-shape the way technology companies think about their obligations to society.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Going back even before your current position, what aspect of AI Ethics first caught your attention, and how has that shaped your view of technology?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Over my career I have primarily worked at the intersection of media and technology (including launching the first original content on the PlayStation Network and the first video on demand channel on cable with Comcast). There was very little industry regulation and the goal was always to be first.  We followed the Silicon Valley mantra of “go fast and break things.” Unfortunately, we have seen the negative impacts of that approach – especially on privacy. Since AI has the capacity to transform society in ways not previously imagined, I feel an obligation to be a part of making sure AI innovation works for the benefit of humanity.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What has surprised you most about your work as the VP of Ethics?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I have been surprised by two things. 1) The amount of emphasis and demands the public and policymakers are now placing on ethics in technology  and 2) the number of technologists willing to admit how little they know about embedding ethics in their workflows and the products they make.&lt;/p&gt;
&lt;h2&gt;AI Code of Ethics&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Hyper Giant has a heavy emphasis on the use of ethics in AI development, and even has an AI Code of Ethics, which is something rare to see in a tech startup. What led to the decision in the first place to create this code, and what was it like developing it?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The decision to create the AI Code of Ethics came from the top, Ben Lamm. I was brought in specifically to ensure that we stay at the forefront of best practices in operationalizing ethics in our AI workflows and educating our team, clients, and partners on the importance of embedding ethical reasoning at every step of the process. Developing our ethical code and framework has been a lot of fun. As part of the process I visit, and vet our approach, with some of the top thought leaders on ethics in AI. Sangeeta Mudnal (Group Project Manager, for Microsoft’s Responsible AI initiative) is on our Board of Advisors and is very helpful. Frank Buytendijk , head of Ethics at Gartner Research is a great sounding board and thought partner. The professors at Stanford’s AI Lab have been gracious in sharing their approach to ethics education with their computer science students. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Can you give an example of a time where you needed to make technical compromises to align with your ethical code?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To date our AI uses cases have not caused us to make technical compromises. Instead, we have to make changes to our AI development workflows. In the old days (pre-2019), AI developers would be presented with a business problem and immediately begin crafting a technical solution and the thought of ethics (or compliance) was an afterthought. Now we incorporate ethical reasoning at the outset and it forces the team to create a range of technical solutions to a given problem and choose the solution with the least ethical challenges and most people-centered beneficial outcomes. In the end, it makes the team more creative because we have to think of more solutions to client problems.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;There are many accusations that an ethics code only leads to ethics washing, as there is no proof that a company actually follows such a code. How do you ensure that this is not the case with Hyper Giant?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The AI Code of Ethics has been helpful, but the mindset that created it is the secret sauce. Every company should (and likely will) develop a statement for public relations reasons, but the companies that embed ethical reasoning into their workflows and culture are the ones who will own the future. We have found that becoming well-versed into ethical reasoning has increased our overall creativity and led our product teams to develop more robust AI solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How has having an AI Code of Ethics helped shape the work HyperGiant is doing?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Boards of Directors and C-suite executives are well-aware of the well-publicized cases of ethical failures in the technology industry. The image of Mark Zuckerberg sitting in front of Congress being taken to task for Facebook’s myriad ethical failures, including billions of dollars in fines, has prompted introspection on the part of senior leaders. This introspection has increased the calls for companies with a well-defined set of values and tangible experience embedding ethics into the products they create. This has increased the number of pitches we have invited into, but more importantly demonstrating ethics has allowed us to expand our scope of work and amount of projects with our current clients.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Can you describe an example where it was hard work to help your client’s adjust their expectations so that they would be compatible with your ethics code?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I would not want to identify a specific client, but I can speak to issues that apply to every client. Embedding ethics into AI development workflows is challenging at first. The first step is getting the CEO and other responsible C-suite executives to mandate compliance with the AI Code of Ethics. The second step is traing the relevant stakeholders in our ethical reasoning model and vetting all proposed use cases through the model. Once clients see the creativity unleashed and the number of technical solutions they are able to create, they began to embrace the model as a value-add to their overall workflow.&lt;/p&gt;
&lt;h2&gt;Ethics in Technology&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What do you believe is the biggest challenge we are facing in AI Ethics?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The biggest challenge is the prevailing Silicon Valley conventional wisdom that ethical mishaps are just the price to be paid for being first (“move fast, break things”). With the prediction that AI leadership will be worth trillions of dollars, it is easy to see why many companies will not want to change their approach to product development and deployment. We view our role as educating the industry on the economic value of ethics in AI and the trust it will create within the marketplace. We believe the trust generated is what will be required to compete and gain market leadership in the AI-driven world.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;At Ethical Intelligence, we are working to change the narrative around AI Ethics from ethics being a blocker in technology to ethics being an asset to long-term innovation. What inspires you about the field of AI Ethics, and what good do you see if doing in the world of AI?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I agree wholeheartedly with Ethical Intelligence’s approach. I am inspired by all of the discussion (and research) in industry, academia, and the media to raise awareness of the importance of ethics in AI. If privacy had this much attention at the early stages of the internet, I believe social media and other platforms would have evolved with many more protections built-in. The growth of AI gives us another chance to get ethics in technology right, and I am excited to be a part of that process.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anything further you would like to share?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ethics equals trust. Trust equals sustained economic value creation.&lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Tweeting from the Afterlife: Exploring the Deaths of Social Network Members and the Birth of Online Remembrance</title><link href="/tweeting-from-the-afterlife.html" rel="alternate"></link><published>2020-01-08T07:20:00+01:00</published><updated>2020-01-08T07:20:00+01:00</updated><author><name>Robert Seddon</name></author><id>tag:None,2020-01-08:/tweeting-from-the-afterlife.html</id><summary type="html">&lt;p&gt;Tweeting from the Afterlife: Exploring the Deaths of Social Network Members and the Birth of Online Remembrance&lt;/p&gt;</summary><content type="html">&lt;p&gt;Social networks such as Twitter and Facebook are frequent targets of criticism for handling personal data in ways that undermine the privacy and dignity of their users. Twitter recently found itself in the unusual position of receiving criticism for an effort to protect the privacy of user data, by announcing plans to initiate a cull of abandoned accounts on December 11 2019 as part of an effort to comply with EU data privacy regulations.&lt;/p&gt;
&lt;p&gt;Who would miss accounts that had been disused for ages? Quite a few people, it turned out. It wasn’t because lapsed users thought they might start using their accounts again. The accounts most at stake will never tweet again, because they fell silent when their owners died.&lt;/p&gt;
&lt;p&gt;By leaving their thoughts behind on the social network, people had created a presence for themselves that persisted long after death. Twitter suddenly faced objections from people who would visit the preserved thoughts of a departed &lt;a href="https://www.inc.com/jason-aten/twitter-said-it-would-delete-unused-accounts-then-it-realized-some-of-them-belong-to-people-we-want-to-remember.html"&gt;father&lt;/a&gt;, &lt;a href="https://www.bbc.co.uk/news/newsbeat-50584688"&gt;boyfriend&lt;/a&gt; or other loved ones.&lt;/p&gt;
&lt;p&gt;For a site that promotes itself as the place to go to discover ‘what’s happening in the world and what people are talking about right now’ this came as a surprise. In fact, it highlights one of the significant changes in online culture since use of the Internet became widespread. A decade ago, Friendster was a major social network, but usage declined until the site eventually closed, an event foreshadowed by an article in the satirical publication The Onion, which &lt;a href="https://www.theonion.com/internet-archaeologists-find-ruins-of-friendster-civili-1819594871"&gt;spoofed Friendster&lt;/a&gt; as an archaeological site marking a lost civilisation. Abandoned accounts on a social network aren’t just a liability, they reflect the health of the platform, and can be an undesirable signal that platforms might wish to conceal. It’s very natural for a business to value current and potential customers, and lost customers that might yet be regained, but much less to value those that are deceased. How should a social network value users that can no longer use its product? &lt;/p&gt;
&lt;h2&gt;Read-Only Memorials&lt;/h2&gt;
&lt;p&gt;As more and more of us have started living significant parts of our lives online, however, an increasing amount of the content on social platforms has been created by people who are no longer alive. Because of this, we can all expect to have ‘digital afterlives’.&lt;/p&gt;
&lt;p&gt;The sheer scale is remarkable. Take Facebook as an example, which already has a memorialisation feature for deceased users. &lt;a href="https://journals.sagepub.com/doi/full/10.1177/2053951719842540"&gt;Carl Öhman and David Watson project&lt;/a&gt; that billions of Facebook users will have passed away before 2100, by which time ‘the dead may well outnumber the living’. Öhman’s research with Luciano Floridi examines a whole &lt;a href="https://link.springer.com/article/10.1007/s11023-017-9445-2"&gt;digital afterlife industry&lt;/a&gt; dealing with &lt;a href="https://ora.ox.ac.uk/objects/uuid:c059841d-a702-4218-8c1b-39ff49dc6c65/download_file?safe_filename=Ohman_Floridi_R1_edited.pdf&amp;amp;file_format=application%252Fpdf&amp;amp;type_of_work=Journal+article"&gt;online remains&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Firms such as Eterni.me and Replica now offer consumers online chat bots, based on one’s digital footprint, which continue to live on after users die, enabling the bereaved to “stay in touch” with the deceased. This new phenomenon has opened up opportunities for commercial enterprises to monetise the digital afterlife of Internet users. As a consequence the economic interests of these firms are increasingly shaping the presence of the online dead.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In large part this is about how those still living can be helped to feel better and cope with loss—and that’s no small thing. Technology can honour the past as well as building the future. When London’s underground railway upgraded its automatic announcement system, in the process replacing the old ‘Mind the gap’ recording, it emerged that the actor who made it had left behind a widow who still listened for her husband’s voice at her station. Railway staff worked to digitise and restore the recording, securing emotional succour for one woman and over &lt;a href="https://twitter.com/garius/status/1204795961731629058"&gt;forty thousand likes on Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another form of benefit for those still living can come from the rich historical resource which all these online posters have cumulatively created. Öhman and Watson emphasise this aspect, describing the aggregate contributions of social media users as a form of cultural heritage which is of value both to historians and ‘to future generations as part of their record and self-understanding’. They advocate ‘a multi-stakeholder approach’ to the maintenance of this record: a commercial platform like Facebook has an obvious economic interest in the running of its own services, but other interested parties might include ‘states, NGOs, universities, libraries, museums’ and the like. Historic data might someday be handled like historic buildings, as assets that come with special responsibilities for their owners.&lt;/p&gt;
&lt;h2&gt;Mortal Obligations?&lt;/h2&gt;
&lt;p&gt;To say that preserving data of this sort benefits the living, however, isn’t to say that we should understand the value of their digital artifacts entirely in relation to the interests of living people’s wants and needs. We may care not only about doing good for the living, but also about doing the rightly respectful thing for the departed themselves.&lt;/p&gt;
&lt;p&gt;If we believe they are departed, though, to either oblivion or an otherworldly afterlife, then we may be perplexed about how we might be able to treat them well or badly: how they could be, in the philosophical jargon, moral patients. Explanations we might give for moral responsibilities towards sentient beings—explanations involving the capacity to suffer, for example—seem doubtfully applicable towards those who have gone to rest in peace.&lt;/p&gt;
&lt;p&gt;The problem isn’t that we can’t conceive of how there could be any kind of moral patient besides a living, thinking, feeling being. Some philosophers do believe there are other kinds of moral patient, and quite possibly you do as well: if you care about ‘the environment’ then you care for something that, though it incorporates various kinds of sentient organism, isn’t reducible to any of them. The problem is that the conceptual toolkit I’d use in asking, say, what could be wrong with wantonly destroying a fossilised ammonite isn’t a toolkit one can simply go ahead and apply to things left behind by human beings. We don’t relate to that long-dead organism as we do to a dead person.&lt;/p&gt;
&lt;p&gt;So questions are explored about what could make dead people, as a class, qualify as moral patients. Is it possible to harm the dead? Do practices like writing and honouring wills imply that obligations towards the dead person who is are disguised duties to the living one who was, or does that merely restate the paradox in another form?&lt;/p&gt;
&lt;h2&gt;Social Media Absence&lt;/h2&gt;
&lt;p&gt;Twitter was forced to consider dead people as a class among its account-holders, but that’s because of some users’ very specific connections with particular people they’ve lost: with parents and spouses and lovers and friends, people with names and personal histories. Parts of those individual histories linger online. When we remember people through mementos—a portrait, say—our treatment of the objects expresses attitudes towards the people themselves. If you see a social media profile in this way, like a portrait you’d hang in a place of honour, then the point of view from which it’s obsolete clutter in the database is going to be far from what you can personally endorse.&lt;/p&gt;
&lt;p&gt;Öhman and Floridi suggest that we should literally regard the digital material people leave behind as a form of human remains—‘not merely regarded as a chattel or an estate, but as something constitutive of one’s personhood’—and should draw on archaeological ethics to identify the principles behind respectful display of them. It’s unclear how far existing archaeological ethics will take us towards working out whether anyone has an obligation to fund the ongoing display, however; the Internet has nothing analogous to reburial.&lt;/p&gt;
&lt;p&gt;Some writers on the ethics of heritage and human remains contrast a materialist, empiricist West, which thinks of being dead as being gone, with different societies in which the dead are understood to have an ongoing presence in the life of a community. &lt;a href="http://www.piotrbienkowski.co.uk/"&gt;Piotr Bienkowski&lt;/a&gt;, for example, has written that scepticism about connections with people from former times arises from a view of the world that regards the dead ‘as no longer existing or having personhood in any sense’. If that’s how the West truly thinks, though, then perhaps our technological society is developing so that we start to think differently.&lt;/p&gt;
&lt;p&gt;It is now common for some of our most significant relationships to take place entirely online. If we encounter people through their online presence while they live to update it, maybe it’s not so strange a notion that something of them remains present while it persists on the servers. We should bear in mind, though, that for many people, their  online personae are not authoritative self-portraitures but often impressionistic or playful takes on themselves, sometimes multiple masques that relate only part of their personalities to specific audiences. We should perhaps be careful about these subtleties when memorializing online self-expression, and not take it more seriously in death than the mind behind it did in life. Nevertheless, even an outright parody account can be fondly regarded as part of a community. It leaves a hole in that community when the posting suddenly ceases, and can retain its place of honour in the ‘social graph’ of friend or follower relationships.&lt;/p&gt;
&lt;h2&gt;Last Words&lt;/h2&gt;
&lt;p&gt;In ten years we’ve gone from seeing Friendster’s decline spoofed as an archaeological dig to Öhman and Watson’s serious proposal that Internet hosts are custodians of a form of human cultural heritage. And as stories about bereavement go, Twitter’s experience carries a heartwarming moral: what seemed to be a load of disused data was in fact a memorial to the dead and is actually giving living people reasons to keep coming back to the site.&lt;/p&gt;
&lt;p&gt;Twitter was taken aback by the discovery that deceased people can still be part of its community. This is not the only form online remembrance can take—you may even have had a secret memorial sent to you in &lt;a href="https://xclacksoverhead.org/home/about"&gt;HTTP headers&lt;/a&gt; — but it’s a form that underlines how those content posters who’ve passed on can still have significant social and therefore ethical relationships with living people: relationships that matter to those people in ways that foster a deep commitment to keeping them going.&lt;/p&gt;
&lt;p&gt;Digital afterlives are a recent and growing source of questions for industry and policy—and Twitter will still be working out how best to square them with EU privacy rules. These are the early stages of working out how to handle personal grief on the public Internet: who shall bear the costs, and when it’s acceptable for the ‘digital afterlife industry’ to explore the opportunities. Will this lead to the involvement of religious bodies, such as the Vatican, when it comes to the preservation of digital afterlives? For tech companies already under the spotlight, approaching these questions with ethical sensitivity will be part of securing their own survival. For regulators and ethicists, this is an important issue that problematises recent approaches to privacy and data rights. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry><entry><title>Falsehoods Programmers Believe About Ethics</title><link href="/falsehoods-programmers-believe-about-ethics.html" rel="alternate"></link><published>2019-12-02T07:20:00+01:00</published><updated>2019-12-02T07:20:00+01:00</updated><author><name>Olivia Gambelin</name></author><id>tag:None,2019-12-02:/falsehoods-programmers-believe-about-ethics.html</id><summary type="html">&lt;p&gt;Some thoughts about the interface between programmers and tech ethics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a Tech Ethicist, I come into contact with programmers almost daily, as my work requires me to navigate the technical objectives and implementation details in addition to the ethical dimensions of the projects I analyze. Through these experiences, I’ve found myself having some of the same conversations over and over, all surrounding the applicability of ethics in the tech industry. There are some persistent misconceptions about both the study of ethics and its application to technical work. In order to help bring clarity to the confusion surrounding ethics in tech, I’m going to briefly discuss three significant misconceptions I've observed. &lt;/p&gt;
&lt;h2&gt;There is one right answer &lt;br /&gt; &lt;em&gt;(or no right answers, if you take a subjective approach)&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Ethics is the study of right and wrong, so shouldn’t there be a right and wrong answer to every ethical question? If there was, then the ethical dilemmas programers and technologists face would have long been answered by general principles with universal applicability across projects, cultures, and domains. Of course, if this were true, the daily news would not include so many stories involving ethical lapses or miscalculations where technological platforms, tools, and solutions cause serious harms. We are all familiar with the headline stories, self-driving cars making errors that human drivers would not, autonomous weapons systems being developed and deployed by militaries, banks and governments using AI driven gatekeepers to control access to credit and social services, and fintech companies making trading decisions at the speed of light, far faster than humans can intervene. These are all headline worthy examples of tech gone wrong due to ethical lapses. But, when we take a step back from these bigger stories, we also discover the points in which software engineers and technologists are at risk for the same ethical lapses, as they make decisions every day in the course of their work that can have real ethical impacts. ‘Do I use a social sign-on SDK, because it’s faster, even though there are privacy tradeoffs? Do we let a test suite languish to save time? Do we optimize for the biggest curve, even if that means missing important edge cases? Is the dataset we are pre-training an algorithm with fair, and ethically sourced?’&lt;/p&gt;
&lt;p&gt;It might seem like there are only two possible solutions to these ethical dilemmas. Either we finally discover this elusive single, straightforward, universal guideline that can be applied across the board, or, if that doesn’t exist, then we must throw up our hands in defeat because ethics is too subjective and we will never arrive at an acceptable single answer.  Why is it that these seem to be our only two possible solutions to solving tech ethics problems? Well, we all come from different backgrounds, which in turn shape our understanding of what constitutes right and wrong. Everything from our culture, our education, and our life experiences can potentially influence how we approach ethical problems. Because of this, being aware of, and finding solutions to the ethical dilemmas raised by technology requires careful navigation of the multiple understandings of right and wrong at play. To complicate matters further, ethics is an emotionally charged topic. We are very attached to our personal ethical frameworks, and are hurt when it is threatened or violated. And yet, there are situations in which two people can have opposing ethical frameworks, and still both arrive at what could be considered a right answer in accordance to those individual frameworks.  The combination of this emotional attachment along with the varying ethical frameworks results in the feeling that the right answer to our ethical dilemmas is impossible to find. &lt;/p&gt;
&lt;p&gt;However, I would argue otherwise, as there are significant overlaps in the ethical frameworks between individuals. This overlap in turn enables us to uncover an understanding of the problems we face and from there make informed decisions. What I'm suggesting is that we don't need to solve the fundamental metaethical dilemmas that philosophers have been arguing about for millennia in order to appreciate and mitigate the ethical risks that technical decisions expose us to. Often an informed ethical awareness is the most significant step towards improving our ability to foresee and to mitigate the ethical risks in our technical practices. &lt;/p&gt;
&lt;h2&gt;It is possible to eliminate ethical bias&lt;/h2&gt;
&lt;p&gt;Algorithmic bias has become a buzzword with a strong negative connotation which results in  often hearing that we need to eliminate bias in our algorithms. However, when it comes to bias in terms of ethical decision making, it is not actually possible to completely eliminate that bias. This is because data is not neutral, it reproduces the biases in the world it comes from, as well as new biases introduced knowingly or accidentally through collection, processing and use. When we try to mitigate bias in data, we must often confront real ethical dilemmas from the world it came from, and make our best efforts to address them. Confronting these dilemmas, or, in other words, attempting to mitigate the biases inherent in the data, involves taking a stance of our own, which is a sort of bias itself. So, in this sense, ethical bias is impossible to fully eliminate, which makes it something we should take on as a serious responsibility.&lt;/p&gt;
&lt;p&gt;Let’s explore this. You’re working for a university, collecting simple data on students such as class attendance, grades, library usage, when you’re asked to develop specific insight on student mental health out of this data. You of course know to clean the data and set controls to mitigate any previous biases as a first step. However, in order to move on to creating the algorithm that will allow you to draw insights into the mental health status of students, you must first take a biased stance on ethics. You either have to decide to create an algorithm that will track student mental health with the hopes of providing care and wellbeing for the greatest amount of people, or decide that this would be a violation of respecting the dignity of an individual, and so not create the algorithm.  In other words, your decision between the two equally as important ethical values depends on your own bias towards those values. Since there is no way to honor both values equally in this situation, a decision must be taken by prioritizing one value over the other, and the ethical bias cannot be avoided. It is also important to note that in this example, the ethical dilemma surfaces only after the info from the data is extracted. By extracting this knowledge on the mental status of students, an ethical imperative to help the identified students is created. This all goes to show, collecting knowledge from datasets is not an ethically neutral task. 
Programmers need to be acutely aware of what ethical values are at play and how they are prone to prioritize these values. It's easy to get lost in the weeds solving architectural and implementation problems, but even these ground level decisions can have ethical impacts we must be aware of. Choices of technologies, datasets, integration partners, and problem definitions all expose the ethical edges of technical development. At the macro level, it's important to take a step back, and make sure we have articulated and examined the values that have guided and shape the formulation of business and technical goals, so that the ways these values interface with other values in the world in which our systems are deployed are understood and made clear. It is therefore essential to to build this kind of ethical awareness into organizational culture and technical decision-making. &lt;/p&gt;
&lt;h2&gt;Ethics is a blocker to innovation&lt;/h2&gt;
&lt;p&gt;Is ethics someone else's problem? Surely the software’s only responsibility is to is to work, not understand ethics. This is a view I often hear when speaking with programmers, as they express an underlying fear of having constraints placed on their previously unencumbered freedom to develop their technology. Ethics can appear to be a blocker if it is viewed as just another piece of paperwork that needs to be filed or the opportunity for someone from a non-technical background who just doesn’t get ‘it’ to kill an interesting project before it’s had the chance to get off the ground. &lt;/p&gt;
&lt;p&gt;However, there’s another perspective. You can think about tech ethics in much the same way programmers are accustomed to thinking about technical debt. Making short term architectural decisions to meet immediate needs involves taking on a notion of "well, it works" that is dangerously myopic. Most programmers are intimately familiar with the exponential accumulation of technical debt, and the astounding difficulty in mitigating it, when working in an environment that doesn’t value longer term thinking. Ethics is the same way. Tiny shortcuts in seemingly insignificant systems can have real effects on people’s lives. In a recent project we looked at, a contractor working on the logged-out view of a particular web application state left in personally identifying information that could be used to unmask users in ways at risk to be that could be abused into pinpointing places and times they had attended particular events. The contractor satisfied the requirements they were given, the test suite didn’t check this, QA didn’t, and a higher level-commitment to recognizing the failure modes that mishandling the unique data this company held had never been made.&lt;/p&gt;
&lt;p&gt;Building in ethical awareness from the ground up is an asset, not a liability. If we go about developing our technology ethically, we are innovating for long-term sustainability, not short-term profit. Whereas, if we “move fast and break things”, without ethical considerations, then it is only a matter of time before consequences arise that are difficult or impossible to unwind. When we break things, people get hurt. There are financial, social, legal, and moral costs involved. Technical innovation needs ethics (and technical advancement likewise allows use to make ethical advances), because at the end of the day, we as humans try to do the right thing, and expect our technology to do the same. &lt;/p&gt;</content><category term="articles"></category><category term="ethics"></category></entry></feed>